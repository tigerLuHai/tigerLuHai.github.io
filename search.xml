<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>深入理解java的BIO、NIO、AIO</title>
      <link href="/2020/02/29/shen-ru-li-jie-java-de-bio-nio-aio/"/>
      <url>/2020/02/29/shen-ru-li-jie-java-de-bio-nio-aio/</url>
      
        <content type="html"><![CDATA[<h1 id="IO-介绍"><a href="#IO-介绍" class="headerlink" title="IO 介绍"></a>IO 介绍</h1><p>我们通常所说的 BIO 是相对于 NIO 来说的，BIO 也就是 Java 开始之初推出的 IO 操作模块，BIO 是 BlockingIO 的缩写，顾名思义就是阻塞 IO 的意思。</p><h2 id="BIO、NIO、AIO的区别"><a href="#BIO、NIO、AIO的区别" class="headerlink" title="BIO、NIO、AIO的区别"></a>BIO、NIO、AIO的区别</h2><ol><li>BIO 就是传统的 <a href="http://java.io" target="_blank" rel="noopener">java.io</a> 包，它是基于流模型实现的，交互的方式是同步、阻塞方式，也就是说在读入输入流或者输出流时，在读写动作完成之前，线程会一直阻塞在那里，它们之间的调用时可靠的线性顺序。它的有点就是代码比较简单、直观；缺点就是 IO 的效率和扩展性很低，容易成为应用性能瓶颈。</li><li>NIO 是 Java 1.4 引入的 java.nio 包，提供了 Channel、Selector、Buffer 等新的抽象，可以构建多路复用的、同步非阻塞 IO 程序，同时提供了更接近操作系统底层高性能的数据操作方式。</li><li>AIO 是 Java 1.7 之后引入的包，是 NIO 的升级版本，提供了异步非堵塞的 IO 操作方式，所以人们叫它 AIO（Asynchronous IO），异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。</li></ol><p><strong>在JAVA NIO框架中，我们说到了一个重要概念“selector”（选择器）。它负责代替应用查询中所有已注册的通道到操作系统中进行IO事件轮询、管理当前注册的通道集合，定位发生事件的通道等操操作；但是在JAVA AIO框架中，由于应用程序不是“轮询”方式，而是订阅-通知方式，所以不再需要“selector”（选择器）了，改由channel通道直接到操作系统注册监听。</strong> </p><p><strong>JAVA AIO框架中，只实现了两种网络IO通道“AsynchronousServerSocketChannel”（服务器监听通道）、“AsynchronousSocketChannel”（socket套接字通道）。但是无论哪种通道他们都有独立的fileDescriptor（文件标识符）、attachment（附件，附件可以使任意对象，类似“通道上下文”），并被独立的SocketChannelReadHandle类实例引用。我们通过debug操作来看看它们的引用结构</strong> </p><h1 id="同步、异步、阻塞、非阻塞"><a href="#同步、异步、阻塞、非阻塞" class="headerlink" title="同步、异步、阻塞、非阻塞"></a>同步、异步、阻塞、非阻塞</h1><p>上面说了很多关于同步、异步、阻塞和非阻塞的概念，接下来就具体聊一下它们4个的含义，以及组合之后形成的性能分析。</p><h2 id="同步与异步"><a href="#同步与异步" class="headerlink" title="同步与异步"></a>同步与异步</h2><p>同步就是一个任务的完成需要依赖另外一个任务时，只有等待被依赖的任务完成后，依赖的任务才能算完成，这是一种可靠的任务序列。要么成功都成功，失败都失败，两个任务的状态可以保持一致。而异步是不需要等待被依赖的任务完成，只是通知被依赖的任务要完成什么工作，依赖的任务也立即执行，只要自己完成了整个任务就算完成了。至于被依赖的任务最终是否真正完成，依赖它的任务无法确定，所以它是不可靠的任务序列。我们可以用打电话和发短信来很好的比喻同步与异步操作。</p><h2 id="阻塞与非阻塞"><a href="#阻塞与非阻塞" class="headerlink" title="阻塞与非阻塞"></a>阻塞与非阻塞</h2><p>阻塞与非阻塞主要是从 CPU 的消耗上来说的，阻塞就是 CPU 停下来等待一个慢的操作完成 CPU 才接着完成其它的事。非阻塞就是在这个慢的操作在执行时 CPU 去干其它别的事，等这个慢的操作完成时，CPU 再接着完成后续的操作。虽然表面上看非阻塞的方式可以明显的提高 CPU 的利用率，但是也带了另外一种后果就是系统的线程切换增加。增加的 CPU 使用时间能不能补偿系统的切换成本需要好好评估。</p><h2 id="同-异、阻-非堵塞-组合"><a href="#同-异、阻-非堵塞-组合" class="headerlink" title="同/异、阻/非堵塞 组合"></a>同/异、阻/非堵塞 组合</h2><p>同/异、阻/非堵塞的组合，有四种类型，如下表：</p><table><thead><tr><th>组合方式</th><th>性能分析</th></tr></thead><tbody><tr><td>同步阻塞</td><td>最常用的一种用法，使用也是最简单的，但是 I/O 性能一般很差，CPU 大部分在空闲状态。</td></tr><tr><td>同步非阻塞</td><td>提升 I/O 性能的常用手段，就是将 I/O 的阻塞改成非阻塞方式，尤其在网络 I/O 是长连接，同时传输数据也不是很多的情况下，提升性能非常有效。 这种方式通常能提升 I/O 性能，但是会增加CPU 消耗，要考虑增加的 I/O 性能能不能补偿 CPU 的消耗，也就是系统的瓶颈是在 I/O 还是在 CPU 上。</td></tr><tr><td>异步阻塞</td><td>这种方式在分布式数据库中经常用到，例如在网一个分布式数据库中写一条记录，通常会有一份是同步阻塞的记录，而还有两至三份是备份记录会写到其它机器上，这些备份记录通常都是采用异步阻塞的方式写 I/O。异步阻塞对网络 I/O 能够提升效率，尤其像上面这种同时写多份相同数据的情况。</td></tr><tr><td>异步非阻塞</td><td>这种组合方式用起来比较复杂，只有在一些非常复杂的分布式情况下使用，像集群之间的消息同步机制一般用这种 I/O 组合方式。如 Cassandra 的 Gossip 通信机制就是采用异步非阻塞的方式。它适合同时要传多份相同的数据到集群中不同的机器，同时数据的传输量虽然不大，但是却非常频繁。这种网络 I/O 用这个方式性能能达到最高。</td></tr></tbody></table><h1 id="Socket-和-NIO-的多路复用"><a href="#Socket-和-NIO-的多路复用" class="headerlink" title="Socket 和 NIO 的多路复用"></a>Socket 和 NIO 的多路复用</h1><p>本节带你实现最基础的 Socket 的同时，同时会实现 NIO 多路复用，还有 AIO 中 Socket 的实现。</p><h2 id="传统的-Socket-实现"><a href="#传统的-Socket-实现" class="headerlink" title="传统的 Socket 实现"></a>传统的 Socket 实现</h2><p>接下来我们将会实现一个简单的 Socket，服务器端只发给客户端信息，再由客户端打印出来的例子，代码如下：</p><pre><code>int port = 4343; //端口号// Socket 服务器端（简单的发送信息）Thread sThread = new Thread(new Runnable() {    @Override    public void run() {        try {            ServerSocket serverSocket = new ServerSocket(port);            while (true) {                // 等待连接                Socket socket = serverSocket.accept();                Thread sHandlerThread = new Thread(new Runnable() {                    @Override                    public void run() {                        try (PrintWriter printWriter = new PrintWriter(socket.getOutputStream())) {                            printWriter.println(&quot;hello world！&quot;);                            printWriter.flush();                        } catch (IOException e) {                            e.printStackTrace();                        }                    }                });                sHandlerThread.start();            }        } catch (IOException e) {            e.printStackTrace();        }    }});sThread.start();// Socket 客户端（接收信息并打印）try (Socket cSocket = new Socket(InetAddress.getLocalHost(), port)) {    BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(cSocket.getInputStream()));    bufferedReader.lines().forEach(s -&gt; System.out.println(&quot;客户端：&quot; + s));} catch (UnknownHostException e) {    e.printStackTrace();} catch (IOException e) {    e.printStackTrace();}</code></pre><ul><li>调用 accept 方法，阻塞等待客户端连接；</li><li>利用 Socket 模拟了一个简单的客户端，只进行连接、读取和打印；</li></ul><p>在 Java 中，线程的实现是比较重量级的，所以线程的启动或者销毁是很消耗服务器的资源的，即使使用线程池来实现，使用上述传统的 Socket 方式，当连接数极具上升也会带来性能瓶颈，原因是线程的上线文切换开销会在高并发的时候体现的很明显，并且以上操作方式还是同步阻塞式的编程，性能问题在高并发的时候就会体现的尤为明显。</p><p>以上的流程，如下图：</p><p><img src="/2020/02/29/shen-ru-li-jie-java-de-bio-nio-aio/2.png" alt></p><h2 id="NIO-多路复用"><a href="#NIO-多路复用" class="headerlink" title="NIO 多路复用"></a>NIO 多路复用</h2><p>介于以上高并发的问题，NIO 的多路复用功能就显得意义非凡了。</p><p>NIO 是利用了单线程轮询事件的机制，通过高效地定位就绪的 Channel，来决定做什么，仅仅 select 阶段是阻塞的，可以有效避免大量客户端连接时，频繁线程切换带来的问题，应用的扩展能力有了非常大的提高。</p><pre><code>// NIO 多路复用ThreadPoolExecutor threadPool = new ThreadPoolExecutor(4, 4,        60L, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());threadPool.execute(new Runnable() {    @Override    public void run() {        try (Selector selector = Selector.open();             ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();) {            serverSocketChannel.bind(new InetSocketAddress(InetAddress.getLocalHost(), port));            serverSocketChannel.configureBlocking(false);            serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);            while (true) {                selector.select(); // 阻塞等待就绪的Channel                Set&lt;SelectionKey&gt; selectionKeys = selector.selectedKeys();                Iterator&lt;SelectionKey&gt; iterator = selectionKeys.iterator();                while (iterator.hasNext()) {                    SelectionKey key = iterator.next();                    try (SocketChannel channel = ((ServerSocketChannel) key.channel()).accept()) {                        channel.write(Charset.defaultCharset().encode(&quot;你好，世界&quot;));                    }                    iterator.remove();                }            }        } catch (IOException e) {            e.printStackTrace();        }    }});// Socket 客户端（接收信息并打印）try (Socket cSocket = new Socket(InetAddress.getLocalHost(), port)) {    BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(cSocket.getInputStream()));    bufferedReader.lines().forEach(s -&gt; System.out.println(&quot;NIO 客户端：&quot; + s));} catch (IOException e) {    e.printStackTrace();}</code></pre><ul><li>首先，通过 Selector.open() 创建一个 Selector，作为类似调度员的角色；</li><li>然后，创建一个 ServerSocketChannel，并且向 Selector 注册，通过指定 SelectionKey.OP_ACCEPT，告诉调度员，它关注的是新的连接请求；</li><li>为什么我们要明确配置非阻塞模式呢？这是因为阻塞模式下，注册操作是不允许的，会抛出 IllegalBlockingModeException 异常；</li><li>Selector 阻塞在 select 操作，当有 Channel 发生接入请求，就会被唤醒；</li></ul><p>下面的图，可以有效的说明 NIO 复用的流程：</p><p><img src="/2020/02/29/shen-ru-li-jie-java-de-bio-nio-aio/1.png" alt></p><p>就这样 NIO 的多路复用就大大提升了服务器端响应高并发的能力。</p><h2 id="AIO-版-Socket-实现"><a href="#AIO-版-Socket-实现" class="headerlink" title="AIO 版 Socket 实现"></a>AIO 版 Socket 实现</h2><p>Java 1.7 提供了 AIO 实现的 Socket 是这样的，如下代码：</p><pre><code>// AIO线程复用版Thread sThread = new Thread(new Runnable() {    @Override    public void run() {        AsynchronousChannelGroup group = null;        try {            group = AsynchronousChannelGroup.withThreadPool(Executors.newFixedThreadPool(4));            AsynchronousServerSocketChannel server = AsynchronousServerSocketChannel.open(group).bind(new InetSocketAddress(InetAddress.getLocalHost(), port));            server.accept(null, new CompletionHandler&lt;AsynchronousSocketChannel, AsynchronousServerSocketChannel&gt;() {                @Override                public void completed(AsynchronousSocketChannel result, AsynchronousServerSocketChannel attachment) {                    server.accept(null, this); // 接收下一个请求                    try {                        Future&lt;Integer&gt; f = result.write(Charset.defaultCharset().encode(&quot;你好，世界&quot;));                        f.get();                        System.out.println(&quot;服务端发送时间：&quot; + new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;).format(new Date()));                        result.close();                    } catch (InterruptedException | ExecutionException | IOException e) {                        e.printStackTrace();                    }                }                @Override                public void failed(Throwable exc, AsynchronousServerSocketChannel attachment) {                }            });            group.awaitTermination(Long.MAX_VALUE, TimeUnit.SECONDS);        } catch (IOException | InterruptedException e) {            e.printStackTrace();        }    }});sThread.start();// Socket 客户端AsynchronousSocketChannel client = AsynchronousSocketChannel.open();Future&lt;Void&gt; future = client.connect(new InetSocketAddress(InetAddress.getLocalHost(), port));future.get();ByteBuffer buffer = ByteBuffer.allocate(100);client.read(buffer, null, new CompletionHandler&lt;Integer, Void&gt;() {    @Override    public void completed(Integer result, Void attachment) {        System.out.println(&quot;客户端打印：&quot; + new String(buffer.array()));    }    @Override    public void failed(Throwable exc, Void attachment) {        exc.printStackTrace();        try {            client.close();        } catch (IOException e) {            e.printStackTrace();        }    }});Thread.sleep(10 * 1000);</code></pre>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>对MySQL执行引擎InnoDB的认识</title>
      <link href="/2020/02/29/dui-mysql-zhi-xing-yin-qing-innodb-de-ren-shi/"/>
      <url>/2020/02/29/dui-mysql-zhi-xing-yin-qing-innodb-de-ren-shi/</url>
      
        <content type="html"><![CDATA[<h2 id="InnoDB索引实现"><a href="#InnoDB索引实现" class="headerlink" title="InnoDB索引实现"></a>InnoDB索引实现</h2><p>虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。因为InnoDB支持聚簇索引（主键索引），聚簇索引就是表，所以InnoDB不用像MyISAM那样需要独立的行存储。也就是说，InnoDB的数据文件本身就是索引文件。</p><p>聚簇索引的每一个叶子节点都包含了主键值、事务ID、用于事务和MVCC的回滚指针以及<strong>所有的剩余列</strong>。假设我们以col1为主键，则下图是一个InnoDB表的聚簇索引（主键索引）（Primary key）示意。</p><p><img src="/2020/02/29/dui-mysql-zhi-xing-yin-qing-innodb-de-ren-shi/7.png" alt></p><p>与MyISAM不同的是，InnoDB的二级索引和聚簇索引很不相同。<strong>InnoDB的二级索引的叶子节点存储的不是行号（行指针），而是主键列</strong>。这种策略的缺点是二级索引需要两次索引查找，第一次在二级索引中查找主键，第二次在聚簇索引中通过主键查找需要的数据行。</p><p>画外音：可以通过我们前面提到过的<strong>索引覆盖</strong>来避免回表查询，这样就只需要一次回表查询，对于InnoDB而言，就是只需要一次索引查找就可以查询到需要的数据记录，因为需要的数据记录已经被索引到二级索引中，直接就可以找到。</p><p>好处是InnoDB在移动行时无需更新一级索引中的这个”指针“，因为主键是不会改变的，但是行指针却会改变。</p><p>InnoDB的二级索引示意如图：</p><p><img src="/2020/02/29/dui-mysql-zhi-xing-yin-qing-innodb-de-ren-shi/6.png" alt></p><h3 id="使用InnoDB主键应该知道的事项"><a href="#使用InnoDB主键应该知道的事项" class="headerlink" title="使用InnoDB主键应该知道的事项"></a>使用InnoDB主键应该知道的事项</h3><p>因为InnoDB的索引的方式通过主键聚集数据，严重依赖主键。索引如果没有定义主键，那么InnoDB会选择一个唯一的非空索引代替。如果没有这样的索引，InnoDB会隐式定义一个主键来作为聚簇索引。</p><p><strong>聚簇索引的优点有：</strong></p><p>1.可以把相关数据存储在一起，减少数据查询时的磁盘I/O</p><p>2.数据访问更快，因为聚簇索引就是表，索引和数据保存在一个B+Tree中</p><p>3.使用索引覆盖的查询时可以直接使用页节点中的主键值</p><p><strong>聚簇索引的缺点有：</strong></p><p>1.插入速度严重依赖插入顺序</p><p>2.更新聚簇索引列的代价很高，因为会强制InnoDB把更新的列移动到新的位置</p><p>3.基于聚簇索引的表在插入新行，或者主键被更新导致需要移动行的时候，可能会导致“页分裂”。当行的主键值要求必须将这一行插入到已满的页中时，存储引擎会将该页分裂为两个页面来容纳该行，这就是一次页分裂操作，页分裂会导致表占用更多的存储空间。</p><p>画外音：关于<strong>页</strong>，我们在上一篇文章中也提到过。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页。存和磁盘以页为单位交换数据。<strong>数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次磁盘I/O就可以完全载入</strong>。</p><p>基于聚簇索引以上的这些特点，<strong>在InnoDB中，我们应该尽量使用和应用无关的主键，例如自增主键，这样可以保证数据行是按照顺序写入的</strong>。而不是使用GUID、UUID生成随机的主键。</p><p><strong>向聚簇索引中插入顺序的索引值：</strong></p><p>每条新纪录总是在前一条记录的后面插入：</p><p><img src="/2020/02/29/dui-mysql-zhi-xing-yin-qing-innodb-de-ren-shi/5.png" alt></p><p>当页被插满后，继续插入到新的页：</p><p><img src="/2020/02/29/dui-mysql-zhi-xing-yin-qing-innodb-de-ren-shi/4.png" alt></p><p><strong>向聚簇索引中插入随机的索引值：</strong></p><p>新的记录可能被插入到之前记录的中间，导致需要强制移动之前的记录：</p><p><img src="/2020/02/29/dui-mysql-zhi-xing-yin-qing-innodb-de-ren-shi/3.png" alt></p><p>被写满且已经刷到磁盘上的页可能会被重新读取用于再次插入，此时还需要进行页分裂：</p><p><img src="/2020/02/29/dui-mysql-zhi-xing-yin-qing-innodb-de-ren-shi/2.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>MyISAM和InnoDB两个存储引擎的索引虽然都是使用的B+Tree数据结构，但是在具体实现上还是存在不小差别的。InnoDB支持聚簇索引，聚簇索引就是表，所以InnoDB不用像MyISAM那样需要独立的行存储。也就是说，InnoDB的数据文件本身就是索引文件。而MyISAM的数据文件和索引文件是分开存储的。可以通过MyISAM和InnoDB如何存放表的抽象图帮助快速理解。</p><p><strong>InnoDB（聚簇）表分布：</strong></p><p><img src="/2020/02/29/dui-mysql-zhi-xing-yin-qing-innodb-de-ren-shi/1.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>MongoDB使用及底层原理</title>
      <link href="/2020/02/17/mongodb-shi-yong-ji-di-ceng-yuan-li/"/>
      <url>/2020/02/17/mongodb-shi-yong-ji-di-ceng-yuan-li/</url>
      
        <content type="html"><![CDATA[<h2 id="mongoDB起步"><a href="#mongoDB起步" class="headerlink" title="mongoDB起步"></a>mongoDB起步</h2><ul><li><p>1.安装mongoDB</p><p>安装这里，我就不详细介绍，大家可以根据官网选择匹配自己电脑系统的版本安装即可。</p><p>mongoDB download地址： <a href="https://www.mongodb.com/download-center" target="_blank" rel="noopener">download</a></p><p>下载下来之后，点击打开直接下一步，下一步就 <code>ok</code> 了。</p></li><li><p>2.初体验</p><ol><li>配置环境变量</li></ol><p>安装完成后，会在安装目录下面生成一个 <code>mongoDB</code> 的文件夹，打开文件夹，进入 <code>bin</code> 文件夹中，把这里的路径配置成环境变量。</p><ol><li>创建存储数据库文件 <code>data</code></li></ol><p>在任意盘符根目录下创建一个 <code>data</code> 目录，用来存放数据库文件。 <code>mongoDB</code> 会自动把自己安装位置的盘符根目录下的 <code>data</code> 文件夹作为自己的数据存储目录，这里也可以直接在安装位置所在盘符创建。</p><ol><li>启动 mongoDB 数据库</li></ol><p>如果 <code>data</code> 目录创建在安装位置的盘符根目录下，直接打开命令行，敲入：</p><pre><code> mongod</code></pre><p>如果是其他位置，则需要指定数据存放的位置:</p><pre><code> mongod --dbpath 文件路径</code></pre><p>如果看到输出： <code>waiting for connections on port 27017</code> 说明启动数据库成功。</p><ol><li>连接数据库</li></ol><p>再打开一个命令行，敲入 <code>mongo</code> ，则会默认连接到本地开启的数据库。好了，到这里我们就完成了如何开启一个 <code>mongoDB</code>  数据库了，接下来只需往数据库里存数据，操作数据即可。</p></li></ul><h2 id="MongoDB-概念解析"><a href="#MongoDB-概念解析" class="headerlink" title="MongoDB 概念解析"></a>MongoDB 概念解析</h2><p><code>mongoDB</code> 作为一个 <code>NoSQL</code> 数据库，对于我们前端学习成本非常低，后期会结合node一起使用。mongoDB 中存储的都是 键值对（<code>key</code> - <code>value</code>），格式类似于 <code>JSON</code> ，操作起来也是非常爽，完全不需要我们有什么 <code>SQL</code> 语言的基础。</p><p>在<code>mongodb</code>中有三个基本核心的概念：</p><ul><li>文档</li><li>集合</li><li>数据库</li></ul><p>它们之间是逐层包含的关系，一个集合可以包含多个文档，一个数据库可以有多个集合，下面听我逐一道来：</p><p><code>文档</code> ： 文档是一个键值(<code>key-value</code>)对(即<code>BSON</code>)，本质类似于<code>json</code>对象 的键值对。</p><pre><code>{&quot;name&quot;:&quot;pubdreamcc&quot;, &quot;age&quot;: 24}</code></pre><p><code>集合</code>：集合就是 MongoDB 文档组，实质上就是包含多个对象的数组。</p><p>比如，我们可以将以下不同数据结构的文档插入到集合中：</p><pre><code>{&quot;name&quot;:&quot;pubdreamcc&quot;}{&quot;name&quot;:&quot;pubdreamcc1&quot;,&quot;name&quot;:&quot;pubdreamcc2&quot;}{&quot;name&quot;:&quot;pubdreamcc3&quot;,&quot;name&quot;:&quot;pubdreamcc4&quot;,&quot;num&quot;:5}</code></pre><p><code>数据库（dataBase）</code></p><p>这里的数据库概念同 关系型数据库中的数据库概念一致，数据库可以包含多个集合。</p><p>下面给出一张图用来表示 <code>mongoDB</code> 中的一些概念同 <code>SQL</code> 概念 的 对比，辅助理解。</p><p><img src="/2020/02/17/mongodb-shi-yong-ji-di-ceng-yuan-li/1.png" alt></p><h2 id="mongoDB基本操作"><a href="#mongoDB基本操作" class="headerlink" title="mongoDB基本操作"></a>mongoDB基本操作</h2><p>在刚才我们连接上本地数据库之后，在这个命令行，我们可以进行很多  <code>mongoDB</code>  提供的增删改查等的基本操作，且听我一一道来。</p><ul><li><p>1.创建数据库 : <code>use 数据库名称</code>。</p><p><em>如果数据库不存在，则创建数据库，否则切换到指定数据库。</em></p><p><em>MongoDB 中默认的数据库为 test，如果你没有创建新的数据库，集合将存放在 test 数据库中。</em></p></li><li><p>2.查看所有数据库： <code>show dbs</code></p></li><li><p>3.删除数据库： <code>db.dropDatabase()</code>，你可以使用 db 命令查看当前数据库名。</p></li><li><p>4.创建集合：  <code>db.createCollection(集合名称)</code></p></li><li><p>5.查看已有集合： <code>show collections</code></p></li><li><p>6.删除集合：<code>db.集合名称.drop()</code></p><p><em>如果成功删除选定集合，则 drop() 方法返回 true，否则返回 false。</em></p></li><li><p>7.插入文档：<code>db.集合名称.insert(document)</code>。往指定集合插入文档，文档的数据结构和 <code>JSON</code> 基本一样。</p></li><li><p>8.更新文档： <code>update()</code> 方法用于更新已存在的文档，语法格式：</p><pre><code>   db.collection.update(     &lt;query&gt;,     &lt;update&gt;,     {       upsert: &lt;boolean&gt;,       multi: &lt;boolean&gt;,       writeConcern: &lt;document&gt;     }   )</code></pre><p><strong>参数说明</strong>：</p><ul><li><code>query</code> : update的查询条件，类似sql update查询内where后面的。</li><li><code>update</code> : update的对象和一些更新的操作符（如<img src="https://juejin.im/equation?tex=%2C" alt=",">inc…）等，也可以理解为sql update查询内set后面的。</li><li><code>upsert</code> : 可选，这个参数的意思是，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入。</li><li><code>multi</code> : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。</li><li><code>writeConcern</code> :可选，抛出异常的级别。</li></ul></li><li><p>9.删除文档：</p><pre><code>db.collection.remove(  &lt;query&gt;,  &lt;justOne&gt;)</code></pre><p><strong>参数说明</strong></p><ul><li><code>query</code> :（可选）删除的文档的条件。</li><li><code>justOne</code> : （可选）如果设为 true 或 1，则只删除一个文档，如果不设置该参数，或使用默认值 false，则删除所有匹配条件的文档。</li><li><code>writeConcern</code> :（可选）抛出异常的级别。</li></ul></li><li><p>10.删除集合所有数据：<code>db.collection.remove({})</code></p></li><li><p>11.查询文档：<code>db.collection.find(query, projection)</code></p><p><strong>参数说明</strong></p><ul><li><code>query</code> ：可选，使用查询操作符指定查询条件。</li><li><code>projection</code> ：可选，使用投影操作符指定返回的键。查询时返回文档中所有键值， 只需省略该参数即可（默认省略）。</li></ul><p><code>PS</code>:</p><pre><code>如果你需要以易读的方式来读取数据，可以使用 `pretty()` 方法，语法格式如下： ```json db.col.find().pretty()</code></pre><pre><code>`pretty()` 方法以格式化的方式来显示所有文档。</code></pre></li><li><p>12.查询一个文档（匹配条件的第一个）：<code>db.collection.findOne()</code></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Shiro框架原理及简单使用</title>
      <link href="/2020/02/04/shiro-kuang-jia-yuan-li-ji-jian-dan-shi-yong/"/>
      <url>/2020/02/04/shiro-kuang-jia-yuan-li-ji-jian-dan-shi-yong/</url>
      
        <content type="html"><![CDATA[<h2 id="Shiro是什么？"><a href="#Shiro是什么？" class="headerlink" title="Shiro是什么？"></a><strong>Shiro是什么？</strong></h2><p>Apache Shiro是Java的一个安全框架，一个强大而灵活的开源安全框架，它干净利落地处理身份认证，授权，企业会话管理和加密。</p><p>目前，使用Apache Shiro的人越来越多，因为它相当简单，对比Spring Security，可能没有Spring Security做的功能强大，但是在实际工作时可能并不需要那么复杂的东西，所以使用小而简单的Shiro就足够了。</p><p>功能:1.验证用户来核实他们的身份2.对用户执行访问控制</p><p>我们可以看一下Apache官网上对Shiro，它都包含哪些功能。</p><p><img src="/2020/02/04/shiro-kuang-jia-yuan-li-ji-jian-dan-shi-yong/3.png" alt></p><p>Authentication：认证，有时也简称为“登录”，这是一个证明用户是他们所说的他们是谁的行为。</p><p>Authorization：授权，访问控制的过程，也就是绝对“谁”去访问“什么” 授权用于回答安全问题，例如“用户是否允许编辑帐户”，“该用户是否允许查看此网页”，“该用户是否可以访问”到这个按钮？“这些都是决定用户有权访问的决定，因此都代表授权检查。</p><p>Cryptography：密码术是通过隐藏信息或将其转换为无意义来保护信息免受不良访问的做法，因此没有其他人可以阅读它。Shiro专注于密码学的两个核心要素：使用公钥或私钥加密数据的密码，以及对密码等数据进行不可逆转加密的哈希（也称为消息摘要）。</p><p>Shiro Cryptography的主要目标是采用传统上非常复杂的领域，并在提供强大的密码学功能的同时使其他人轻松实现。</p><p>Session Management：Session会话，会话是您的用户在使用您的应用程序时携带一段时间的数据桶。传统上，会话专用于Web或EJB环境。不再！Shiro支持任何应用程序环境的会话。此外，Shiro还提供许多其他强大功能来帮助您管理会话。</p><p>Web Support：Shiro的web支持的API能够轻松地帮助保护 Web 应用程序。主要就是用来对Web程序进行一个好的支持的。</p><p>Caching：缓存，他是Apache Shiro中的第一层公民，来确保安全操作快速而又高效。</p><p>Concurrency：shiro利用它的并发特性来支持多线程应用程序。</p><p>Testing：测试支持的存在来帮助你编写单元测试和集成测试，并确保你的能够如预期的一样安全。</p><p>“Run As”：其实这个就是有是有允许一个用户假设为另外一个用户身份的功能，有时候在管理脚本的时候很有效果。</p><p>Remember Me：在会话中记住用户的身份，所以他们只需要在强制时候登录。</p><h2 id="Shiro核心"><a href="#Shiro核心" class="headerlink" title="Shiro核心"></a>Shiro核心</h2><p>Shiro其实是有三大核心组件的，Subject、SecurityManager和Realms。</p><p>Subject：Subject实质上是一个当前执行用户的特定的安全“视图”。鉴于”User”一词通常意味着一个人，而一个Subject可以是一个人， 但它还可以代表第三方服务，daemon account，cron job，或其他类似的任何东西——基本上是当前正与软件进行交互的任何东西。 </p><p>所有Subject实例都被绑定到（且这是必须的）一个SecurityManager上。当你与一个Subject交互时，那些交互作用转化为与SecurityManager交互的特定subject的交互作用。 我们可以把Subject认为是一个门面，SecurityManager才是真正的执行者。</p><p>SecurityManager：安全管理器，也就是说所有与安全有关的操作都会与SecurityManager进行交互，而且他管理这Subject，它其实是Shiro的核心 是Shiro架构的心脏。并作为一种“保护伞”对象来协调内部的安全组件共同构成一个对象图。</p><p>Realms： 域，Shiro从从Realm获取安全数据（如用户、角色、权限），就是说SecurityManager要验证用户身份，那么它需要从Realm获取相应的用户进行比较以确定用户身份是否合法；也需要从Realm得到用户相应的角色/权限进行验证用户是否能进行操作；可以把Realm看成DataSource，即安全数据源。</p><p>当配置Shiro时，你必须指定至少一个Realm用来进行身份验证和/或授权。SecurityManager可能配置多个Realms，但至少有一个是必须的。</p><p><img src="/2020/02/04/shiro-kuang-jia-yuan-li-ji-jian-dan-shi-yong/1.png" alt></p><p><strong>我们看一下它具体的登录图解</strong></p><p><strong><img src="/2020/02/04/shiro-kuang-jia-yuan-li-ji-jian-dan-shi-yong/2.png" alt></strong></p><hr><p><strong>1、登陆操作 携带用户名密码给subject，subject调用自己的登陆方法传递用户名和密码给权限管理器，权限管理器将用户名密码传递给开发人员编写的realm的认证方法，realm根据用户名到数据库中查找是否存在该用户，若存在将认证信息存入到session中</strong></p><p><strong>2、权限管理器会自动判断传递的密码与正确密码是否一致</strong></p><p><strong>3、访问3类资源（页面） 过滤器寻找权限管理器判断该用户是否拥有xxx权限，权限管理器从session中取出认证信息对象，返回给realm，realm判断该用户拥有什么权限，封装到授权信息中返回给权限管理器，权限管理器将判断的结果返回给过滤器</strong></p><p><strong>4、访问3类资源（xxx添加需要访问service）（对于过滤器来说属于2类资源），在执行方法时，会到达前置通知（esrvice方法上添加注解@RequiresPermissions(“courier:list”)），权限通知寻找权限管理器判断该用户是否拥有xxx权限，权限管理器从session中取出认证信息对象，返回给realm，realm判断该用户拥有什么权限，封装到授权信息中返回给权限管理器，权限管理器将判断的结果返回给权限通知</strong></p><p><strong>其实简单来说 /userAction_login ———-&gt;请求先到达权限过滤器shiroFilter，先判断是几类资源</strong></p><p><strong>登录属于一类资源直接放行到————&gt;userActon中（userAction中调用执行subject对象（使用入口是一个操作入口对象，里面有登陆方法，登出方法，获取当前对象方法）的登陆方法subject.login方法（携带着用户名，密码）</strong></p><p><strong>————&gt;subject对象调用 securityManager的login方法 权限管理器不能判断用户和密码是对的需要</strong></p><p><strong>————&gt;ream认证|授权器（开发人员编写，判断用户名是否存在，拥有什么权限）————&gt;处理完后把认证信息对象返回给securityManager（）如果认证信息没有问题，权限管理器会把认证信息存入session（证明认证登陆过了）</strong></p><p><strong>可以自定义一个Realm；</strong></p><pre><code>public class LoginRelam extends AuthorizingRealm {    @Autowired    private UserDao userDao;    @Override    protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principalCollection) {        return null;    }    @Override    protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException {        UsernamePasswordToken upToken = (UsernamePasswordToken) authenticationToken;        String username = upToken.getUsername();        User user = userDao.QueryUser(username);        if(user == null){            throw new UnknownAccountException(&quot;不存在用户名&quot;);        }        return new SimpleAuthenticationInfo(user.getUsername(),user.getPassword(),getName());    }}</code></pre><p><strong>登陆完以后访问页面资源（页面资源属于三类资源需要权限），</strong></p><p><strong>shiroFilter（已经配置了哪些资源是一类哪些资源是三类）</strong></p><p><strong>————&gt;访问权限管理器，找权限管理器判断是否有xxx权限（权限管理器本身不能做出判断），权限管理器把之前登陆时保存在session中的认证信息取出</strong></p><p><strong>交给————&gt;realm判断(realm中认证方法是登陆时候调用的)，realm查询数据库获得权限，把权限信息返还给————&gt;权限管理器。</strong></p><p><strong>权限管理器根据realm的授权信息判断是否拥有xxx权限， 判断后把结果通知给————&gt;权限管理器，权限管理器ShiraFilter 如果没有权限跳转到响应页面。</strong></p><h2 id="Shiro继承Spring"><a href="#Shiro继承Spring" class="headerlink" title="Shiro继承Spring"></a>Shiro继承Spring</h2><pre><code>导入jar包（shiro的jar有很多，针对不同的项目导入不同的jar包，但是为了防止第一次学习的时候出错，所有使用的是shiro-all-版本号.jar的jar包）；</code></pre><p><strong>步骤一： 在web.xml中配置一个过滤器，是由spring提供的，用于整合shiro：</strong><br><strong>web.xml文件（一定要注意配置shiro框架以及Spring，Struts之间的顺序问题，否则报错！</strong></p><pre><code>&lt;!DOCTYPE web-app PUBLIC        &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot;        &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt;&lt;web-app&gt;    &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt;    &lt;!-- 配置前端控制器：服务器启动必须加载，需要加载springmvc.xml配置文件 --&gt;    &lt;servlet&gt;        &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt;        &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet        &lt;/servlet-class&gt;        &lt;!-- 配置初始化参数，创建完DispatcherServlet对象，加载springmvc.xml配置文件 --&gt;        &lt;init-param&gt;            &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;            &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt;        &lt;/init-param&gt;        &lt;!-- 服务器启动的时候，让DispatcherServlet对象创建 --&gt;        &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;    &lt;/servlet&gt;    &lt;servlet-mapping&gt;        &lt;servlet-name&gt;dispatcherServlet&lt;/servlet-name&gt;        &lt;url-pattern&gt;/&lt;/url-pattern&gt;    &lt;/servlet-mapping&gt;    &lt;!-- 配置Spring的监听器 --&gt;    &lt;listener&gt;        &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;    &lt;/listener&gt;    &lt;!-- 配置加载类路径的配置文件 --&gt;    &lt;context-param&gt;        &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;        &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt;    &lt;/context-param&gt;    &lt;!-- 配置解决中文乱码的过滤器 --&gt;    &lt;filter&gt;        &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt;        &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt;        &lt;init-param&gt;            &lt;param-name&gt;encoding&lt;/param-name&gt;            &lt;param-value&gt;UTF-8&lt;/param-value&gt;        &lt;/init-param&gt;    &lt;/filter&gt;    &lt;filter-mapping&gt;        &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt;        &lt;url-pattern&gt;/*&lt;/url-pattern&gt;    &lt;/filter-mapping&gt;    &lt;!-- 配置Shiro--&gt;    &lt;filter&gt;        &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt;        &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt;        &lt;init-param&gt;            &lt;param-name&gt;targetFilterLifecycle&lt;/param-name&gt;            &lt;param-value&gt;true&lt;/param-value&gt;        &lt;/init-param&gt;    &lt;/filter&gt;    &lt;filter-mapping&gt;        &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt;        &lt;url-pattern&gt;/*&lt;/url-pattern&gt;    &lt;/filter-mapping&gt;&lt;/web-app&gt;</code></pre><p><strong>步骤二： 在applicationContext.xml中配置bean，ID必须为shiroFilter：</strong></p><p><strong>applicationContext.xml文件配置</strong><br><strong>shiro 框架由于大量的使用了代理模式，所以在使用的过程中如果配置不当，可能会出现问题，另外在使用注解开发时候尽量的使用Spring的注解，不要使用JDK自带的原生注解，减少出错的几率</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;       xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;       xmlns:task=&quot;http://www.springframework.org/schema/task&quot;       xmlns:context=&quot;http://www.springframework.org/schema/context&quot;       xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans          http://www.springframework.org/schema/beans/spring-beans.xsd           http://www.springframework.org/schema/context           http://www.springframework.org/schema/context/spring-context.xsd            http://www.springframework.org/schema/task http://www.springframework.org/schema/task/spring-task-3.0.xsd&quot;&gt;    &lt;!-- 开启注解扫描，要扫描的是service和dao层的注解，要忽略web层注解，因为web层让SpringMVC框架 去管理 --&gt;    &lt;context:component-scan base-package=&quot;cqu&quot;&gt;        &lt;!-- 配置要忽略的注解 --&gt;        &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Controller&quot;/&gt;    &lt;/context:component-scan&gt;    &lt;!--    1. 配置 SecurityManager!    --&gt;    &lt;bean id=&quot;securityManager&quot; class=&quot;org.apache.shiro.web.mgt.DefaultWebSecurityManager&quot;&gt;        &lt;property name=&quot;cacheManager&quot; ref=&quot;cacheManager&quot;/&gt;        &lt;property name=&quot;realm&quot; ref=&quot;loginRealm&quot;/&gt;    &lt;/bean&gt;    &lt;!--    2. 配置 CacheManager.    2.1 需要加入 ehcache 的 jar 包及配置文件.    --&gt;    &lt;bean id=&quot;cacheManager&quot; class=&quot;org.apache.shiro.cache.ehcache.EhCacheManager&quot;&gt;        &lt;property name=&quot;cacheManagerConfigFile&quot; value=&quot;classpath:ehcache.xml&quot;/&gt;    &lt;/bean&gt;    &lt;!-- 配置C3P0的连接池对象 --&gt;    &lt;bean id=&quot;dataSource&quot; class=&quot;org.springframework.jdbc.datasource.DriverManagerDataSource&quot;&gt;        &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt;        &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql:///ssm&quot;/&gt;        &lt;property name=&quot;username&quot; value=&quot;root&quot;/&gt;        &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt;    &lt;/bean&gt;    &lt;!--    3. 配置 Realm    3.1 直接配置实现了 org.apache.shiro.realm.Realm 接口的 bean    --&gt;    &lt;bean id=&quot;loginRealm&quot; class=&quot;cqu.controller.LoginRelam&quot;&gt;&lt;/bean&gt;    &lt;!--  4. 配置 LifecycleBeanPostProcessor. 可以自定的来调用配置在 Spring IOC 容器中 shiro bean 的生命周期方法.    --&gt;    &lt;bean id=&quot;lifecycleBeanPostProcessor&quot; class=&quot;org.apache.shiro.spring.LifecycleBeanPostProcessor&quot;/&gt;    &lt;!--    5. 启用 IOC 容器中使用 shiro 的注解. 但必须在配置了 LifecycleBeanPostProcessor 之后才可以使用.    --&gt;    &lt;bean class=&quot;org.springframework.aop.framework.autoproxy.DefaultAdvisorAutoProxyCreator&quot;          depends-on=&quot;lifecycleBeanPostProcessor&quot;/&gt;    &lt;bean class=&quot;org.apache.shiro.spring.security.interceptor.AuthorizationAttributeSourceAdvisor&quot;&gt;        &lt;property name=&quot;securityManager&quot; ref=&quot;securityManager&quot;/&gt;    &lt;/bean&gt;    &lt;!--    6. 配置 ShiroFilter.    6.1 id 必须和 web.xml 文件中配置的 DelegatingFilterProxy 的 &lt;filter-name&gt; 一致.                      若不一致, 则会抛出: NoSuchBeanDefinitionException. 因为 Shiro 会来 IOC 容器中查找和 &lt;filter-name&gt; 名字对应的 filter bean.    --&gt;    &lt;bean id=&quot;shiroFilter&quot; class=&quot;org.apache.shiro.spring.web.ShiroFilterFactoryBean&quot;&gt;        &lt;property name=&quot;securityManager&quot; ref=&quot;securityManager&quot;/&gt;        &lt;property name=&quot;loginUrl&quot; value=&quot;/index.jsp&quot;/&gt;        &lt;property name=&quot;successUrl&quot; value=&quot;/list.jsp&quot;/&gt;        &lt;property name=&quot;unauthorizedUrl&quot; value=&quot;/unauthorized.jsp&quot;/&gt;        &lt;!--            配置哪些页面需要受保护.            以及访问这些页面需要的权限.            1). anon 可以被匿名访问            2). authc 必须认证(即登录)后才可能访问的页面.            3). logout 登出.            4). roles 角色过滤器        --&gt;        &lt;property name=&quot;filterChainDefinitions&quot;&gt;            &lt;value&gt;                /index.jsp = anon                /css/** = anon                /js/** = anon                /images/** = anon                /media/** = anon                /res/** = anon                /index.css = anon                # everything else requires authentication:                /* = authc            &lt;/value&gt;        &lt;/property&gt;    &lt;/bean&gt;    &lt;!-- 配置SqlSession的工厂 --&gt;    &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt;        &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt;    &lt;/bean&gt;        &lt;!-- 配置扫描dao的包 --&gt;    &lt;bean id=&quot;mapperScanner&quot; class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt;        &lt;property name=&quot;basePackage&quot; value=&quot;cqu.dao&quot;/&gt;    &lt;/bean&gt;    &lt;bean id=&quot;taskExecutor&quot; class=&quot;org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor&quot;&gt;        &lt;!-- 核心线程数 --&gt;        &lt;property name=&quot;corePoolSize&quot; value=&quot;5&quot; /&gt;        &lt;!-- 最大线程数 --&gt;        &lt;property name=&quot;maxPoolSize&quot; value=&quot;10&quot; /&gt;        &lt;!-- 队列最大长度 --&gt;        &lt;property name=&quot;queueCapacity&quot; value=&quot;20&quot; /&gt;        &lt;!-- 线程池维护线程所允许的空闲时间，默认为60s --&gt;        &lt;property name=&quot;keepAliveSeconds&quot; value=&quot;60&quot; /&gt;    &lt;/bean&gt;    &lt;!-- 注解式 --&gt;    &lt;task:annotation-driven /&gt;&lt;/beans&gt;</code></pre><p><strong>步骤三： 登录系统使用shrio框架管理，修改Action中login方法：</strong></p><pre><code> Subject currentUser = SecurityUtils.getSubject(); if(!currentUser.isAuthenticated()){     UsernamePasswordToken token = new UsernamePasswordToken(username, password);     token.setRememberMe(true); try {     currentUser.login(token); } catch (AuthenticationException e) {     return null; }}</code></pre><p><strong>步骤四： 开发属于自己的realm类：</strong></p><pre><code>public class LoginRelam extends AuthorizingRealm {    @Autowired    private UserDao userDao;    @Override    protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException {        UsernamePasswordToken upToken = (UsernamePasswordToken) authenticationToken;        String username = upToken.getUsername();        User user = userDao.QueryUser(username);        if(user == null){            throw new UnknownAccountException(&quot;不存在用户名&quot;);        }        return new SimpleAuthenticationInfo(user.getUsername(),user.getPassword(),getName());    }}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><strong>Shiro是一个功能很齐全的框架，使用起来也很容易，总结一下 三大核心内容：</strong></p><p><strong>1.Subject2.SecurityManager3.Realms</strong></p><p><strong>Shiro 功能强大、且 简单、灵活。是Apache 下的项目比较可靠，且不跟任何的框架或者容器绑定，可以独立运行(JavaSE环境也可以使用)</strong></p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>HTTP请求方法的幂等性</title>
      <link href="/2020/02/02/http-qing-qiu-fang-fa-de-mi-deng-xing/"/>
      <url>/2020/02/02/http-qing-qiu-fang-fa-de-mi-deng-xing/</url>
      
        <content type="html"><![CDATA[<h2 id="怎么理解幂等性"><a href="#怎么理解幂等性" class="headerlink" title="怎么理解幂等性"></a>怎么理解幂等性</h2><p>HTTP幂等方法，是指无论调用多少次都不会有不同结果的 HTTP 方法。不管你调用一次，还是调用一百次，一千次，结果都是相同的。</p><p>还是以之前的博文的例子为例。</p><pre><code>GET     /tickets       # 获取ticket列表GET     /tickets/12    # 查看某个具体的ticketPOST    /tickets       # 新建一个ticketPUT     /tickets/12    # 更新ticket 12PATCH   /tickets/12    # 更新ticket 12DELETE  /tickets/12    # 删除ticekt 12</code></pre><h3 id="HTTP-GET方法"><a href="#HTTP-GET方法" class="headerlink" title="HTTP GET方法"></a>HTTP GET方法</h3><p>HTTP GET方法，用于获取资源，不管调用多少次接口，结果都不会改变，所以是幂等的。</p><pre><code>GET     /tickets       # 获取ticket列表GET     /tickets/12    # 查看某个具体的ticket</code></pre><p>只是查询数据，不会影响到资源的变化，因此我们认为它幂等。</p><p>值得注意，幂等性指的是作用于结果而非资源本身。怎么理解呢？例如，这个HTTP GET方法可能会每次得到不同的返回内容，但并不影响资源。</p><p>可能你会问有这种情况么？当然有咯。例如，我们有一个接口获取当前时间，我们就应该设计成</p><pre><code>GET     /service_time # 获取服务器当前时间</code></pre><p>它本身不会对资源本身产生影响，因此满足幂等性。</p><h3 id="HTTP-POST方法"><a href="#HTTP-POST方法" class="headerlink" title="HTTP POST方法"></a>HTTP POST方法</h3><p>HTTP POST方法是一个非幂等方法，因为调用多次，都将产生新的资源。</p><pre><code>POST    /tickets       # 新建一个ticket</code></pre><p>因为它会对资源本身产生影响，每次调用都会有新的资源产生，因此不满足幂等性。</p><h3 id="HTTP-PUT方法"><a href="#HTTP-PUT方法" class="headerlink" title="HTTP PUT方法"></a>HTTP PUT方法</h3><p>HTTP PUT方法是不是幂等的呢？我们来看下</p><pre><code>PUT     /tickets/12    # 更新ticket 12</code></pre><p>因为它直接把实体部分的数据替换到服务器的资源，我们多次调用它，只会产生一次影响，但是有相同结果的 HTTP 方法，所以满足幂等性。</p><h3 id="HTTP-PATCH方法"><a href="#HTTP-PATCH方法" class="headerlink" title="HTTP PATCH方法"></a>HTTP PATCH方法</h3><p>HTTP PATCH方法是非幂等的。HTTP POST方法和HTTP PUT方法可能比较好理解，但是HTTP PATCH方法只是更新部分资源，怎么是非幂等的呢?</p><p>因为，PATCH提供的实体则需要根据程序或其它协议的定义，解析后在服务器上执行，以此来修改服务器上的资源。换句话说，PATCH请求是会执行某个程序的，如果重复提交，程序可能执行多次，对服务器上的资源就可能造成额外的影响，这就可以解释它为什么是非幂等的了。</p><p>可能你还不能理解这点。我们举个例子</p><pre><code>PATCH   /tickets/12    # 更新ticket 12</code></pre><p>此时，我们服务端对方法的处理是，当调用一次方法，更新部分字段，将这条ticket记录的操作记录加一，这次，每次调用的资源是不是变了呢，所以它是有可能是非幂等的操作。</p><h3 id="HTTP-DELETE方法"><a href="#HTTP-DELETE方法" class="headerlink" title="HTTP DELETE方法"></a>HTTP DELETE方法</h3><p>HTTP DELETE方法用于删除资源，会将资源删除。</p><pre><code>DELETE  /tickets/12    # 删除ticekt 12</code></pre><p>调用一次和多次对资源产生影响是相同的，所以也满足幂等性。</p><h2 id="如何设计符合幂等性的高质量RESTful-API"><a href="#如何设计符合幂等性的高质量RESTful-API" class="headerlink" title="如何设计符合幂等性的高质量RESTful API"></a>如何设计符合幂等性的高质量RESTful API</h2><h3 id="HTTP-GET方法-vs-HTTP-POST方法"><a href="#HTTP-GET方法-vs-HTTP-POST方法" class="headerlink" title="HTTP GET方法 vs HTTP POST方法"></a>HTTP GET方法 vs HTTP POST方法</h3><p>也许，你会想起一个面试题。HTTP请求的GET与POST方式有什么区别？你可能会回答到：GET方式通过URL提交数据，数据在URL中可以看到；POST方式，数据放置在HTML HEADER内提交。但是，我们现在从RESTful的资源角度来看待问题，HTTP GET方法是幂等的，所以它适合作为查询操作，HTTP POST方法是非幂等的，所以用来表示新增操作。</p><p>但是，也有例外，我们有的时候可能需要把查询方法改造成HTTP POST方法。比如，超长（1k）的GET URL使用POST方法来替代，因为GET受到URL长度的限制。虽然，它不符合幂等性，但是它是一种折中的方案。</p><h3 id="HTTP-POST方法-vs-HTTP-PUT方法"><a href="#HTTP-POST方法-vs-HTTP-PUT方法" class="headerlink" title="HTTP POST方法 vs HTTP PUT方法"></a>HTTP POST方法 vs HTTP PUT方法</h3><p>对于HTTP POST方法和TTP PUT方法，我们一般的理解是POST表示创建资源，PUT表示更新资源。当然，这个是正确的理解。</p><p>但是，实际上，两个方法都用于创建资源，更为本质的差别是在幂等性。HTTP POST方法是非幂等，所以用来表示创建资源，HTTP PUT方法是幂等的，因此表示更新资源更加贴切。</p><h3 id="HTTP-PUT方法-vs-HTTP-PATCH方法"><a href="#HTTP-PUT方法-vs-HTTP-PATCH方法" class="headerlink" title="HTTP PUT方法 vs HTTP PATCH方法"></a>HTTP PUT方法 vs HTTP PATCH方法</h3><p>此时，你看会有另外一个问题。HTTP PUT方法和HTTP PATCH方法，都是用来表述更新资源，它们之间有什么区别呢？我们一般的理解是PUT表示更新全部资源，PATCH表示更新部分资源。首先，这个是我们遵守的第一准则。根据上面的描述，PATCH方法是非幂等的，因此我们在设计我们服务端的RESTful API的时候，也需要考虑。如果，我们想要明确的告诉调用者我们的资源是幂等的，我的设计更倾向于使用HTTP PUT方法</p>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>对Mysql读写分离的认识</title>
      <link href="/2020/01/26/dui-mysql-du-xie-fen-chi-de-ren-shi/"/>
      <url>/2020/01/26/dui-mysql-du-xie-fen-chi-de-ren-shi/</url>
      
        <content type="html"><![CDATA[<h3 id="读写分离应用场景"><a href="#读写分离应用场景" class="headerlink" title="读写分离应用场景"></a>读写分离应用场景</h3><p>因为用户的增多，数据的增多，单机的数据库往往支撑不住快速发展的业务，所以数据库集群就产生了！今天来说说读写分离的数据库集群方式！读写分离顾名思义就是读和写分离了，对应到数据库集群一般都是一主一从(一个主库，一个从库)或者一主多从(一个主库，多个从库)，业务服务器把需要写的操作都写到主数据库中，读的操作都去从库查询。主库会同步数据到从库保证数据的一致性。</p><p><img src="/2020/01/26/dui-mysql-du-xie-fen-chi-de-ren-shi/1.png" alt></p><p>把访问的压力从主库转移到从库</p><p>写的操作很多的话不适合这种集群方式</p><p>在单机的情况下，一般我们做数据库优化都会加索引，但是加了索引对查询有优化，但是会影响写入，因为写入数据会更新索引。所以做了<strong>主从之后，我们可以单独的针对从库(读库)做索引上的优化，而主库(写库)可以减少索引而提高写的效率。</strong></p><p>看起来还是很简单的，但是有两点要注意：<strong>主从同步延迟、分配机制的考虑</strong>；</p><h3 id="主从同步延迟"><a href="#主从同步延迟" class="headerlink" title="主从同步延迟"></a>主从同步延迟</h3><p>主库有数据写入之后，同时也写入在binlog(二进制日志文件)中，从库是通过binlog文件来同步数据的，这期间会有一定时间的延迟，可能是1秒，如果同时有大量数据写入的话，时间可能更长。</p><p>这会导致什么问题呢？比如有一个付款操作，你付款了，主库是已经写入数据，但是查询是到从库查，从库里还没有你的付款记录，所以页面上查询的时候你还没付款。那可不急眼了啊，吞钱了这还了得！打电话给客服投诉！</p><p>所以为了解决主从同步延迟的问题有以下几个方法：</p><h4 id="1、二次读取"><a href="#1、二次读取" class="headerlink" title="1、二次读取"></a>1、二次读取</h4><p>二次读取的意思就是读从库没读到之后再去主库读一下，只要通过对数据库访问的API进行封装就能实现这个功能。很简单，并且和业务之间没有耦合。但是有个问题，如果有很多二次读取相当于压力还是回到了主库身上，等于读写分离白分了。而且如有人恶意攻击，就一直访问没有的数据，那主库就可能爆了。</p><h4 id="2、写之后的马上的读操作访问主库"><a href="#2、写之后的马上的读操作访问主库" class="headerlink" title="2、写之后的马上的读操作访问主库"></a>2、写之后的马上的读操作访问主库</h4><p>也就是写操作之后，立马的读操作指定访问主库，之后的读操作采取访问从库。这就等于写死了，和业务强耦合了。</p><h4 id="3、关键业务读写都由主库承担，非关键业务读写分离"><a href="#3、关键业务读写都由主库承担，非关键业务读写分离" class="headerlink" title="3、关键业务读写都由主库承担，非关键业务读写分离"></a>3、关键业务读写都由主库承担，非关键业务读写分离</h4><p>类似付钱的这种业务，读写都到主库，避免延迟的问题，但是例如改个头像啊，个人签名这种比较不重要的就读写分离，查询都去从库查，毕竟延迟一下影响也不大，不会立马打客服电话哈哈。</p><h3 id="分配机制的考虑"><a href="#分配机制的考虑" class="headerlink" title="分配机制的考虑"></a>分配机制的考虑</h3><p>分配机制的考虑也就是怎么制定写操作是去主库写，读操作是去从库读。</p><p>一般有两种方式：<strong>代码封装、数据库中间件</strong>。</p><p>1、代码封装代码封装的实现很简单，就是抽出一个中间层，让这个中间层来实现读写分离和数据库连接。讲白点就是搞个provider封装了save,select等通常数据库操作，内部save操作的dataSource是主库的，select操作的dataSource是从库的。</p><p>优点：就是实现简单，并且可以根据业务定制化变化，随心所欲。</p><p>缺点：就是是如果哪个数据库宕机了，发生主从切换了之后，就得修改配置重启。并且如果你的系统很大，一个业务可能包含多个子系统，一个子系统是java写的一个子系统用go写的，这样的话得分别为不同语言实现一套中间层，重复开发。</p><p><img src="/2020/01/26/dui-mysql-du-xie-fen-chi-de-ren-shi/2.png" alt="代码封装数据访问层"></p><p>2、数据库中间件就是有一个独立的系统，专门来实现读写分离和数据库连接管理，业务服务器和数据库中间件之间是通过标准的SQL协议交流的，所以在业务服务器看来数据库中间件其实就是个数据库。</p><p>优点：因为是通过sql协议的所以可以兼容不同的语言不需要单独写一套，并且有中间件来实现主从切换，业务服务器不需要关心这点。</p><p>缺点：多了一个系统其实就等于多了一个关心。。如果数据库中间件挂了的话对吧，而且多了一个系统就等于多了一个瓶颈，所以对中间件的性能要求也高，并且所有的数据库操作都要经过它。并且中间件实现很复杂，难度比代码封装高多了。</p><p>但是有开源的数据库中间件例如Mysql Proxy,Mysql Route,Atlas。</p><p><img src="/2020/01/26/dui-mysql-du-xie-fen-chi-de-ren-shi/3.png" alt="数据库中间件"></p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>读写分离相对而言是比较简单的，比分表分库简单，但是它只能分担访问的压力，分担不了存储的压力，也就是你的数据库表的数据逐渐增多，但是面对一张表海量的数据，查询还是很慢的，所以如果业务发展的快数据暴增，到一定时间还是得分库分表。</p><p>但是正常情况下，只要当单机真的顶不住压力了才会集群，<strong>不要一上来就集群</strong>，没这个必要。有关于软件的东西都是越简单越好，复杂都是形势所迫。</p><p><strong>一般我们是先优化，优化一些慢查询，优化业务逻辑的调用或者加入缓存等，如果真的优化到没东西优化了然后才上集群，先读写分离，读写分离之后顶不住就再分库分表。</strong></p>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>HashMap与ConcurrentHashMap</title>
      <link href="/2020/01/23/hashmap-yu-concurrenthashmap/"/>
      <url>/2020/01/23/hashmap-yu-concurrenthashmap/</url>
      
        <content type="html"><![CDATA[<h2 id="HashMap"><a href="#HashMap" class="headerlink" title="HashMap"></a>HashMap</h2><p>众所周知 HashMap 底层是基于 <code>数组 + 链表</code> 组成的，不过在 jdk1.7 和 1.8 中具体实现稍有不同。</p><h3 id="Base-1-7"><a href="#Base-1-7" class="headerlink" title="Base 1.7"></a>Base 1.7</h3><p>1.7 中的数据结构图：</p><p><img src="/2020/01/23/hashmap-yu-concurrenthashmap/1.png" alt></p><p>先来看看 1.7 中的实现。</p><p><img src="/2020/01/23/hashmap-yu-concurrenthashmap/2.png" alt></p><p>这是 HashMap 中比较核心的几个成员变量；看看分别是什么意思？</p><ol><li>初始化桶大小，因为底层是数组，所以这是数组默认的大小。</li><li>桶最大值。</li><li>默认的负载因子（0.75）</li><li><code>table</code> 真正存放数据的数组。</li><li><code>Map</code> 存放数量的大小。</li><li>桶大小，可在初始化时显式指定。</li><li>负载因子，可在初始化时显式指定。</li></ol><p>重点解释下负载因子：</p><p>由于给定的 HashMap 的容量大小是固定的，比如默认初始化：</p><pre><code>    public HashMap() {        this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR);    }    public HashMap(int initialCapacity, float loadFactor) {        if (initialCapacity &lt; 0)            throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; +                                               initialCapacity);        if (initialCapacity &gt; MAXIMUM_CAPACITY)            initialCapacity = MAXIMUM_CAPACITY;        if (loadFactor &lt;= 0 || Float.isNaN(loadFactor))            throw new IllegalArgumentException(&quot;Illegal load factor: &quot; +                                               loadFactor);        this.loadFactor = loadFactor;        threshold = initialCapacity;        init();    }</code></pre><p>给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 <code>16 * 0.75 = 12</code> 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。</p><p>因此通常建议能提前预估 HashMap 的大小最好，尽量的减少扩容带来的性能损耗。</p><p>根据代码可以看到其实真正存放数据的是</p><p><code>transient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE;</code></p><p>这个数组，那么它又是如何定义的呢？</p><p><img src="/2020/01/23/hashmap-yu-concurrenthashmap/3.png" alt></p><p>Entry 是 HashMap 中的一个内部类，从他的成员变量很容易看出：</p><ul><li>key 就是写入时的键。</li><li>value 自然就是值。</li><li>开始的时候就提到 HashMap 是由数组和链表组成，所以这个 next 就是用于实现链表结构。</li><li>hash 存放的是当前 key 的 hashcode。</li></ul><p>知晓了基本结构，那来看看其中重要的写入、获取函数：</p><h4 id="put-方法"><a href="#put-方法" class="headerlink" title="put 方法"></a>put 方法</h4><pre><code>    public V put(K key, V value) {        if (table == EMPTY_TABLE) {            inflateTable(threshold);        }        if (key == null)            return putForNullKey(value);        int hash = hash(key);        int i = indexFor(hash, table.length);        for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) {            Object k;            if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) {                V oldValue = e.value;                e.value = value;                e.recordAccess(this);                return oldValue;            }        }        modCount++;        addEntry(hash, key, value, i);        return null;    }</code></pre><ul><li>判断当前数组是否需要初始化。</li><li>如果 key 为空，则 put 一个空值进去。</li><li>根据 key 计算出 hashcode。</li><li>根据计算出的 hashcode 定位出所在桶。</li><li>如果桶是一个链表则需要遍历判断里面的 hashcode、key 是否和传入 key 相等，如果相等则进行覆盖，并返回原来的值。</li><li>如果桶是空的，说明当前位置没有数据存入；新增一个 Entry 对象写入当前位置。</li></ul><pre><code>    void addEntry(int hash, K key, V value, int bucketIndex) {        if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) {            resize(2 * table.length);            hash = (null != key) ? hash(key) : 0;            bucketIndex = indexFor(hash, table.length);        }        createEntry(hash, key, value, bucketIndex);    }    void createEntry(int hash, K key, V value, int bucketIndex) {        Entry&lt;K,V&gt; e = table[bucketIndex];        table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e);        size++;    }</code></pre><p>当调用 addEntry 写入 Entry 时需要判断是否需要扩容。</p><p>如果需要就进行两倍扩充，并将当前的 key 重新 hash 并定位。</p><p>而在 <code>createEntry</code> 中会将当前位置的桶传入到新建的桶中，如果当前桶有值就会在位置形成链表。</p><h4 id="get-方法"><a href="#get-方法" class="headerlink" title="get 方法"></a>get 方法</h4><p>再来看看 get 函数：</p><pre><code>    public V get(Object key) {        if (key == null)            return getForNullKey();        Entry&lt;K,V&gt; entry = getEntry(key);        return null == entry ? null : entry.getValue();    }    final Entry&lt;K,V&gt; getEntry(Object key) {        if (size == 0) {            return null;        }        int hash = (key == null) ? 0 : hash(key);        for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)];             e != null;             e = e.next) {            Object k;            if (e.hash == hash &amp;&amp;                ((k = e.key) == key || (key != null &amp;&amp; key.equals(k))))                return e;        }        return null;    }</code></pre><ul><li>首先也是根据 key 计算出 hashcode，然后定位到具体的桶中。</li><li>判断该位置是否为链表。</li><li>不是链表就根据 <code>key、key 的 hashcode</code> 是否相等来返回值。</li><li>为链表则需要遍历直到 key 及 hashcode 相等时候就返回值。</li><li>啥都没取到就直接返回 null 。</li></ul><h3 id="Base-1-8"><a href="#Base-1-8" class="headerlink" title="Base 1.8"></a>Base 1.8</h3><p>不知道 1.7 的实现大家看出需要优化的点没有？</p><p>其实一个很明显的地方就是：</p><blockquote><p>当 Hash 冲突严重时，在桶上形成的链表会变的越来越长，这样在查询时的效率就会越来越低；时间复杂度为 <code>O(N)</code>。</p></blockquote><p>因此 1.8 中重点优化了这个查询效率。</p><p>1.8 HashMap 结构图：</p><p><img src="/2020/01/23/hashmap-yu-concurrenthashmap/4.png" alt></p><p>先来看看几个核心的成员变量：</p><pre><code>    static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16    /**     * The maximum capacity, used if a higher value is implicitly specified     * by either of the constructors with arguments.     * MUST be a power of two &lt;= 1&lt;&lt;30.     */    static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;    /**     * The load factor used when none specified in constructor.     */    static final float DEFAULT_LOAD_FACTOR = 0.75f;    static final int TREEIFY_THRESHOLD = 8;    transient Node&lt;K,V&gt;[] table;    /**     * Holds cached entrySet(). Note that AbstractMap fields are used     * for keySet() and values().     */    transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet;    /**     * The number of key-value mappings contained in this map.     */    transient int size;</code></pre><p>和 1.7 大体上都差不多，还是有几个重要的区别：</p><ul><li><code>TREEIFY_THRESHOLD</code> 用于判断是否需要将链表转换为红黑树的阈值。</li><li>HashEntry 修改为 Node。</li></ul><p>Node 的核心组成其实也是和 1.7 中的 HashEntry 一样，存放的都是 <code>key value hashcode next</code> 等数据。</p><p>再来看看核心方法。</p><h4 id="put-方法-1"><a href="#put-方法-1" class="headerlink" title="put 方法"></a>put 方法</h4><p><img src="/2020/01/23/hashmap-yu-concurrenthashmap/5.png" alt></p><p>看似要比 1.7 的复杂，我们一步步拆解：</p><ol><li>判断当前桶是否为空，空的就需要初始化（resize 中会判断是否进行初始化）。</li><li>根据当前 key 的 hashcode 定位到具体的桶中并判断是否为空，为空表明没有 Hash 冲突就直接在当前位置创建一个新桶即可。</li><li>如果当前桶有值（ Hash 冲突），那么就要比较当前桶中的 <code>key、key 的 hashcode</code> 与写入的 key 是否相等，相等就赋值给 <code>e</code>,在第 8 步的时候会统一进行赋值及返回。</li><li>如果当前桶为红黑树，那就要按照红黑树的方式写入数据。</li><li>如果是个链表，就需要将当前的 key、value 封装成一个新节点写入到当前桶的后面（形成链表）。</li><li>接着判断当前链表的大小是否大于预设的阈值，大于时就要转换为红黑树。</li><li>如果在遍历过程中找到 key 相同时直接退出遍历。</li><li>如果 <code>e != null</code> 就相当于存在相同的 key,那就需要将值覆盖。</li><li>最后判断是否需要进行扩容。</li></ol><h4 id="get-方法-1"><a href="#get-方法-1" class="headerlink" title="get 方法"></a>get 方法</h4><pre><code>    public V get(Object key) {        Node&lt;K,V&gt; e;        return (e = getNode(hash(key), key)) == null ? null : e.value;    }    final Node&lt;K,V&gt; getNode(int hash, Object key) {        Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k;        if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp;            (first = tab[(n - 1) &amp; hash]) != null) {            if (first.hash == hash &amp;&amp; // always check first node                ((k = first.key) == key || (key != null &amp;&amp; key.equals(k))))                return first;            if ((e = first.next) != null) {                if (first instanceof TreeNode)                    return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key);                do {                    if (e.hash == hash &amp;&amp;                        ((k = e.key) == key || (key != null &amp;&amp; key.equals(k))))                        return e;                } while ((e = e.next) != null);            }        }        return null;    }</code></pre><p>get 方法看起来就要简单许多了。</p><ul><li>首先将 key hash 之后取得所定位的桶。</li><li>如果桶为空则直接返回 null 。</li><li>否则判断桶的第一个位置(有可能是链表、红黑树)的 key 是否为查询的 key，是就直接返回 value。</li><li>如果第一个不匹配，则判断它的下一个是红黑树还是链表。</li><li>红黑树就按照树的查找方式返回值。</li><li>不然就按照链表的方式遍历匹配返回值。</li></ul><p>从这两个核心方法（get/put）可以看出 1.8 中对大链表做了优化，修改为红黑树之后查询效率直接提高到了 <code>O(logn)</code>。</p><p>但是 HashMap 原有的问题也都存在，比如在并发场景下使用时容易出现死循环。</p><pre><code>final HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();for (int i = 0; i &lt; 1000; i++) {    new Thread(new Runnable() {        @Override        public void run() {            map.put(UUID.randomUUID().toString(), &quot;&quot;);        }    }).start();}</code></pre><p>但是为什么呢？简单分析下。</p><p>看过上文的还记得在 HashMap 扩容的时候会调用 <code>resize()</code> 方法，就是这里的并发操作容易在一个桶上形成环形链表；这样当获取一个不存在的 key 时，计算出的 index 正好是环形链表的下标就会出现死循环。</p><h3 id="遍历方式"><a href="#遍历方式" class="headerlink" title="遍历方式"></a>遍历方式</h3><p>还有一个值得注意的是 HashMap 的遍历方式，通常有以下几种：</p><pre><code>Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; entryIterator = map.entrySet().iterator();        while (entryIterator.hasNext()) {            Map.Entry&lt;String, Integer&gt; next = entryIterator.next();            System.out.println(&quot;key=&quot; + next.getKey() + &quot; value=&quot; + next.getValue());        }Iterator&lt;String&gt; iterator = map.keySet().iterator();        while (iterator.hasNext()){            String key = iterator.next();            System.out.println(&quot;key=&quot; + key + &quot; value=&quot; + map.get(key));        }</code></pre><p><code>强烈建议</code>使用第一种 EntrySet 进行遍历。</p><p>第一种可以把 key value 同时取出，第二种还得需要通过 key 取一次 value，效率较低。</p><blockquote><p>简单总结下 HashMap：无论是 1.7 还是 1.8 其实都能看出 JDK 没有对它做任何的同步操作，所以并发会出问题，甚至出现死循环导致系统不可用。</p></blockquote><p>因此 JDK 推出了专项专用的 ConcurrentHashMap ，该类位于 <code>java.util.concurrent</code> 包下，专门用于解决并发问题。</p><blockquote><p>坚持看到这里的朋友算是已经把 ConcurrentHashMap 的基础已经打牢了，下面正式开始分析。</p></blockquote><h2 id="ConcurrentHashMap"><a href="#ConcurrentHashMap" class="headerlink" title="ConcurrentHashMap"></a>ConcurrentHashMap</h2><p>ConcurrentHashMap 同样也分为 1.7 、1.8 版，两者在实现上略有不同。</p><h3 id="Base-1-7-1"><a href="#Base-1-7-1" class="headerlink" title="Base 1.7"></a>Base 1.7</h3><p>先来看看 1.7 的实现，下面是他的结构图：</p><p><img src="/2020/01/23/hashmap-yu-concurrenthashmap/6.png" alt></p><p>如图所示，是由 Segment 数组、HashEntry 组成，和 HashMap 一样，仍然是数组加链表。</p><p>它的核心成员变量：</p><pre><code>    /**     * Segment 数组，存放数据时首先需要定位到具体的 Segment 中。     */    final Segment&lt;K,V&gt;[] segments;    transient Set&lt;K&gt; keySet;    transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet;</code></pre><p>Segment 是 ConcurrentHashMap 的一个内部类，主要的组成如下：</p><pre><code>    static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable {        private static final long serialVersionUID = 2249069246763182397L;        // 和 HashMap 中的 HashEntry 作用一样，真正存放数据的桶        transient volatile HashEntry&lt;K,V&gt;[] table;        transient int count;        transient int modCount;        transient int threshold;        final float loadFactor;    }</code></pre><p>看看其中 HashEntry 的组成：</p><p><img src="/2020/01/23/hashmap-yu-concurrenthashmap/7.png" alt></p><p>和 HashMap 非常类似，唯一的区别就是其中的核心数据如 value ，以及链表都是 volatile 修饰的，保证了获取时的可见性。</p><p>原理上来说：ConcurrentHashMap 采用了分段锁技术，其中 Segment 继承于 ReentrantLock。不会像 HashTable 那样不管是 put 还是 get 操作都需要做同步处理，理论上 ConcurrentHashMap 支持 CurrencyLevel (Segment 数组数量)的线程并发。每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment。</p><p>下面也来看看核心的 <code>put get</code> 方法。</p><h4 id="put-方法-2"><a href="#put-方法-2" class="headerlink" title="put 方法"></a>put 方法</h4><pre><code>    public V put(K key, V value) {        Segment&lt;K,V&gt; s;        if (value == null)            throw new NullPointerException();        int hash = hash(key);        int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask;        if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject          // nonvolatile; recheck             (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) //  in ensureSegment            s = ensureSegment(j);        return s.put(key, hash, value, false);    }</code></pre><p>首先是通过 key 定位到 Segment，之后在对应的 Segment 中进行具体的 put。</p><pre><code>        final V put(K key, int hash, V value, boolean onlyIfAbsent) {            HashEntry&lt;K,V&gt; node = tryLock() ? null :                scanAndLockForPut(key, hash, value);            V oldValue;            try {                HashEntry&lt;K,V&gt;[] tab = table;                int index = (tab.length - 1) &amp; hash;                HashEntry&lt;K,V&gt; first = entryAt(tab, index);                for (HashEntry&lt;K,V&gt; e = first;;) {                    if (e != null) {                        K k;                        if ((k = e.key) == key ||                            (e.hash == hash &amp;&amp; key.equals(k))) {                            oldValue = e.value;                            if (!onlyIfAbsent) {                                e.value = value;                                ++modCount;                            }                            break;                        }                        e = e.next;                    }                    else {                        if (node != null)                            node.setNext(first);                        else                            node = new HashEntry&lt;K,V&gt;(hash, key, value, first);                        int c = count + 1;                        if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY)                            rehash(node);                        else                            setEntryAt(tab, index, node);                        ++modCount;                        count = c;                        oldValue = null;                        break;                    }                }            } finally {                unlock();            }            return oldValue;        }</code></pre><p>虽然 HashEntry 中的 value 是用 volatile 关键词修饰的，但是并不能保证并发的原子性，所以 put 操作时仍然需要加锁处理。</p><p>首先第一步的时候会尝试获取锁，如果获取失败肯定就有其他线程存在竞争，则利用 <code>scanAndLockForPut()</code> 自旋获取锁。</p><p><img src="/2020/01/23/hashmap-yu-concurrenthashmap/8.png" alt></p><ol><li>尝试自旋获取锁。</li><li>如果重试的次数达到了 <code>MAX_SCAN_RETRIES</code> 则改为阻塞锁获取，保证能获取成功。</li></ol><p><img src="/2020/01/23/hashmap-yu-concurrenthashmap/9.png" alt></p><p>再结合图看看 put 的流程。</p><ol><li>将当前 Segment 中的 table 通过 key 的 hashcode 定位到 HashEntry。</li><li>遍历该 HashEntry，如果不为空则判断传入的 key 和当前遍历的 key 是否相等，相等则覆盖旧的 value。</li><li>不为空则需要新建一个 HashEntry 并加入到 Segment 中，同时会先判断是否需要扩容。</li><li>最后会解除在 1 中所获取当前 Segment 的锁。</li></ol><h4 id="get-方法-2"><a href="#get-方法-2" class="headerlink" title="get 方法"></a>get 方法</h4><pre><code>    public V get(Object key) {        Segment&lt;K,V&gt; s; // manually integrate access methods to reduce overhead        HashEntry&lt;K,V&gt;[] tab;        int h = hash(key);        long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE;        if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp;            (tab = s.table) != null) {            for (HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile                     (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE);                 e != null; e = e.next) {                K k;                if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k)))                    return e.value;            }        }        return null;    }</code></pre><p>get 逻辑比较简单：</p><p>只需要将 Key 通过 Hash 之后定位到具体的 Segment ，再通过一次 Hash 定位到具体的元素上。</p><p>由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值。</p><p>ConcurrentHashMap 的 get 方法是非常高效的，<strong>因为整个过程都不需要加锁</strong>。</p><h3 id="Base-1-8-1"><a href="#Base-1-8-1" class="headerlink" title="Base 1.8"></a>Base 1.8</h3><p>1.7 已经解决了并发问题，并且能支持 N 个 Segment 这么多次数的并发，但依然存在 HashMap 在 1.7 版本中的问题。</p><blockquote><p>那就是查询遍历链表效率太低。</p></blockquote><p>因此 1.8 做了一些数据结构上的调整。</p><p>首先来看下底层的组成结构：</p><p><img src="/2020/01/23/hashmap-yu-concurrenthashmap/10.png" alt></p><p>看起来是不是和 1.8 HashMap 结构类似？</p><p>其中抛弃了原有的 Segment 分段锁，而采用了 <code>CAS + synchronized</code> 来保证并发安全性。</p><p><img src="/2020/01/23/hashmap-yu-concurrenthashmap/11.png" alt></p><p>也将 1.7 中存放数据的 HashEntry 改为 Node，但作用都是相同的。</p><p>其中的 <code>val next</code> 都用了 volatile 修饰，保证了可见性。</p><h4 id="put-方法-3"><a href="#put-方法-3" class="headerlink" title="put 方法"></a>put 方法</h4><p>重点来看看 put 函数：</p><p><img src="/2020/01/23/hashmap-yu-concurrenthashmap/12.png" alt></p><ul><li>根据 key 计算出 hashcode 。</li><li>判断是否需要进行初始化。</li><li><code>f</code> 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。</li><li>如果当前位置的 <code>hashcode == MOVED == -1</code>,则需要进行扩容。</li><li>如果都不满足，则利用 synchronized 锁写入数据。</li><li>如果数量大于 <code>TREEIFY_THRESHOLD</code> 则要转换为红黑树。</li></ul><h4 id="get-方法-3"><a href="#get-方法-3" class="headerlink" title="get 方法"></a>get 方法</h4><p><img src="/2020/01/23/hashmap-yu-concurrenthashmap/13.png" alt></p><ul><li>根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值。</li><li>如果是红黑树那就按照树的方式获取值。</li><li>就不满足那就按照链表的方式遍历获取值。</li></ul><blockquote><p>1.8 在 1.7 的数据结构上做了大的改动，采用红黑树之后可以保证查询效率（<code>O(logn)</code>），甚至取消了 ReentrantLock 改为了 synchronized，这样可以看出在新版的 JDK 中对 synchronized 优化是很到位的。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>线程池原理</title>
      <link href="/2020/01/22/xian-cheng-chi-yuan-li/"/>
      <url>/2020/01/22/xian-cheng-chi-yuan-li/</url>
      
        <content type="html"><![CDATA[<h1 id="1-为什么要使用线程池"><a href="#1-为什么要使用线程池" class="headerlink" title="1. 为什么要使用线程池"></a>1. 为什么要使用线程池</h1><p>在实际使用中，线程是很占用系统资源的，如果对线程管理不善很容易导致系统问题。因此，在大多数并发框架中都会使用<strong>线程池</strong>来管理线程，使用线程池管理线程主要有如下好处：</p><ol><li><strong>降低资源消耗</strong>。通过复用已存在的线程和降低线程关闭的次数来尽可能降低系统性能损耗；</li><li><strong>提升系统响应速度</strong>。通过复用线程，省去创建线程的过程，因此整体上提升了系统的响应速度；</li><li><strong>提高线程的可管理性</strong>。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，因此，需要使用线程池来管理线程。</li></ol><h1 id="2-线程池的工作原理"><a href="#2-线程池的工作原理" class="headerlink" title="2. 线程池的工作原理"></a>2. 线程池的工作原理</h1><p>当一个并发任务提交给线程池，线程池分配线程去执行任务的过程如下图所示：</p><p><img src="/2020/01/22/xian-cheng-chi-yuan-li/1.png" alt="线程池执行流程"></p><p>从图可以看出，线程池执行所提交的任务过程主要有这样几个阶段：</p><ol><li>先判断线程池中<strong>核心线程池</strong>所有的线程是否都在执行任务。如果不是，则新创建一个线程执行刚提交的任务，否则，核心线程池中所有的线程都在执行任务，则进入第 2 步；</li><li>判断当前<strong>阻塞队列</strong>是否已满，如果未满，则将提交的任务放置在阻塞队列中；否则，则进入第 3 步；</li><li>判断<strong>线程池中所有的线程</strong>是否都在执行任务，如果没有，则创建一个新的线程来执行任务，否则，则交给饱和策略进行处理</li></ol><h1 id="3-线程池的创建"><a href="#3-线程池的创建" class="headerlink" title="3. 线程池的创建"></a>3. 线程池的创建</h1><p>创建线程池主要是<strong>ThreadPoolExecutor</strong>类来完成，ThreadPoolExecutor 的有许多重载的构造方法，通过参数最多的构造方法来理解创建线程池有哪些需要配置的参数。ThreadPoolExecutor 的构造方法为：</p><pre><code>ThreadPoolExecutor(int corePoolSize,                              int maximumPoolSize,                              long keepAliveTime,                              TimeUnit unit,                              BlockingQueue&lt;Runnable&gt; workQueue,                              ThreadFactory threadFactory,                              RejectedExecutionHandler handler)</code></pre><p>下面对参数进行说明：</p><ol><li>corePoolSize：表示核心线程池的大小。当提交一个任务时，如果当前核心线程池的线程个数没有达到 corePoolSize，则会创建新的线程来执行所提交的任务，<strong>即使当前核心线程池有空闲的线程</strong>。如果当前核心线程池的线程个数已经达到了 corePoolSize，则不再重新创建线程。如果调用了<code>prestartCoreThread()</code>或者 <code>prestartAllCoreThreads()</code>，线程池创建的时候所有的核心线程都会被创建并且启动。</li><li>maximumPoolSize：表示线程池能创建线程的最大个数。如果当阻塞队列已满时，并且当前线程池线程个数没有超过 maximumPoolSize 的话，就会创建新的线程来执行任务。</li><li>keepAliveTime：空闲线程存活时间。如果当前线程池的线程个数已经超过了 corePoolSize，并且线程空闲时间超过了 keepAliveTime 的话，就会将这些空闲线程销毁，这样可以尽可能降低系统资源消耗。</li><li>unit：时间单位。为 keepAliveTime 指定时间单位。</li><li>workQueue：阻塞队列。用于保存任务的阻塞队列。可以使用<strong>ArrayBlockingQueue, LinkedBlockingQueue, SynchronousQueue, PriorityBlockingQueue</strong>。</li><li>threadFactory：创建线程的工程类。可以通过指定线程工厂为每个创建出来的线程设置更有意义的名字，如果出现并发问题，也方便查找问题原因。</li><li><ol><li>AbortPolicy： 直接拒绝所提交的任务，并抛出<strong>RejectedExecutionException</strong>异常；</li><li>CallerRunsPolicy：只用调用者所在的线程来执行任务；</li><li>DiscardPolicy：不处理直接丢弃掉任务；</li><li>DiscardOldestPolicy：丢弃掉阻塞队列中存放时间最久的任务，执行当前任务</li></ol></li></ol><p>通过 ThreadPoolExecutor 创建线程池后，提交任务后执行过程是怎样的，下面来通过源码来看一看。execute 方法源码如下：</p><pre><code>public void execute(Runnable command) {    if (command == null)        throw new NullPointerException();    /*     * Proceed in 3 steps:     *     * 1. If fewer than corePoolSize threads are running, try to     * start a new thread with the given command as its first     * task.  The call to addWorker atomically checks runState and     * workerCount, and so prevents false alarms that would add     * threads when it shouldn&#39;t, by returning false.     *     * 2. If a task can be successfully queued, then we still need     * to double-check whether we should have added a thread     * (because existing ones died since last checking) or that     * the pool shut down since entry into this method. So we     * recheck state and if necessary roll back the enqueuing if     * stopped, or start a new thread if there are none.     *     * 3. If we cannot queue task, then we try to add a new     * thread.  If it fails, we know we are shut down or saturated     * and so reject the task.     */    int c = ctl.get();    //如果线程池的线程个数少于corePoolSize则创建新线程执行当前任务    if (workerCountOf(c) &lt; corePoolSize) {        if (addWorker(command, true))            return;        c = ctl.get();    }    //如果线程个数大于corePoolSize或者创建线程失败，则将任务存放在阻塞队列workQueue中    if (isRunning(c) &amp;&amp; workQueue.offer(command)) {        int recheck = ctl.get();        if (! isRunning(recheck) &amp;&amp; remove(command))            reject(command);        else if (workerCountOf(recheck) == 0)            addWorker(null, false);    }    //如果当前任务无法放进阻塞队列中，则创建新的线程来执行任务    else if (!addWorker(command, false))        reject(command);}</code></pre><p>ThreadPoolExecutor 的 execute 方法执行逻辑请见注释。下图为 ThreadPoolExecutor 的 execute 方法的执行示意图：</p><p><img src="/2020/01/22/xian-cheng-chi-yuan-li/2.png" alt="execute执行过程"></p><p>execute 方法执行逻辑有这样几种情况：</p><ol><li>如果当前运行的线程少于 corePoolSize，则会创建新的线程来执行新的任务；</li><li>如果运行的线程个数等于或者大于 corePoolSize，则会将提交的任务存放到阻塞队列 workQueue 中；</li><li>如果当前 workQueue 队列已满的话，则会创建新的线程来执行任务；</li><li>如果线程个数已经超过了 maximumPoolSize，则会使用饱和策略 RejectedExecutionHandler 来进行处理。</li></ol><p>需要注意的是，线程池的设计思想就是使用了<strong>核心线程池 corePoolSize，阻塞队列 workQueue 和线程池 maximumPoolSize</strong>，这样的缓存策略来处理任务，实际上这样的设计思想在需要框架中都会使用。</p><h1 id="4-线程池的关闭"><a href="#4-线程池的关闭" class="headerlink" title="4. 线程池的关闭"></a>4. 线程池的关闭</h1><p>关闭线程池，可以通过<code>shutdown</code>和<code>shutdownNow</code>这两个方法。它们的原理都是遍历线程池中所有的线程，然后依次中断线程。<code>shutdown</code>和<code>shutdownNow</code>还是有不一样的地方：</p><ol><li><code>shutdownNow</code>首先将线程池的状态设置为<strong>STOP</strong>,然后尝试<strong>停止所有的正在执行和未执行任务</strong>的线程，并返回等待执行任务的列表；</li><li><code>shutdown</code>只是将线程池的状态设置为<strong>SHUTDOWN</strong>状态，然后中断所有没有正在执行任务的线程</li></ol><p>可以看出 shutdown 方法会将正在执行的任务继续执行完，而 shutdownNow 会直接中断正在执行的任务。调用了这两个方法的任意一个，<code>isShutdown</code>方法都会返回 true，当所有的线程都关闭成功，才表示线程池成功关闭，这时调用<code>isTerminated</code>方法才会返回 true。</p><h1 id="5-如何合理配置线程池参数？"><a href="#5-如何合理配置线程池参数？" class="headerlink" title="5. 如何合理配置线程池参数？"></a>5. 如何合理配置线程池参数？</h1><p>要想合理的配置线程池，就必须首先分析任务特性，可以从以下几个角度来进行分析：</p><ol><li>任务的性质：CPU 密集型任务，IO 密集型任务和混合型任务。</li><li>任务的优先级：高，中和低。</li><li>任务的执行时间：长，中和短。</li><li>任务的依赖性：是否依赖其他系统资源，如数据库连接。</li></ol><p>任务性质不同的任务可以用不同规模的线程池分开处理。CPU 密集型任务配置尽可能少的线程数量，如配置<strong>Ncpu+1</strong>个线程的线程池。IO 密集型任务则由于需要等待 IO 操作，线程并不是一直在执行任务，则配置尽可能多的线程，如<strong>2xNcpu</strong>。混合型的任务，如果可以拆分，则将其拆分成一个 CPU 密集型任务和一个 IO 密集型任务，只要这两个任务执行的时间相差不是太大，那么分解后执行的吞吐率要高于串行执行的吞吐率，如果这两个任务执行时间相差太大，则没必要进行分解。我们可以通过<code>Runtime.getRuntime().availableProcessors()</code>方法获得当前设备的 CPU 个数。</p><p>优先级不同的任务可以使用优先级队列 PriorityBlockingQueue 来处理。它可以让优先级高的任务先得到执行，需要注意的是如果一直有优先级高的任务提交到队列里，那么优先级低的任务可能永远不能执行。</p><p>执行时间不同的任务可以交给不同规模的线程池来处理，或者也可以使用优先级队列，让执行时间短的任务先执行。</p><p>依赖数据库连接池的任务，因为线程提交 SQL 后需要等待数据库返回结果，如果等待的时间越长 CPU 空闲时间就越长，那么线程数应该设置越大，这样才能更好的利用 CPU。</p><p>并且，阻塞队列<strong>最好是使用有界队列</strong>，如果采用无界队列的话，一旦任务积压在阻塞队列中的话就会占用过多的内存资源，甚至会使得系统崩溃。</p><h1 id="6-常用的封装好的线程池"><a href="#6-常用的封装好的线程池" class="headerlink" title="6.常用的封装好的线程池"></a>6.常用的封装好的线程池</h1><h4 id="1-SingleThreadExecutor-线程池"><a href="#1-SingleThreadExecutor-线程池" class="headerlink" title="1. SingleThreadExecutor 线程池"></a><strong>1. SingleThreadExecutor 线程池</strong></h4><p>这个线程池只有一个核心线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。</p><ul><li>corePoolSize：1，只有一个核心线程在工作。</li><li>maximumPoolSize：1。</li><li>keepAliveTime：0L。</li><li>workQueue：new LinkedBlockingQueue<runnable>()，其缓冲队列是无界的。</runnable></li></ul><h4 id="2-FixedThreadPool-线程池"><a href="#2-FixedThreadPool-线程池" class="headerlink" title="2. FixedThreadPool 线程池"></a><strong>2. FixedThreadPool 线程池</strong></h4><p>FixedThreadPool 是固定大小的线程池，只有核心线程。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。</p><p>FixedThreadPool 多数针对一些很稳定很固定的正规并发线程，多用于服务器。</p><ul><li>corePoolSize：nThreads</li><li>maximumPoolSize：nThreads</li><li>keepAliveTime：0L</li><li>workQueue：new LinkedBlockingQueue<runnable>()，其缓冲队列是无界的。</runnable></li></ul><h4 id="3-CachedThreadPool-线程池"><a href="#3-CachedThreadPool-线程池" class="headerlink" title="3. CachedThreadPool 线程池"></a><strong>3. CachedThreadPool 线程池</strong></h4><p>CachedThreadPool 是无界线程池，如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲（60 秒不执行任务）线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。</p><p>线程池大小完全依赖于操作系统（或者说 JVM）能够创建的最大线程大小。SynchronousQueue 是一个是缓冲区为 1 的阻塞队列。</p><p>缓存型池子通常用于执行一些生存期很短的异步型任务，因此在一些面向连接的 daemon 型 SERVER 中用得不多。但对于生存期短的异步任务，它是 Executor 的首选。</p><ul><li>corePoolSize：0</li><li>maximumPoolSize：Integer.MAX_VALUE</li><li>keepAliveTime：60L</li><li>workQueue：new SynchronousQueue<runnable>()，一个是缓冲区为 1 的阻塞队列。</runnable></li></ul><h4 id="4-ScheduledThreadPool-线程池"><a href="#4-ScheduledThreadPool-线程池" class="headerlink" title="4. ScheduledThreadPool 线程池"></a><strong>4. ScheduledThreadPool 线程池</strong></h4><p>ScheduledThreadPool：核心线程池固定，大小无限的线程池。此线程池支持定时以及周期性执行任务的需求。创建一个周期性执行任务的线程池。如果闲置，非核心线程池会在 DEFAULT_KEEPALIVEMILLIS 时间内回收。</p><ul><li>corePoolSize：corePoolSize</li><li>maximumPoolSize：Integer.MAX_VALUE</li><li>keepAliveTime：DEFAULT_KEEPALIVE_MILLIS</li><li>workQueue：new DelayedWorkQueue()</li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>稳定排序与非稳定排序的应用场景</title>
      <link href="/2020/01/22/wen-ding-pai-xu-yu-fei-wen-ding-pai-xu-de-ying-yong-chang-jing/"/>
      <url>/2020/01/22/wen-ding-pai-xu-yu-fei-wen-ding-pai-xu-de-ying-yong-chang-jing/</url>
      
        <content type="html"><![CDATA[<h3 id="稳定非稳定如何界定"><a href="#稳定非稳定如何界定" class="headerlink" title="稳定非稳定如何界定"></a>稳定非稳定如何界定</h3><p><img src="/2020/01/22/wen-ding-pai-xu-yu-fei-wen-ding-pai-xu-de-ying-yong-chang-jing/stable1.png" alt></p><p>原始数据，a2和a4的位置都是3。对于稳定排序来说，排序后的序列，a2一定还是在a4前面。但是对于非稳定排序来说，就不一定了，可能排完序之后，a4反而在a2的前面了。</p><p>哪些常用算法是稳定的，哪些是不稳定的呢？</p><p><strong>(1)冒泡排序</strong><br>        冒泡排序就是把小的元素往前调或者把大的元素往后调。比较是相邻的两个元素比较，交换也发生在这两个元素之间。所以，如果两个元素相等，我想你是不会再无聊地把他们俩交换一下的；如果两个相等的元素没有相邻，那么即使通过前面的两两交换把两个相邻起来，这时候也不会交换，所以相同元素的前后顺序并没有改变，所以冒泡排序是一种稳定排序算法。<br><strong>(2)选择排序</strong><br>      选择排序是给每个位置选择当前元素最小的，比如给第一个位置选择最小的，在剩余元素里面给第二个元素选择第二小的，依次类推，直到第n-1个元素，第n个元素不用选择了，因为只剩下它一个最大的元素了。那么，在一趟选择，如果当前元素比一个元素小，而该小的元素又出现在一个和当前元素相等的元素后面，那么交换后稳定性就被破坏了。比较拗口，举个例子，序列5 8 5 2 9，我们知道第一遍选择第1个元素5会和2交换，那么原序列中2个5的相对前后顺序就被破坏了，所以选择排序不是一个稳定的排序算法。<br><strong>(3)插入排序</strong><br>     插入排序是在一个已经有序的小序列的基础上，一次插入一个元素。当然，刚开始这个有序的小序列只有1个元素，就是第一个元素。比较是从有序序列的末尾开始，也就是想要插入的元素和已经有序的最大者开始比起，如果比它大则直接插入在其后面，否则一直往前找直到找到它该插入的位置。如果碰见一个和插入元素相等的，那么插入元素把想插入的元素放在相等元素的后面。所以，相等元素的前后顺序没有改变，从原无序序列出去的顺序就是排好序后的顺序，所以插入排序是稳定的。</p><p><strong>(4)快速排序</strong><br>    快速排序有两个方向，左边的i下标一直往右走，当a[i] &lt;= a[center_index]，其中center_index是中枢元素的数组下标，一般取为数组第0个元素。而右边的j下标一直往左走，当a[j] &gt; a[center_index]。如果i和j都走不动了，i &lt;= j, 交换a[i]和a[j],重复上面的过程，直到i&gt;j。交换a[j]和a[center_index]，完成一趟快速排序。在中枢元素和a[j]交换的时候，很有可能把前面的元素的稳定性打乱，比如序列为 5 3 3 4 3 8 9 10 11，现在中枢元素5和3(第5个元素，下标从1开始计)交换就会把元素3的稳定性打乱，所以快速排序是一个不稳定的排序算法，不稳定发生在中枢元素和a[j] 交换的时刻。</p><p><strong>(5)归并排序</strong><br>    归并排序是把序列递归地分成短序列，递归出口是短序列只有1个元素(认为直接有序)或者2个序列(1次比较和交换),然后把各个有序的段序列合并成一个有序的长序列，不断合并直到原序列全部排好序。可以发现，在1个或2个元素时，1个元素不会交换，2个元素如果大小相等也没有人故意交换，这不会破坏稳定性。那么，在短的有序序列合并的过程中，稳定是是否受到破坏？没有，合并过程中我们可以保证如果两个当前元素相等时，我们把处在前面的序列的元素保存在结果序列的前面，这样就保证了稳定性。所以，归并排序也是稳定的排序算法。<br><strong>(6)基数排序</strong><br>   基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序，最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。基数排序基于分别排序，分别收集，所以其是稳定的排序算法。<br><strong>(7)希尔排序(shell)</strong><br>    希尔排序是按照不同步长对元素进行插入排序，当刚开始元素很无序的时候，步长最大，所以插入排序的元素个数很少，速度很快；当元素基本有序了，步长很小，插入排序对于有序的序列效率很高。所以，希尔排序的时间复杂度会比o(n^2)好一些。由于多次插入排序，我们知道一次插入排序是稳定的，不会改变相同元素的相对顺序，但在不同的插入排序过程中，相同的元素可能在各自的插入排序中移动，最后其稳定性就会被打乱，所以shell排序是不稳定的。<br><strong>(8)堆排序</strong><br>   我们知道堆的结构是节点i的孩子为2<em>i和2</em>i+1节点，大顶堆要求父节点大于等于其2个子节点，小顶堆要求父节点小于等于其2个子节点。在一个长为n 的序列，堆排序的过程是从第n/2开始和其子节点共3个值选择最大(大顶堆)或者最小(小顶堆),这3个元素之间的选择当然不会破坏稳定性。但当为n /2-1, n/2-2, …1这些个父节点选择元素时，就会破坏稳定性。有可能第n/2个父节点交换把后面一个元素交换过去了，而第n/2-1个父节点把后面一个相同的元素没有交换，那么这2个相同的元素之间的稳定性就被破坏了。所以，堆排序不是稳定的排序算法。</p><p><img src="/2020/01/22/wen-ding-pai-xu-yu-fei-wen-ding-pai-xu-de-ying-yong-chang-jing/0_1275286328kIDC.png" alt></p><p>综上，得出结论: 选择排序、快速排序、希尔排序、堆排序不是稳定的排序算法，而冒泡排序、插入排序、归并排序和基数排序是稳定的排序算法。</p><p><strong>思考</strong>：既然最后都是有序序列，为什么还要分稳定和非稳定的排序呢？</p><h3 id="为什么要分稳定和非稳定呢？"><a href="#为什么要分稳定和非稳定呢？" class="headerlink" title="为什么要分稳定和非稳定呢？"></a>为什么要分稳定和非稳定呢？</h3><p><strong>看一个典型的场景：每次考试完成后，都会按照分数进行排序。分高的自然就是第一名。分数相同的同学怎么办呢？那就是按照上次的分数来分高低。上次分高的排在前面。</strong></p><p><strong>这个时候就应该用稳定排序，在上次排好序的序列上，再针对这次的分数进行排序。稳定排序的结果能保证这次相同分数的人，上次分高的在前面。</strong></p><p><strong>再比如我们熟知的基数排序与计数排序,当对最后一位进行计数排序后,当倒数第二位的数字相同时,必须保证第一轮技术排序排在前面的经过重排后还在前面。这时就需要计数排序是稳定的。</strong></p><p><strong>其实就是有两个排序关键字的时候，稳定排序可以让第一个关键字排序的结果服务于第二个关键字排序中数值相等的那些数。</strong></p>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>HTTP与HTTPS原理</title>
      <link href="/2020/01/21/http-yu-https-yuan-li/"/>
      <url>/2020/01/21/http-yu-https-yuan-li/</url>
      
        <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>HTTP 是一个无状态的协议。无状态是指客户机（Web 浏览器）和服务器之间不需要建立持久的连接，这意味着当一个客户端向服务器端发出请求，然后服务器返回响应(response)，连接就被关闭了，在服务器端不保留连接的有关信息.HTTP 遵循请求(Request)/应答(Response)模型。客户机（浏览器）向服务器发送请求，服务器处理请求并返回适当的应答。所有 HTTP 连接都被构造成一套请求和应答。</p><h3 id="传输流程"><a href="#传输流程" class="headerlink" title="传输流程"></a>传输流程</h3><h4 id="地址解析"><a href="#地址解析" class="headerlink" title="地址解析"></a>地址解析</h4><p>如用客户端浏览器请求这个页面： <a href="http://localhost.com:8080/index.htm" target="_blank" rel="noopener">http://localhost.com:8080/index.htm</a> 从中分解出协议名、主机名、端口、对象路径等部分，对于我们的这个地址，解析得到的结果如下：</p><ul><li>协议名： http</li><li>主机名： localhost.com</li><li>端口： 8080</li><li>对象路径： /index.htm</li></ul><p>在这一步，需要域名系统 DNS 解析域名 localhost.com,得主机的 IP 地址。</p><h4 id="封装-HTTP-请求数据包"><a href="#封装-HTTP-请求数据包" class="headerlink" title="封装 HTTP 请求数据包"></a>封装 HTTP 请求数据包</h4><p>把以上部分结合本机自己的信息，封装成一个 HTTP 请求数据包</p><h4 id="封装成-TCP-包并建立连接"><a href="#封装成-TCP-包并建立连接" class="headerlink" title="封装成 TCP 包并建立连接"></a>封装成 TCP 包并建立连接</h4><p>封装成 TCP 包，建立 TCP 连接（TCP 的三次握手）</p><h4 id="客户机发送请求命令"><a href="#客户机发送请求命令" class="headerlink" title="客户机发送请求命令"></a>客户机发送请求命令</h4><p>建立连接后，客户机发送一个请求给服务器，请求方式的格式为：统一资源标识符（URL）、协议版本号，后边是 MIME 信息包括请求修饰符、客户机信息和可能的内容。</p><h4 id="服务器响应"><a href="#服务器响应" class="headerlink" title="服务器响应"></a>服务器响应</h4><p>服务器接到请求后，给予相应的响应信息， 其格式为一个状态行，包括信息的协议版本号、一个成功或错误的代码，后边是 MIME 信息包括服务器信息、实体信息和可能的内容。</p><h4 id="服务器关闭-TCP-连接"><a href="#服务器关闭-TCP-连接" class="headerlink" title="服务器关闭 TCP 连接"></a>服务器关闭 TCP 连接</h4><p>服务器关闭 TCP 连接： 一般情况下，一旦 Web 服务器向浏览器发送了请求数据，它就要关闭 TCP 连接，然后如果浏览器或者服务器在其头信息加入了这行代码 Connection:keep-alive， TCP 连接在发送后将仍然保持打开状态，于是，浏览器可以继续通过相同的连接发送请求。保持连接节省了为每个请求建立新连接所需的时间，还节约了网络带宽。</p><p><img src="/2020/01/21/http-yu-https-yuan-li/1.png" alt></p><h4 id="HTTP响应状态"><a href="#HTTP响应状态" class="headerlink" title="HTTP响应状态"></a>HTTP响应状态</h4><ul><li>1XX</li></ul><p><img src="/2020/01/21/http-yu-https-yuan-li/2.png" alt></p><ul><li>2XX</li></ul><p><img src="/2020/01/21/http-yu-https-yuan-li/3.png" alt></p><ul><li>3XX</li></ul><p><img src="/2020/01/21/http-yu-https-yuan-li/4.png" alt></p><ul><li>4XX</li></ul><p><img src="/2020/01/21/http-yu-https-yuan-li/5.png" alt></p><ul><li>5XX</li></ul><p><img src="/2020/01/21/http-yu-https-yuan-li/6.png" alt></p><h3 id="小插曲：301和302有啥区别"><a href="#小插曲：301和302有啥区别" class="headerlink" title="小插曲：301和302有啥区别"></a>小插曲：301和302有啥区别</h3><p>官方说法：</p><blockquote><p>301，302 都是HTTP状态的编码，都代表着某个URL发生了转移，不同之处在于： 301 redirect: 301 代表永久性转移(Permanently Moved)。 302 redirect: 302 代表暂时性转移(Temporarily Moved )。</p></blockquote><p>现实中的差异：</p><ul><li><strong>对于用户</strong>：</li></ul><p>301，302对用户来说没有区别，他们看到效果只是一个跳转，浏览器中旧的URL变成了新的URL。页面跳到了这个新的url指向的地方。</p><ul><li><strong>对于引擎及站长</strong>：</li></ul><ol><li>302：302转向可能会有URL规范化及网址劫持的问题。可能被搜索引擎判为可疑转向，甚至认为是作弊</li></ol><p><strong>网址劫持</strong>： 302重定向和网址劫持（URL hijacking）有什么关系呢？这要从搜索引擎如何处理302转向说起。从定义来说，从网址A做一个302重定向到网址B时，主机服务器的隐含意思是网址A随时有可能改主意，重新显示本身的内容或转向其他的地方。大部分的搜索引擎在大部分情况下，当收到302重定向时，一般只要去抓取目标网址就可以了，也就是说网址B。 实际上如果搜索引擎在遇到302转向时，百分之百的都抓取目标网址B的话，就不用担心网址URL劫持了。问题就在于，有的时候搜索引擎，尤其是Google，并不能总是抓取目标网址。为什么呢？比如说，有的时候A网址很短，但是它做了一个302重定向到B网址，而B网址是一个很长的乱七八糟的URL网址，甚至还有可能包含一些问号之类的参数。很自然的，A网址更加用户友好，而B网址既难看，又不用户友好。这时Google很有可能会仍然显示网址A。</p><p>由于搜索引擎排名算法只是程序而不是人，在遇到302重定向的时候，并不能像人一样的去准确判定哪一个网址更适当，这就造成了网址URL劫持的可能性。也就是说，一个不道德的人在他自己的网址A做一个302重定向到你的网址B，出于某种原因， Google搜索结果所显示的仍然是网址A，但是所用的网页内容却是你的网址B上的内容，这种情况就叫做网址URL劫持。你辛辛苦苦所写的内容就这样被别人偷走了。</p><ol><li>301：当网页A用301重定向转到网页B时，搜索引擎可以肯定网页A永久的改变位置，或者说实际上不存了，搜索引擎就会把网页B当作唯一有效目标。 <strong>301的好处是</strong>: 第一， 没有网址规范化问题。 第二， 也很重要的，网页A的PR网页级别会传到网页B。</li></ol><h3 id="HTTPS"><a href="#HTTPS" class="headerlink" title="HTTPS"></a>HTTPS</h3><p>HTTPS（全称： Hypertext Transfer Protocol over Secure Socket Layer），是以安全为目标的HTTP 通道，简单讲是 HTTP 的安全版。<strong>即 HTTP 下加入 SSL 层</strong>， HTTPS 的安全基础是 SSL。其所用的端口号是 <strong>443</strong>。 过程大致如下：</p><h4 id="建立连接获取证书"><a href="#建立连接获取证书" class="headerlink" title="建立连接获取证书"></a>建立连接获取证书</h4><p>SSL 客户端通过 TCP 和服务器建立连接之后（443 端口），并且在一般的 tcp 连接协商（握手）过程中请求证书。即客户端发出一个消息给服务器，这个消息里面包含了自己可实现的算法列表和其它一些需要的消息， SSL 的服务器端会回应一个数据包，这里面确定了这次通信所需要的算法，然后服务器向客户端返回证书。（<strong>证书里面包含了服务器信息：域名。申请证书的公司，公共秘钥</strong>）</p><h4 id="证书验证"><a href="#证书验证" class="headerlink" title="证书验证"></a>证书验证</h4><p>Client 在收到服务器返回的证书后，判断签发这个证书的公共签发机构，并使用这个机构的公共秘钥确认签名是否有效，客户端还会确保证书中列出的域名就是它正在连接的域名。</p><h4 id="数据加密和传输"><a href="#数据加密和传输" class="headerlink" title="数据加密和传输"></a>数据加密和传输</h4><p>如果确认证书有效，那么生成对称秘钥并使用服务器的公共秘钥进行加密。然后发送给服务器，服务器使用它的私钥对它进行解密，这样两台计算机可以开始进行对称加密进行通信。</p>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>因特网五层架构</title>
      <link href="/2020/01/20/yin-te-wang-wu-ceng-jia-gou/"/>
      <url>/2020/01/20/yin-te-wang-wu-ceng-jia-gou/</url>
      
        <content type="html"><![CDATA[<h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>TCP/IP 协议不是 TCP 和 IP 这两个协议的合称，而是指因特网整个 TCP/IP 协议族。从协议分层模型方面来讲， <strong>TCP/IP 由四个层次组成：网络接口层、网络层、传输层、应用层(因特网没有对链路层和物理层有任何规定,统一为为网络层提供服务的更底层)</strong>。</p><p><img src="/2020/01/20/yin-te-wang-wu-ceng-jia-gou/tcpip1.png" alt></p><h4 id="网络接口层"><a href="#网络接口层" class="headerlink" title="网络接口层"></a>网络接口层</h4><p>网络访问层(Network Access Layer)在 TCP/IP 参考模型中并没有详细描述,(<strong>这也是为什么会有因特网是四层架构还是五层架构机构的问题,严格来说他的实现是五层,规定只有四层</strong>) <strong>只是指出主机必须使用某种协议与网络相连</strong>。</p><h4 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h4><p>网络层(Internet Layer)是整个体系结构的关键部分，其功能是使主机可以把分组发往任何网络，并使分组独立地传向目标。这些分组可能经由不同的网络，到达的顺序和发送的顺序也可能不同。高层如果需要顺序收发，那么就必须自行处理对分组的排序。 一些书上也把网络层分开为控制平面和数据平面两个层面去讲解。<strong>互联网层使用因特网协议(IP， Internet Protocol)</strong>。</p><h4 id="传输层"><a href="#传输层" class="headerlink" title="传输层"></a>传输层</h4><p>传输层(Tramsport Layer)使源端和目的端机器上的对等实体可以进行会话。 <strong>在这一层定义了两个端到端的协议</strong>：传输控制协议(<strong>TCP</strong>， Transmission Control Protocol)和用户数据报协议(<strong>UDP</strong>， User Datagram Protocol)。 TCP 是面向连接的协议，它提供可靠的报文传输和对上层应用的连接服务。为此，除了基本的数据传输外，它还有可靠性保证、流量控制、多路复用、优先权和安全性控制等功能。 UDP 是面向无连接的不可靠传输的协议，主要用于实时性要求高但是对数据丢失可容忍的应用程序（微信视频等）。</p><h4 id="应用层"><a href="#应用层" class="headerlink" title="应用层"></a>应用层</h4><p>应用层为操作系统或网络应用程序提供访问网络服务的接口只要通过调用底层功能配合自身协议实现特定功能（如文件传输，域名解析,邮件传输）。</p><h4 id="各层常见协议"><a href="#各层常见协议" class="headerlink" title="各层常见协议"></a>各层常见协议</h4><p><strong>应用层 文件传输，电子邮件，文件服务，虚拟终端 TFTP，HTTP，SNMP，FTP，SMTP，DNS，Telnet</strong><br><strong>传输层 提供端对端的接口 TCP，UDP</strong><br><strong>网络层 为数据包选择路由 IP，ICMP，RIP，OSPF，BGP，IGMP</strong><br><strong>数据链路层 传输有地址的帧以及错误检测功能 SLIP，CSLIP，PPP，ARP，RARP，MTU</strong><br><strong>物理层 以二进制数据形式在物理媒体上传输数据 ISO2110，IEEE802，IEEE802.2</strong></p>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>网络七层架构</title>
      <link href="/2020/01/20/wang-luo-qi-ceng-jia-gou/"/>
      <url>/2020/01/20/wang-luo-qi-ceng-jia-gou/</url>
      
        <content type="html"><![CDATA[<h3 id="网络七层架构"><a href="#网络七层架构" class="headerlink" title="网络七层架构"></a>网络七层架构</h3><p>7 层模型主要包括：</p><h4 id="物理层"><a href="#物理层" class="headerlink" title="物理层"></a>物理层</h4><p>主要定义物理设备标准，如网线的接口类型、光纤的接口类型、各种传输介质的传输速率等。 它的主要作用是传输比特流（就是由 1、 0 转化为电流强弱来进行传输,到达目的地后在转化为1、 0，也就是我们常说的<strong>模数转换与数模转换</strong>）。<strong>这一层的数据叫做比特</strong>。</p><h4 id="数据链路层"><a href="#数据链路层" class="headerlink" title="数据链路层"></a>数据链路层</h4><p>主要将从物理层接收的数据进行 MAC 地址（网卡的地址）的封装与解封装。<strong>常把这一层的数据叫做帧(Frame)</strong>。在这一层工作的设备是交换机，<strong>数据通过交换机来传输</strong>,有一些书本也将改层细分为LLC(逻辑链路控制)和MAC(媒体访问控制)。</p><h4 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h4><p>主要将从下层接收到的数据进行 IP 地址（例 192.168.0.1)的封装与解封装。<strong>在这一层工作的设备是路由器</strong>，<strong>常把这一层的数据叫做数据包(Packet)</strong>,其又可以细分控制层面和数据层面.</p><h4 id="传输层"><a href="#传输层" class="headerlink" title="传输层"></a>传输层</h4><p>定义了一些<strong>传输数据的协议和端口号</strong>（WWW 端口 80 等），如： <strong>TCP</strong>（传输控制协议，传输效率低，可靠性强，用于传输可靠性要求高，数据量大的数据）， <strong>UDP</strong>（用户数据报协议，与 TCP 特性恰恰相反，用于传输可靠性要求不高，数据量小的数据，如 QQ 聊天数据就是通过这种方式传输的）。 主要是将从下层接收的数据进行分段进行传输，到达目的地址后在进行重组。<strong>常常把这一层数据叫做段(Segment)</strong></p><h4 id="会话层"><a href="#会话层" class="headerlink" title="会话层"></a>会话层</h4><p>通过传输层（端口号：传输端口与接收端口） <strong>建立数据传输的通路</strong>。主要在你的系统之间发起会话或或者接受会话请求（设备之间需要互相认识可以是 IP 也可以是 MAC 或者是主机名）</p><h4 id="表示层"><a href="#表示层" class="headerlink" title="表示层"></a>表示层</h4><p>主要是进行对接收的数据进行<strong>解释、加密与解密、压缩与解压缩</strong>等（也就是把计算机能够识别的东西转换成人能够能识别的东西（如图片、声音等））</p><h4 id="应用层"><a href="#应用层" class="headerlink" title="应用层"></a>应用层</h4><p>主要是一些终端的应用，比如说FTP（各种文件下载）， HTTP（浏览器请求内容），DNS(域名解析为ip地址),SMTP(邮件传输)</p><p><img src="/2020/01/20/wang-luo-qi-ceng-jia-gou/OSIModel.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Nginx使用心得</title>
      <link href="/2020/01/19/nginx-shi-yong-xin-de/"/>
      <url>/2020/01/19/nginx-shi-yong-xin-de/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是Nginx"><a href="#什么是Nginx" class="headerlink" title="什么是Nginx"></a>什么是Nginx</h3><p><img src="/2020/01/19/nginx-shi-yong-xin-de/1526187409033.png" alt></p><p>nginx可以作为web服务器，但更多的时候，我们把它作为网关，因为它具备网关必备的功能：</p><ul><li>反向代理</li><li>负载均衡</li><li>动态路由</li><li>请求过滤</li></ul><h3 id="nginx作为web服务器"><a href="#nginx作为web服务器" class="headerlink" title="nginx作为web服务器"></a>nginx作为web服务器</h3><p>Web服务器分2类：</p><ul><li>web应用服务器，如：<ul><li>tomcat</li><li>resin</li><li>jetty</li></ul></li><li>web服务器，如：<ul><li>Apache 服务器</li><li>Nginx</li><li>IIS</li></ul></li></ul><p>区分：web服务器不能解析jsp等页面，只能处理js、css、html等静态资源。 并发：web服务器的并发能力远高于web应用服务器。</p><h3 id="nginx作为反向代理"><a href="#nginx作为反向代理" class="headerlink" title="nginx作为反向代理"></a>nginx作为反向代理</h3><p>什么是反向代理？</p><ul><li>代理：通过客户机的配置，实现让一台服务器(代理服务器)代理客户机，客户的所有请求都交给代理服务器处理。</li><li>反向代理：用一台服务器，代理真实服务器，用户访问时，不再是访问真实服务器，而是代理服务器。</li></ul><p>nginx可以当做反向代理服务器来使用：</p><ul><li>我们需要提前在nginx中配置好反向代理的规则，不同的请求，交给不同的真实服务器处理</li><li>当请求到达nginx，nginx会根据已经定义的规则进行请求的转发，从而实现路由功能</li></ul><p>利用反向代理，就可以解决我们前面所说的端口问题，如图</p><p><img src="/2020/01/19/nginx-shi-yong-xin-de/1526016663674.png" alt></p><h3 id="安装和使用"><a href="#安装和使用" class="headerlink" title="安装和使用"></a>安装和使用</h3><blockquote><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3></blockquote><p>安装非常简单，把课前资料提供的nginx直接解压即可，绿色免安装!</p><p>解压后，目录结构：</p><p><img src="/2020/01/19/nginx-shi-yong-xin-de/Nginx%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97%5C1579422396676.png" alt></p><ol><li>conf：配置目录</li><li>contrib：第三方依赖</li><li>html：默认的静态资源目录，类似于tomcat的webapps</li><li>logs：日志目录</li><li>nginx.exe：启动程序。可双击运行，但不建议这么做。</li></ol><blockquote><h3 id="反向代理配置"><a href="#反向代理配置" class="headerlink" title="反向代理配置"></a>反向代理配置</h3></blockquote><p>示例：</p><p><img src="/2020/01/19/nginx-shi-yong-xin-de/1526188831504.png" alt></p><p>nginx中的每个server就是一个反向代理配置，可以有多个server</p><p>完整配置：</p><pre><code>#user  nobody;worker_processes  1;events {    worker_connections  1024;}http {    include       mime.types;    default_type  application/octet-stream;    sendfile        on;    keepalive_timeout  65;    gzip  on;    server {        listen       80;        server_name  manage.leyou.com;        proxy_set_header X-Forwarded-Host $host;        proxy_set_header X-Forwarded-Server $host;        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;        location / {            proxy_pass http://127.0.0.1:9001;            proxy_connect_timeout 600;            proxy_read_timeout 600;        }    }    server {        listen       80;        server_name  api.leyou.com;        proxy_set_header X-Forwarded-Host $host;        proxy_set_header X-Forwarded-Server $host;        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;        location / {            proxy_pass http://127.0.0.1:10010;            proxy_connect_timeout 600;            proxy_read_timeout 600;        }    }}</code></pre><blockquote><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3></blockquote><p>nginx可以通过命令行来启动，操作命令：</p><ul><li>启动：<code>start nginx.exe</code></li><li>停止：<code>nginx.exe -s stop</code></li><li>重新加载：<code>nginx.exe -s reload</code></li></ul><p>启动过程会闪烁一下，启动成功后，任务管理器中会有两个nginx进程：</p><p><img src="/2020/01/19/nginx-shi-yong-xin-de/sshot-1.png" alt></p><p>现在实现了域名访问网站了，中间的流程是怎样的呢？</p><p><img src="/2020/01/19/nginx-shi-yong-xin-de/1526189945180.png" alt></p><ol><li><p>浏览器准备发起请求，访问<a href="http://mamage.leyou.com，但需要进行域名解析" target="_blank" rel="noopener">http://mamage.leyou.com，但需要进行域名解析</a></p></li><li><p>优先进行本地域名解析，因为我们修改了hosts，所以解析成功，得到地址：127.0.0.1</p></li><li><p>请求被发往解析得到的ip，并且默认使用80端口：<a href="http://127.0.0.1:80" target="_blank" rel="noopener">http://127.0.0.1:80</a></p><p>本机的nginx一直监听80端口，因此捕获这个请求</p></li><li><p>nginx中配置了反向代理规则，将manage.leyou.com代理到127.0.0.1:9001，因此请求被转发</p></li><li><p>后台系统的webpack server监听的端口是9001，得到请求并处理，完成后将响应返回到nginx</p></li><li><p>nginx将得到的结果返回到浏览器使用场景</p></li></ol><h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><p>平时开发分布式系统的时候,作为自己的调试环境往往是只有一台电脑的,这个时候为了配置和实际生产环境一致,我们的配置都是和线上一致,这个时候就需要使用hosts文件中的映射和nginx配合使用实现一台电脑上部署一个分布式的系统的多个部分的效果.如下配置文件:</p><pre><code>#cms页面预览    upstream cms_server_pool{        server 127.0.0.1:31001 weight=10;    }    upstream static_server_pool{        server 127.0.0.1:91 weight=10;    }        #前端动态门户     upstream dynamic_portal_server_pool{         server 127.0.0.1:10000 weight=10;     }    #前端ucenter     upstream ucenter_server_pool{         #server 127.0.0.1:7081 weight=10;         server 127.0.0.1:13000 weight=10;     }    #后台搜索（公开api）     upstream search_server_pool{        server 127.0.0.1:40100 weight=10;     }    #媒体服务    upstream video_server_pool {        server 127.0.0.1:90 weight=10;    }    #认证服务     upstream auth_server_pool{        server 127.0.0.1:40400 weight=10;    }    server {        listen       91;            server_name  localhost;        ssi on;        ssi_silent_errors on;        location /static/company/ {            alias    D:/皇家java教程/04-java项目/xcEduUI01/static/company/;        }        location /static/teacher/ {            alias    D:/皇家java教程/04-java项目/xcEduUI01/static/teacher/;        }        location /static/stat/ {            alias    D:/皇家java教程/04-java项目/xcEduUI01/static/stat/;        }        location /course/detail/ {            alias   D:/皇家java教程/04-java项目/xcEduUI01/static/course/detail/;        }        #分类信息         location /static/category {            default_type application/json;            alias   D:/皇家java教程/04-java项目/xcEduUI01/static/category/;        }    }    server {        listen       80;            server_name  www.xuecheng.com;        ssi on;        ssi_silent_errors on;        location / {            alias   D:/皇家java教程/04-java项目/xcEduUI01/xc-ui-pc-static-portal/;            index  index.html;        }        location /cms/preview/{            proxy_pass http://cms_server_pool/cms/preview/;        }                location /static/company/{            proxy_pass http://static_server_pool;        }        location /static/teacher/{            proxy_pass http://static_server_pool;        }        location /static/stat/{            proxy_pass http://static_server_pool;        }        location /static/course/detail/{            proxy_pass http://static_server_pool;        }        location /static/img/{            alias D:/皇家java教程/04-java项目/xcEduUI01/xc-ui-pc-static-portal/img/;        }        location /static/css/{            alias D:/皇家java教程/04-java项目/xcEduUI01/xc-ui-pc-static-portal/css/;        }        location /static/js/{            alias D:/皇家java教程/04-java项目/xcEduUI01/xc-ui-pc-static-portal/js/;        }        location /static/plugins/{            alias D:/皇家java教程/04-java项目/xcEduUI01/xc-ui-pc-static-portal/plugins/;            add_header Access-Control-Allow-Origin http://ucenter.xuecheng.com;            add_header Access-Control-Allow-Credentials true;            add_header Access-Control-Allow-Methods GET;        }        #前端门户页面搜索        location ^~ /course/search {            proxy_pass http://dynamic_portal_server_pool;        }        #后端搜索服务        location /openapi/search/ {        proxy_pass http://search_server_pool/search/;        }        #分类信息         location /static/category/ {            proxy_pass http://static_server_pool;        }        #开发环境webpack定时加载此文件         location ^~ /__webpack_hmr {            proxy_pass http://dynamic_portal_server_pool/__webpack_hmr;        }        #开发环境nuxt访问_nuxt         location ^~ /_nuxt/ {            proxy_pass http://dynamic_portal_server_pool/_nuxt/;        }        }    #学成网媒体服务代理     map $http_origin $origin_list{        default http://www.xuecheng.com;        &quot;~http://www.xuecheng.com&quot; http://www.xuecheng.com;        &quot;~http://ucenter.xuecheng.com&quot; http://ucenter.xuecheng.com;    }    #学成网媒体服务代理    server {        listen 80;        server_name video.xuecheng.com;        location /video {            proxy_pass http://video_server_pool;            add_header Access‐Control‐Allow‐Origin $origin_list;            #add_header Access‐Control‐Allow‐Origin *;            add_header Access‐Control‐Allow‐Credentials true;            add_header Access‐Control‐Allow‐Methods GET;        }    }    server{        listen        90;        server_name    localhost;        #视频目录        location /video/ {            alias    D:/皇家java教程/04-java项目/xcEduUI01/video/;        }    }    #学成网用户中心    server {        listen 80;        server_name ucenter.xuecheng.com;        #认证         location ^~ /openapi/auth/ {            proxy_pass http://auth_server_pool/auth/;        }        #个人中心         location / {             proxy_pass http://ucenter_server_pool;        }    }</code></pre>]]></content>
      
      
      <categories>
          
          <category> tool </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>红黑树的建立与维护</title>
      <link href="/2020/01/16/hong-hei-shu-de-jian-li-yu-wei-hu/"/>
      <url>/2020/01/16/hong-hei-shu-de-jian-li-yu-wei-hu/</url>
      
        <content type="html"><![CDATA[<h3 id="红黑树介绍"><a href="#红黑树介绍" class="headerlink" title="红黑树介绍"></a>红黑树介绍</h3><p>红黑树(Red-Black Tree，简称R-B Tree)，它一种特殊的二叉查找树。<br>红黑树是特殊的二叉查找树，意味着它满足二叉查找树的特征：任意一个节点所包含的键值，大于等于左孩子的键值，小于等于右孩子的键值。<br>除了具备该特性之外，红黑树还包括许多额外的信息。</p><p>红黑树的每个节点上都有存储位表示节点的颜色，颜色是红(Red)或黑(Black)。</p><p>红黑树的特性:<br>(1) 每个节点或者是黑色，或者是红色。<br>(2) 根节点是黑色。<br>(3) 每个叶子节点是黑色。 [注意：这里叶子节点，是指为空的叶子节点！]<br>(4) 如果一个节点是红色的，则它的子节点必须是黑色的。<br>(5) 从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。</p><p>关于它的特性，需要注意的是：<br>第一，特性(3)中的叶子节点，是只为空(NIL或null)的节点。<br>第二，特性(5)，确保没有一条路径会比其他路径长出俩倍。因而，红黑树是相对是接近平衡的二叉树。</p><p>红黑树的主要是想对2-3查找树进行编码，尤其是对2-3查找树中的3-nodes节点添加额外的信息。红黑树中将节点之间的链接分为两种不同类型，红色链接，他用来链接两个2-nodes节点来表示一个3-nodes节点。黑色链接用来链接普通的2-3节点。特别的，使用红色链接的两个2-nodes来表示一个3-nodes节点，并且向左倾斜，即一个2-node是另一个2-node的左子节点。这种做法的好处是查找的时候不用做任何修改，和普通的二叉查找树相同。</p><p>根据以上描述，红黑树定义如下：</p><p>红黑树是一种具有红色和黑色链接的平衡查找树，同时满足：</p><p>1.红色节点向左倾斜<br>2.一个节点不可能有两个红色链接<br>3.整个树完全黑色平衡，即从根节点到所以叶子结点的路径上，黑色链接的个数都相同。</p><p>下图可以看到红黑树其实是2-3树的另外一种表现形式：如果我们将红色的连线水平绘制，那么他链接的两个2-node节点就是2-3树中的一个3-node节点了。</p><p><img src="/2020/01/16/hong-hei-shu-de-jian-li-yu-wei-hu/1.png" alt></p><h3 id="红黑树java代码实现"><a href="#红黑树java代码实现" class="headerlink" title="红黑树java代码实现"></a>红黑树java代码实现</h3><pre><code>import java.util.ArrayList;public class RBTree&lt;K extends Comparable&lt;K&gt;, V&gt; {    private static final boolean RED = true;    private static final boolean BLACK = false;    private class Node{        public K key;        public V value;        public Node left, right;        public boolean color;        public Node(K key, V value){            this.key = key;            this.value = value;            left = null;            right = null;            color = RED;        }    }    private Node root;    private int size;    public RBTree(){        root = null;        size = 0;    }    public int getSize(){        return size;    }    public boolean isEmpty(){        return size == 0;    }    // 判断节点node的颜色    private boolean isRed(Node node){        if(node == null)            return BLACK;        return node.color;    }    //   node                     x    //  /   \     左旋转         /  \    // T1   x   ---------&gt;   node   T3    //     / \              /   \    //    T2 T3            T1   T2    private Node leftRotate(Node node){        Node x = node.right;        // 左旋转        node.right = x.left;        x.left = node;        x.color = node.color;        node.color = RED;        return x;    }    //     node                   x    //    /   \     右旋转       /  \    //   x    T2   -------&gt;   y   node    //  / \                       /  \    // y  T1                     T1  T2    private Node rightRotate(Node node){        Node x = node.left;        // 右旋转        node.left = x.right;        x.right = node;        x.color = node.color;        node.color = RED;        return x;    }    // 颜色翻转    private void flipColors(Node node){        node.color = RED;        node.left.color = BLACK;        node.right.color = BLACK;    }    // 向红黑树中添加新的元素(key, value)    public void add(K key, V value){        root = add(root, key, value);        root.color = BLACK; // 最终根节点为黑色节点    }    // 向以node为根的红黑树中插入元素(key, value)，递归算法    // 返回插入新节点后红黑树的根    private Node add(Node node, K key, V value){        if(node == null){            size ++;            return new Node(key, value); // 默认插入红色节点        }        if(key.compareTo(node.key) &lt; 0)            node.left = add(node.left, key, value);        else if(key.compareTo(node.key) &gt; 0)            node.right = add(node.right, key, value);        else // key.compareTo(node.key) == 0            node.value = value;        if (isRed(node.right) &amp;&amp; !isRed(node.left))            node = leftRotate(node);        if (isRed(node.left) &amp;&amp; isRed(node.left.left))            node = rightRotate(node);        if (isRed(node.left) &amp;&amp; isRed(node.right))            flipColors(node);        return node;    }    // 返回以node为根节点的二分搜索树中，key所在的节点    private Node getNode(Node node, K key){        if(node == null)            return null;        if(key.equals(node.key))            return node;        else if(key.compareTo(node.key) &lt; 0)            return getNode(node.left, key);        else // if(key.compareTo(node.key) &gt; 0)            return getNode(node.right, key);    }    public boolean contains(K key){        return getNode(root, key) != null;    }    public V get(K key){        Node node = getNode(root, key);        return node == null ? null : node.value;    }    public void set(K key, V newValue){        Node node = getNode(root, key);        if(node == null)            throw new IllegalArgumentException(key + &quot; doesn&#39;t exist!&quot;);        node.value = newValue;    }    // 返回以node为根的二分搜索树的最小值所在的节点    private Node minimum(Node node){        if(node.left == null)            return node;        return minimum(node.left);    }    // 删除掉以node为根的二分搜索树中的最小节点    // 返回删除节点后新的二分搜索树的根    private Node removeMin(Node node){        if(node.left == null){            Node rightNode = node.right;            node.right = null;            size --;            return rightNode;        }        node.left = removeMin(node.left);        return node;    }    // 从二分搜索树中删除键为key的节点    public V remove(K key){        Node node = getNode(root, key);        if(node != null){            root = remove(root, key);            return node.value;        }        return null;    }    private Node remove(Node node, K key){        if( node == null )            return null;        if( key.compareTo(node.key) &lt; 0 ){            node.left = remove(node.left , key);            return node;        }        else if(key.compareTo(node.key) &gt; 0 ){            node.right = remove(node.right, key);            return node;        }        else{   // key.compareTo(node.key) == 0            // 待删除节点左子树为空的情况            if(node.left == null){                Node rightNode = node.right;                node.right = null;                size --;                return rightNode;            }            // 待删除节点右子树为空的情况            if(node.right == null){                Node leftNode = node.left;                node.left = null;                size --;                return leftNode;            }            // 待删除节点左右子树均不为空的情况            // 找到比待删除节点大的最小节点, 即待删除节点右子树的最小节点            // 用这个节点顶替待删除节点的位置            Node successor = minimum(node.right);            successor.right = removeMin(node.right);            successor.left = node.left;            node.left = node.right = null;            return successor;        }    }    public static void main(String[] args){        System.out.println(&quot;Pride and Prejudice&quot;);        ArrayList&lt;String&gt; words = new ArrayList&lt;&gt;();        if(FileOperation.readFile(&quot;pride-and-prejudice.txt&quot;, words)) {            System.out.println(&quot;Total words: &quot; + words.size());            RBTree&lt;String, Integer&gt; map = new RBTree&lt;&gt;();            for (String word : words) {                if (map.contains(word))                    map.set(word, map.get(word) + 1);                else                    map.add(word, 1);            }            System.out.println(&quot;Total different words: &quot; + map.getSize());            System.out.println(&quot;Frequency of PRIDE: &quot; + map.get(&quot;pride&quot;));            System.out.println(&quot;Frequency of PREJUDICE: &quot; + map.get(&quot;prejudice&quot;));        }        System.out.println();    }}</code></pre><h3 id="红黑树优点与使用场景"><a href="#红黑树优点与使用场景" class="headerlink" title="红黑树优点与使用场景"></a>红黑树优点与使用场景</h3><p>红黑树并不追求“完全平衡”——它只要求部分地达到平衡要求，降低了对旋转的要求，从而提高了性能。</p><p>红黑树能够以O(log2 n) 的时间复杂度进行搜索、插入、删除操作。此外，由于它的设计，任何不平衡都会在三次旋转之内解决。当然，还有一些更好的，但实现起来更复杂的数据结构 能够做到一步旋转之内达到平衡，但红黑树能够给我们一个比较“便宜”的解决方案。红黑树的算法时间复杂度和AVL相同，但统计性能比AVL树更高。</p><p>当然，红黑树并不适应所有应用树的领域。如果数据基本上是静态的，那么让他们待在他们能够插入，并且不影响平衡的地方会具有更好的性能。如果数据完全是静态的，例如，做一个哈希表，性能可能会更好一些。</p><p>在实际的系统中，例如，需要使用动态规则的防火墙系统，使用红黑树而不是散列表被实践证明具有更好的伸缩性。</p><p>在jDK1.8以后HashMap结构就引入了红黑树,在链表长度达到八的时候将链表转化为红黑树.</p>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>队列的用法</title>
      <link href="/2020/01/16/dui-lie-de-yong-fa/"/>
      <url>/2020/01/16/dui-lie-de-yong-fa/</url>
      
        <content type="html"><![CDATA[<h3 id="1、队列的基本概念"><a href="#1、队列的基本概念" class="headerlink" title="1、队列的基本概念"></a><strong>1、队列的基本概念</strong></h3><p>队列（queue）是一种特殊的线性表，特殊之处在于它只允许在表的前端（front）进行删除操作，而在表的后端（rear）进行插入操作，和栈一样，队列是一种操作受限制的线性表。进行插入操作的端称为队尾，进行删除操作的端称为队头。队列中没有元素时，称为空队列。队列的数据元素又称为队列元素。在队列中插入一个队列元素称为入队，从队列中删除一个队列元素称为出队。因为队列只允许在一端插入，在另一端删除，所以只有最早进入队列的元素才能最先从队列中删除，故队列又称为先进先出（FIFO—first in first out）线性表。比如我们去电影院排队买票，第一个进入排队序列的都是第一个买到票离开队列的人，而最后进入排队序列排队的都是最后买到票的。在比如在计算机操作系统中，有各种队列在安静的工作着，比如打印机在打印列队中等待打印。</p><p><strong>队列分为：</strong></p><p>①、单向队列（Queue）：只能在一端插入数据，另一端删除数据。</p><p>②、双向队列（Deque）：每一端都可以进行插入数据和删除数据操作。</p><p>这里我们还会介绍一种队列——优先级队列，优先级队列是比栈和队列更专用的数据结构，在优先级队列中，数据项按照关键字进行排序，关键字最小（或者最大）的数据项往往在队列的最前面，而数据项在插入的时候都会插入到合适的位置以确保队列的有序。</p><h4 id="Java中实现好的队列分类"><a href="#Java中实现好的队列分类" class="headerlink" title="Java中实现好的队列分类:"></a>Java中实现好的队列分类:</h4><p><strong>非阻塞队列</strong></p><p>PriorityQueue优先级队列：线程不安全、基于数组存储构成优先级堆 添加元素为空抛出异常 每次操作，对元素进行排序，取优先级最高的元素放在队首，保证获取的元素按优先级从高到低<br>siftUpUsingComparator与siftDownUsingComparator ！！！！！<br>add、offer 添加元素 peek(获取元素不删除)、poll（获取元素并删除）</p><p>ConcurrentLinkedQueue：基于链表存储、线程安全</p><p><strong>阻塞队列</strong></p><p>在获取元素（take）时，若为空，阻塞当前线程（Lock condition的await）,当添加值进去后释放（signal）</p><p>ArrayBlockingQueue ：一个由数组支持的有界队列。LinkedBlockingQueue ：一个由链接节点支持的可选有界队列。PriorityBlockingQueue ：一个由优先级堆支持的无界优先级队列。DelayQueue ：一个由优先级堆支持的、基于时间的调度队列。</p><p><strong>双向队列</strong></p><p>可以向头跟尾放入与取出元素</p><p>ArrayDeque双向队列 基于数据 线程不安全\LinkedList基于链表</p><p>前两者属于单向队列，队尾添加元素，队头取出元素</p><h3 id="2、Java模拟单向队列实现"><a href="#2、Java模拟单向队列实现" class="headerlink" title="2、Java模拟单向队列实现"></a><strong>2、Java模拟单向队列实现</strong></h3><p>Java实现代码如下：</p><pre><code>public interface Queue&lt;E&gt; {    int getSize();    boolean isEmpty();    void enqueue(E e);    E dequeue();    E getFront();}public class ArrayQueue&lt;E&gt; implements Queue&lt;E&gt; {    private Array&lt;E&gt; array;    public ArrayQueue(int capacity){        array = new Array&lt;&gt;(capacity);    }    public ArrayQueue(){        array = new Array&lt;&gt;();    }    @Override    public int getSize(){        return array.getSize();    }    @Override    public boolean isEmpty(){        return array.isEmpty();    }    public int getCapacity(){        return array.getCapacity();    }    @Override    public void enqueue(E e){        array.addLast(e);    }    @Override    public E dequeue(){        return array.removeFirst();    }    @Override    public E getFront(){        return array.getFirst();    }    @Override    public String toString(){        StringBuilder res = new StringBuilder();        res.append(&quot;Queue: &quot;);        res.append(&quot;front [&quot;);        for(int i = 0 ; i &lt; array.getSize() ; i ++){            res.append(array.get(i));            if(i != array.getSize() - 1)                res.append(&quot;, &quot;);        }        res.append(&quot;] tail&quot;);        return res.toString();    }    public static void main(String[] args) {        ArrayQueue&lt;Integer&gt; queue = new ArrayQueue&lt;&gt;();        for(int i = 0 ; i &lt; 10 ; i ++){            queue.enqueue(i);            System.out.println(queue);            if(i % 3 == 2){                queue.dequeue();                System.out.println(queue);            }        }    }}</code></pre><h3 id="3、双端队列"><a href="#3、双端队列" class="headerlink" title="3、双端队列"></a><strong>3、双端队列</strong></h3><p>双端队列就是一个两端都是结尾或者开头的队列， 队列的每一端都可以进行插入数据项和移除数据项，这些方法可以叫做：</p><p><strong>insertRight()、insertLeft()、removeLeft()、removeRight()</strong></p><p>如果严格禁止调用insertLeft()和removeLeft()（或禁用右端操作），那么双端队列的功能就和前面讲的栈功能一样。如果严格禁止调用insertLeft()和removeRight(或相反的另一对方法)，那么双端队列的功能就和单向队列一样了。</p><h3 id="4、优先级队列"><a href="#4、优先级队列" class="headerlink" title="4、优先级队列"></a><strong>4、优先级队列</strong></h3><p>优先级队列（priority queue）是比栈和队列更专用的数据结构，在优先级队列中，数据项按照关键字进行排序，关键字最小（或者最大）的数据项往往在队列的最前面，而数据项在插入的时候都会插入到合适的位置以确保队列的有序。</p><p>优先级队列 是0个或多个元素的集合，每个元素都有一个优先权，对优先级队列执行的操作有：</p><p><strong>（1）</strong>查找<strong>（2）</strong>插入一个新元素<strong>（3）</strong>删除</p><p>一般情况下，查找操作用来搜索优先权最大的元素，删除操作用来删除该元素 。对于优先权相同的元素，可按先进先出次序处理或按任意优先权进行。这里我们用数组实现优先级队列，这种方法插入比较慢，但是它比较简单，适用于数据量比较小并且不是特别注重插入速度的情况。后面我们会讲解堆，用堆的数据结构来实现优先级队列，可以相当快的插入数据。<strong>数组实现优先级队列，声明为int类型的数组，关键字是数组里面的元素，在插入的时候按照从大到小的顺序排列，也就是越小的元素优先级越高。</strong></p><pre><code>public class PriorityQue {    private int maxSize;    private int[] priQueArray;    private int nItems;    public PriorityQue(int s){        maxSize = s;        priQueArray = new int[maxSize];        nItems = 0;    }    //插入数据    public void insert(int value){        int j;        if(nItems == 0){            priQueArray[nItems++] = value;        }else{            j = nItems -1;            //选择的排序方法是插入排序，按照从大到小的顺序排列，越小的越在队列的顶端            while(j &gt;=0 &amp;&amp; value &gt; priQueArray[j]){                priQueArray[j+1] = priQueArray[j];                j--;            }            priQueArray[j+1] = value;            nItems++;        }    }    //移除数据,由于是按照大小排序的，所以移除数据我们指针向下移动    //被移除的地方由于是int类型的，不能设置为null，这里的做法是设置为 -1    public int remove(){        int k = nItems -1;        int value = priQueArray[k];        priQueArray[k] = -1;//-1表示这个位置的数据被移除了        nItems--;        return value;    }    //查看优先级最高的元素    public int peekMin(){        return priQueArray[nItems-1];    }    //判断是否为空    public boolean isEmpty(){        return (nItems == 0);    }    //判断是否满了    public boolean isFull(){        return (nItems == maxSize);    }}</code></pre><p>insert() 方法，先检查队列中是否有数据项，如果没有，则直接插入到下标为0的单元里，否则，从数组顶部开始比较，找到比插入值小的位置进行插入，并把 nItems 加1.</p><p>remove() 方法直接获取顶部元素。</p><p>优先级队列的插入操作需要 O(N)的时间，而删除操作则需要O(1) 的时间，后面会讲解如何通过 堆 来改进插入时间。</p><h3 id="5、队列的使用场景"><a href="#5、队列的使用场景" class="headerlink" title="5、队列的使用场景"></a>5、队列的使用场景</h3><p>我们熟知的消息队列就是使用队列的基本原理完成设计，使用场景如下：</p><p><strong>1、异步处理</strong></p><p><strong>场景说明</strong>：用户注册后，需要发注册邮件和注册短信。传统的做法有两种：串行的方式和并行方式。</p><p><strong>串行方式</strong>：将注册信息写入数据库成功后，发送注册邮件，再发送注册短信。以上三个任务全部完成后，返回给客户。</p><p><img src="/2020/01/16/dui-lie-de-yong-fa/1.png" alt></p><p><strong>并行方式</strong>：将注册信息写入数据库成功后，发送注册邮件的同时，发送注册短信。以上三个任务完成后，返回给客户端。与串行的差别是，并行的方式可以提高处理的时间。</p><p><img src="/2020/01/16/dui-lie-de-yong-fa/2.png" alt></p><p>假设三个业务节点每个使用50毫秒钟，不考虑网络等其他开销，则串行方式的时间是150毫秒，并行的时间可能是100毫秒。</p><p>因为CPU在单位时间内处理的请求数是一定的，假设CPU1秒内吞吐量是100次。则串行方式1秒内CPU可处理的请求量是7次（1000/150）。并行方式处理的请求量是10次（1000/100）。</p><p><strong>小结</strong>：如以上案例描述，传统的方式系统的性能（并发量，吞吐量，响应时间）会有瓶颈。如何解决这个问题呢？</p><p>引入消息队列，将不是必须的业务逻辑，异步处理。改造后的架构如下：</p><p><img src="/2020/01/16/dui-lie-de-yong-fa/3.png" alt></p><p>按照以上约定，用户的响应时间相当于是注册信息写入数据库的时间，也就是50毫秒。注册邮件，发送短信写入消息队列后，直接返回，因此写入消息队列的速度很快，基本可以忽略，因此用户的响应时间可能是50毫秒。因此架构改变后，系统的吞吐量提高到每秒20QPS。比串行提高了3倍，比并行提高了两倍！</p><p><strong>2、应用解耦</strong></p><p><strong>场景说明</strong>：用户下单后，订单系统需要通知库存系统。传统的做法是，订单系统调用库存系统的接口。如下图：</p><p><img src="/2020/01/16/dui-lie-de-yong-fa/4.png" alt></p><p><strong>传统模式的缺点</strong>：</p><p>假如库存系统无法访问，则订单减库存将失败，从而导致订单失败，订单系统与库存系统耦合。</p><p>如何解决以上问题呢？引入应用消息队列后的方案，如下图：</p><p><img src="/2020/01/16/dui-lie-de-yong-fa/5.png" alt></p><p><strong>订单系统</strong>：用户下单后，订单系统完成持久化处理，将消息写入消息队列，返回用户订单下单成功</p><p><strong>库存系统</strong>：订阅下单的消息，采用拉/推的方式，获取下单信息，库存系统根据下单信息，进行库存操作</p><p>假如：在下单时库存系统不能正常使用。也不影响正常下单，因为下单后，订单系统写入消息队列就不再关心其他的后续操作了。实现订单系统与库存系统的应用解耦。</p><p><strong>3、流量削锋</strong></p><p>流量削锋也是消息队列中的常用场景，一般在秒杀或团抢活动中使用广泛！</p><p><strong>应用场景</strong>：秒杀活动，一般会因为流量过大，导致流量暴增，应用挂掉。为解决这个问题，一般需要在应用前端加入消息队列。</p><p>可以控制活动的人数，可以缓解短时间内高流量压垮应用。</p><p><img src="/2020/01/16/dui-lie-de-yong-fa/6.png" alt></p><p>用户的请求，服务器接收后，首先写入消息队列。假如消息队列长度超过最大数量，则直接抛弃用户请求或跳转到错误页面。</p><p>秒杀业务根据消息队列中的请求信息，再做后续处理。</p><p><strong>4、日志处理</strong></p><p>日志处理是指将消息队列用在日志处理中，比如Kafka的应用，解决大量日志传输的问题。架构简化如下：</p><p><img src="/2020/01/16/dui-lie-de-yong-fa/7.png" alt></p><p>日志采集客户端，负责日志数据采集，定时写受写入Kafka队列；Kafka消息队列，负责日志数据的接收，存储和转发；日志处理应用：订阅并消费kafka队列中的日志数据。</p><p>以下是新浪kafka日志处理应用案例：</p><p><img src="/2020/01/16/dui-lie-de-yong-fa/8.png" alt></p><p><strong>Kafka</strong>：接收用户日志的消息队列；</p><p><strong>Logstash</strong>：做日志解析，统一成JSON输出给Elasticsearch；</p><p><strong>Elasticsearch</strong>：实时日志分析服务的核心技术，一个schemaless，实时的数据存储服务，通过index组织数据，兼具强大的搜索和统计功能；</p><p><strong>Kibana</strong>：基于Elasticsearch的数据可视化组件，超强的数据可视化能力是众多公司选择ELK stack的重要原因。</p><p><strong>5、消息通讯</strong></p><p>消息通讯是指，消息队列一般都内置了高效的通信机制，因此也可以用在纯的消息通讯。比如实现点对点消息队列，或者聊天室等。</p><p><strong>点对点通讯</strong>：</p><p><img src="/2020/01/16/dui-lie-de-yong-fa/9.png" alt></p><p>客户端A和客户端B使用同一队列，进行消息通讯。</p><p><strong>聊天室通讯</strong>：</p><p><img src="/2020/01/16/dui-lie-de-yong-fa/10.png" alt></p><p>客户端A，客户端B，客户端N订阅同一主题，进行消息发布和接收。实现类似聊天室效果。</p><p>以上实际是消息队列的两种消息模式，点对点或发布订阅模式。模型为示意图，供参考。<br>除了这些，针对当前互联网公司的技术需求以及结合主流技术，我自己整理了一套系统的架构技术体系，当你技术过硬的时候，能够解决技术问题才会服众。不少公司都很重视高并发高可用的技术，特别是一线互联网公司，分布式、JVM、spring源码分析、微服务等知识点已是面试的必考题，这些东西可能你们平时在工作中接触过，但是缺少的全面系统的学习，加入<strong>后端开发群：943918498</strong>，或是关注<strong>微信公众号：Java资讯库，回复“架构”</strong>，免费领取架构资料。</p>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>栈的性质及一些使用场景</title>
      <link href="/2020/01/16/zhan-de-xing-zhi-ji-yi-xie-shi-yong-chang-jing/"/>
      <url>/2020/01/16/zhan-de-xing-zhi-ji-yi-xie-shi-yong-chang-jing/</url>
      
        <content type="html"><![CDATA[<h2 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h2><p>栈和队列其实是一个工具，他们传统的工具方法 工具类不同，他们是“思想”工具，<strong>栈是后进先出</strong>。</p><p><img src="/2020/01/16/zhan-de-xing-zhi-ji-yi-xie-shi-yong-chang-jing/4.png" alt></p><p><img src="https://user-gold-cdn.xitu.io/2020/1/4/16f6f70211551f99?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="栈结构概念"></p><h2 id="常见的使用栈的场景"><a href="#常见的使用栈的场景" class="headerlink" title="常见的使用栈的场景"></a>常见的使用栈的场景</h2><h3 id="递归"><a href="#递归" class="headerlink" title="递归"></a>递归</h3><blockquote><p>从前山上有座庙，庙里有个老和尚和小和尚，老和尚给小和尚讲故事：“从前山上有座庙……”</p></blockquote><p>有名的斐波那契数列，手动地计算相当困难，即便有计算器在手。而在编程语言中，使用递归可以很好地解决这个难题：</p><pre><code>function F(n) {  return F(n-1) + F(n-2);}</code></pre><p>重点来了，计算机如何实现递归？这是一个很笼统的概念，因为等于这个加那个，那个再加这个…… 知其所以然而不知其然。<strong>应用栈的结构，我们可以把未知的结果推入栈内，在弹出的时候逐个计算。</strong> 如下图</p><p><img src="/2020/01/16/zhan-de-xing-zhi-ji-yi-xie-shi-yong-chang-jing/3.png" alt></p><p>代码解释：</p><pre><code>function recursion() {  // 调用栈  const stack = [];  // 解析时  // 推入栈  // 一般来说，栈有大小限制，如果自己写了个无限递归的函数，那调用栈一直增加，最后溢出  for (let i = n; i &gt; 0; i--) {    stack.push(F(i));  }  // 执行时  // 后入先出，弹出  for (let i = 3; i &lt;= n; i++) {    F(n-2) = stack.pop();    F(n-1) = stack.pop();    F(n) = F(n-1) + F(n-2)    // 计算完成后再推入栈内    stack.push(F(n))  }  // 执行完成，栈内剩下最终结果，弹出并返回  if (n) return stack.pop()}</code></pre><h3 id="四则运算"><a href="#四则运算" class="headerlink" title="四则运算"></a>四则运算</h3><blockquote><p>数学老师：“先乘除，后加减，有括号先算括号。”</p></blockquote><p>分析下计算机四则运算的步骤：</p><ol><li>定义运算符功能</li><li>优先级：乘除 &gt;&gt; 加减</li><li>有括号优先计算括号</li></ol><p>示例：<code>(1 + 2) x 3 - 4 ÷ 5</code></p><p>括号内优先计算，立马得出结果，咋一想还蛮符合队列的规则，先入先出嘛。但在有多个括号的情况下，优先计算最里面的括号，这样就只能推入栈中慢慢计算了。</p><p>但是怎么优雅地推入栈内计算，有个伟大的科学家解决了这个难题，波兰逻辑学家想到了一种<strong>不需要括号的后缀表达式，称之为逆波兰</strong>。</p><p>示例后缀表达式：<code>12+3*45/-</code></p><p>后缀表达式计算过程：</p><p><img src="/2020/01/16/zhan-de-xing-zhi-ji-yi-xie-shi-yong-chang-jing/2.png" alt></p><p>转化后的计算简直不要太简单，来看看又是如何利用栈来转的：</p><p><img src="/2020/01/16/zhan-de-xing-zhi-ji-yi-xie-shi-yong-chang-jing/1.png" alt></p><p># </p>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>HBase建立二级索引的几种方式</title>
      <link href="/2020/01/16/hbase-jian-li-er-ji-suo-yin-de-ji-chong-fang-shi/"/>
      <url>/2020/01/16/hbase-jian-li-er-ji-suo-yin-de-ji-chong-fang-shi/</url>
      
        <content type="html"><![CDATA[<h3 id="为什么需要HBse二级索引"><a href="#为什么需要HBse二级索引" class="headerlink" title="为什么需要HBse二级索引"></a><strong>为什么需要HBse二级索引</strong></h3><p><strong>HBase里面只有rowkey作为一级索引，</strong> 如果要对库里的非rowkey字段进行数据检索和查询， 往往要通过MapReduce/Spark等分布式计算框架进行，硬件资源消耗和时间延迟都会比较高。</p><p><strong>只依赖rowkey作为索引,最高可以支持数据量为十万级的数据快速响应.</strong>为了HBase的数据查询更高效、适应更多的场景， <strong>诸如使用非rowkey字段检索也能做到秒级响应，或者支持各个字段进行模糊查询和多字段组合查询等</strong>， 因此需要在HBase上面构建二级索引， 以满足现实中更复杂多样的业务需求。</p><h4 id="1-MapReduce方案"><a href="#1-MapReduce方案" class="headerlink" title="1.MapReduce方案"></a>1.MapReduce方案</h4><p>自己编写MapReduce实现二级索引,相对成本较高,但是可以将MapReduce的并发特性运用起来,相对开发成本较高,而且以来开发人员代码水平.</p><h4 id="2-Phoenix"><a href="#2-Phoenix" class="headerlink" title="2.Phoenix"></a>2.Phoenix</h4><p><strong>Apache Phoenix：</strong> 功能围绕着SQL on hbase，支持和兼容多个hbase版本， 二级索引只是其中一块功能。 二级索引的创建和管理直接有SQL语法支持，使用起来很简便， 该项目目前社区活跃度和版本更新迭代情况都比较好。</p><p><strong>ApachePhoenix在目前开源的方案中，是一个比较优的选择。主打SQL on HBase ， 基于SQL能完成HBase的CRUD操作，支持JDBC协议</strong>。 Apache Phoenix在Hadoop生态里面位置：</p><p><strong>Phoenix二级索引特点：</strong></p><p>Covered Indexes(覆盖索引) ：把关注的数据字段也附在索引表上，只需要通过索引表就能返回所要查询的数据（列）， 所以索引的列必须包含所需查询的列(SELECT的列和WHRER的列)。</p><p>Functional indexes(函数索引)： 索引不局限于列，支持任意的表达式来创建索引。</p><p>Global indexes(全局索引)：适用于读多写少场景。通过维护全局索引表，所有的更新和写操作都会引起索引的更新，写入性能受到影响。 在读数据时，Phoenix SQL会基于索引字段，执行快速查询。</p><p>Local indexes(本地索引)：适用于写多读少场景。 在数据写入时，索引数据和表数据都会存储在本地。在数据读取时， 由于无法预先确定region的位置，所以在读取数据时需要检查每个region（以找到索引数据），会带来一定性能（网络）开销。</p><p><strong>该方案可适用于数量级百万甚至千万.</strong></p><h4 id="3-ElasticSearch"><a href="#3-ElasticSearch" class="headerlink" title="3.ElasticSearch"></a>3.ElasticSearch</h4><p>总所周知,ElasticSearch是一个基于Lucene开发的分布式搜索引擎,可以做到对亿级数据实现快速查询,我们只需要将需要建立二级索引的字段与rowkey放入elasticsearch中,需要使用时,先在elasticsearch中通过字段查询到对应的rowkey,再利用hbase自身的rowkey索引搜索到对应的数据,<strong>该方案可以实现亿级数据的快速响应</strong>,是一种冷热分离思想的运用,</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop-HDFS与外界交互数据的工具</title>
      <link href="/2020/01/16/sqoop-hdfs-yu-wai-jie-jiao-hu-shu-ju-de-gong-ju/"/>
      <url>/2020/01/16/sqoop-hdfs-yu-wai-jie-jiao-hu-shu-ju-de-gong-ju/</url>
      
        <content type="html"><![CDATA[<h1 id="Sqoop简介"><a href="#Sqoop简介" class="headerlink" title="Sqoop简介"></a>Sqoop简介</h1><p>Sqoop是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。</p><p>Sqoop项目开始于2009年，最早是作为Hadoop的一个第三方模块存在，后来为了让使用者能够快速部署，也为了让开发人员能够更快速的迭代开发，Sqoop独立成为一个<a href="https://baike.baidu.com/item/Apache/6265" target="_blank" rel="noopener">Apache</a>项目。</p><p>Sqoop2的最新版本是1.99.7。请注意，2与1不兼容，且特征不完整，它并不打算用于生产部署。</p><h1 id="Sqoop原理"><a href="#Sqoop原理" class="headerlink" title="Sqoop原理"></a>Sqoop原理</h1><p>将导入或导出命令翻译成mapreduce程序来实现。</p><p>在翻译出的mapreduce中主要是对inputformat和outputformat进行定制。</p><h1 id="Sqoop安装"><a href="#Sqoop安装" class="headerlink" title="Sqoop安装"></a>Sqoop安装</h1><p>安装Sqoop的前提是已经具备Java和Hadoop的环境。</p><h2 id="3-1-下载并解压"><a href="#3-1-下载并解压" class="headerlink" title="3.1 下载并解压"></a>3.1 下载并解压</h2><p>1) 下载地址：<a href="http://mirrors.hust.edu.cn/apache/sqoop/1.4.6/" target="_blank" rel="noopener">http://mirrors.hust.edu.cn/apache/sqoop/1.4.6/</a></p><p>2) 上传安装包sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz到虚拟机中</p><p>3) 解压sqoop安装包到指定目录，如：</p><pre><code> tar -zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /opt/module/</code></pre><h2 id="3-2-修改配置文件"><a href="#3-2-修改配置文件" class="headerlink" title="3.2 修改配置文件"></a>3.2 修改配置文件</h2><p>Sqoop的配置文件与大多数大数据框架类似，在sqoop根目录下的conf目录中。</p><p>1) 重命名配置文件</p><pre><code> mv sqoop-env-template.sh sqoop-env.sh</code></pre><p>2) 修改配置文件</p><pre><code># sqoop-env.shexport HADOOP_COMMON_HOME=/opt/module/hadoop-2.7.2export HADOOP_MAPRED_HOME=/opt/module/hadoop-2.7.2export HIVE_HOME=/opt/module/hiveexport ZOOKEEPER_HOME=/opt/module/zookeeper-3.4.10export ZOOCFGDIR=/opt/module/zookeeper-3.4.10export HBASE_HOME=/opt/module/hbase</code></pre><h2 id="3-3-拷贝JDBC驱动"><a href="#3-3-拷贝JDBC驱动" class="headerlink" title="3.3 拷贝JDBC驱动"></a>3.3 拷贝JDBC驱动</h2><p>拷贝jdbc驱动到sqoop的lib目录下，如：</p><pre><code>cp mysql-connector-java-5.1.27-bin.jar /opt/module/sqoop-1.4.6.bin__hadoop-2.0.4-alpha/lib/</code></pre><h2 id="3-4-验证Sqoop"><a href="#3-4-验证Sqoop" class="headerlink" title="3.4 验证Sqoop"></a>3.4 验证Sqoop</h2><p>我们可以通过某一个command来验证sqoop配置是否正确：</p><p>$ bin/sqoop help</p><p>出现一些Warning警告（警告信息已省略），并伴随着帮助命令的输出：</p><pre><code>Available commands:  codegen            Generate code to interact with database records  create-hive-table     Import a table definition into Hive  eval               Evaluate a SQL statement and display the results  export             Export an HDFS directory to a database table  help               List available commands  import             Import a table from a database to HDFS  import-all-tables     Import tables from a database to HDFS  import-mainframe    Import datasets from a mainframe server to HDFS  job                Work with saved jobs  list-databases        List available databases on a server  list-tables           List available tables in a database  merge              Merge results of incremental imports  metastore           Run a standalone Sqoop metastore  version            Display version information</code></pre><h2 id="3-5-测试Sqoop是否能够成功连接数据库"><a href="#3-5-测试Sqoop是否能够成功连接数据库" class="headerlink" title="3.5 测试Sqoop是否能够成功连接数据库"></a>3.5 测试Sqoop是否能够成功连接数据库</h2><pre><code> bin/sqoop list-databases --connect jdbc:mysql://slave2:3306/ --username root --password 000000</code></pre><p>出现如下输出：</p><pre><code>information_schemametastoremysqloozieperformance_schema</code></pre><h1 id="第4章-Sqoop的简单使用案例"><a href="#第4章-Sqoop的简单使用案例" class="headerlink" title="第4章 Sqoop的简单使用案例"></a>第4章 Sqoop的简单使用案例</h1><h2 id="4-1-导入数据"><a href="#4-1-导入数据" class="headerlink" title="4.1 导入数据"></a>4.1 导入数据</h2><p>在Sqoop中，“导入”概念指：从非大数据集群（RDBMS）向大数据集群（HDFS，HIVE，HBASE）中传输数据，叫做：导入，即使用import关键字。</p><h3 id="4-1-1-RDBMS到HDFS"><a href="#4-1-1-RDBMS到HDFS" class="headerlink" title="4.1.1 RDBMS到HDFS"></a>4.1.1 RDBMS到HDFS</h3><p>1) 确定Mysql服务开启正常</p><p>2) 在Mysql中新建一张表并插入一些数据</p><pre><code>mysql -uroot -p000000mysql&gt; create database company;mysql&gt; create table company.staff(id int(4) primary key not null auto_increment, name varchar(255), sex varchar(255));mysql&gt; insert into company.staff(name, sex) values(&#39;Thomas&#39;, &#39;Male&#39;);mysql&gt; insert into company.staff(name, sex) values(&#39;Catalina&#39;, &#39;FeMale&#39;);</code></pre><p>3) 导入数据</p><pre><code> bin/sqoop import \--connect jdbc:mysql://hadoop102:3306/company \--username root \--password 000000 \--table staff \--target-dir /user/company \--delete-target-dir \--num-mappers 1 \--fields-terminated-by &quot;\t&quot;</code></pre><h2 id="4-2、导出数据"><a href="#4-2、导出数据" class="headerlink" title="4.2、导出数据"></a>4.2、导出数据</h2><p>在Sqoop中，“导出”概念指：从大数据集群（HDFS，HIVE，HBASE）向非大数据集群（RDBMS）中传输数据，叫做：导出，即使用export关键字。</p><h3 id="4-2-1-HIVE-HDFS到RDBMS"><a href="#4-2-1-HIVE-HDFS到RDBMS" class="headerlink" title="4.2.1 HIVE/HDFS到RDBMS"></a>4.2.1 HIVE/HDFS到RDBMS</h3><pre><code>bin/sqoop export \--connect jdbc:mysql://hadoop102:3306/company \--username root \--password 000000 \--table staff \--num-mappers 1 \--export-dir /user/hive/warehouse/staff_hive \--input-fields-terminated-by &quot;\t&quot;</code></pre><p>提示：Mysql中如果表不存在，不会自动创建</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flume-大数据采集工具</title>
      <link href="/2020/01/16/flume-da-shu-ju-cai-ji-gong-ju/"/>
      <url>/2020/01/16/flume-da-shu-ju-cai-ji-gong-ju/</url>
      
        <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h2 id="Flume定义"><a href="#Flume定义" class="headerlink" title="Flume定义"></a>Flume定义</h2><p>Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。Flume基于流式架构，灵活简单。</p><h2 id="Flume组成架构"><a href="#Flume组成架构" class="headerlink" title="Flume组成架构"></a>Flume组成架构</h2><p>Flume组成架构如图所示：</p><p><img src="/2020/01/16/flume-da-shu-ju-cai-ji-gong-ju/wps2.jpg" alt="Flume组成架构"> </p><h3 id="Agent"><a href="#Agent" class="headerlink" title="Agent"></a>Agent</h3><p>Agent是一个JVM进程，它以事件的形式将数据从源头送至目的，是Flume数据传输的基本单元。</p><p>Agent主要有3个部分组成，Source、Channel、Sink。</p><h3 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h3><p>Source是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。</p><h3 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h3><p>Channel是位于Source和Sink之间的缓冲区。因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。</p><p>Flume自带两种Channel：Memory Channel和File Channel。</p><p>Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。</p><p>File Channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。</p><h3 id="Sink"><a href="#Sink" class="headerlink" title="Sink"></a>Sink</h3><p>Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。</p><p>Sink是完全事务性的。在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。</p><p>Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。</p><h3 id="Event"><a href="#Event" class="headerlink" title="Event"></a>Event</h3><p>传输单元，Flume数据传输的基本单元，以事件的形式将数据从源头送至目的地。</p><h2 id="Flume拓扑结构"><a href="#Flume拓扑结构" class="headerlink" title="Flume拓扑结构"></a>Flume拓扑结构</h2><p>Flume的拓扑结构如图所示：</p><p><img src="/2020/01/16/flume-da-shu-ju-cai-ji-gong-ju/wps4.jpg" alt="Flume Agent连接"> </p><p><img src="/2020/01/16/flume-da-shu-ju-cai-ji-gong-ju/wps5.jpg" alt="单source，多channel、sink"> </p><p><img src="/2020/01/16/flume-da-shu-ju-cai-ji-gong-ju/wps6.jpg" alt="Flume负载均衡"> </p><p><img src="/2020/01/16/flume-da-shu-ju-cai-ji-gong-ju/wps7.jpg" alt="Flume Agent聚合"> </p><h2 id="Flume-Agent内部原理"><a href="#Flume-Agent内部原理" class="headerlink" title="Flume Agent内部原理"></a>Flume Agent内部原理</h2><p><img src="/2020/01/16/flume-da-shu-ju-cai-ji-gong-ju/wps8.png" alt></p><h1 id="快速入门"><a href="#快速入门" class="headerlink" title="快速入门"></a>快速入门</h1><h2 id="Flume安装地址"><a href="#Flume安装地址" class="headerlink" title="Flume安装地址"></a>Flume安装地址</h2><p>1） Flume官网地址</p><p><a href="http://flume.apache.org/" target="_blank" rel="noopener">http://flume.apache.org/</a></p><p>2）文档查看地址</p><p><a href="http://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="noopener">http://flume.apache.org/FlumeUserGuide.html</a></p><p>3）下载地址</p><p><a href="http://archive.apache.org/dist/flume/" target="_blank" rel="noopener">http://archive.apache.org/dist/flume/</a></p><h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><p><strong>下载安装包解压配置环境变量即可</strong></p><p>1）解压</p><p>2)配置jdk路径</p><pre><code> vi flume-env.sh export JAVA_HOME=jdk路径</code></pre><h2 id="监控端口数据官方案例"><a href="#监控端口数据官方案例" class="headerlink" title="监控端口数据官方案例"></a>监控端口数据官方案例</h2><p>1）案例需求：首先，Flume监控本机44444端口，然后通过telnet工具向本机44444端口发送消息，最后Flume将监听的数据实时显示在控制台。</p><p>2）实现步骤：</p><p>1．安装telnet工具</p><p>将rpm软件包(xinetd-2.3.14-40.el6.x86_64.rpm、telnet-0.17-48.el6.x86_64.rpm和telnet-server-0.17-48.el6.x86_64.rpm)拷入/opt/software文件夹下面。执行RPM软件包安装命令：</p><pre><code>sudo rpm -ivh xinetd-2.3.14-40.el6.x86_64.rpmsudo rpm -ivh telnet-0.17-48.el6.x86_64.rpmsudo rpm -ivh telnet-server-0.17-48.el6.x86_64.rpm</code></pre><p>2．判断44444端口是否被占用</p><pre><code>sudo netstat -tunlp | grep 44444</code></pre><p>功能描述：netstat命令是一个监控TCP/IP网络的非常有用的工具，它可以显示路由表、实际的网络连接以及每一个网络接口设备的状态信息。 </p><p>基本语法：netstat [选项]</p><p>选项参数：</p><p>​    -t或–tcp：显示TCP传输协议的连线状况； </p><p>​      -u或–udp：显示UDP传输协议的连线状况；</p><p>​    -n或–numeric：直接使用ip地址，而不通过域名服务器； </p><p>​    -l或–listening：显示监控中的服务器的Socket； </p><p>​    -p或–programs：显示正在使用Socket的程序识别码和程序名称；</p><p>3．创建Flume Agent配置文件flume-telnet-logger.conf</p><p>在flume目录下创建job文件夹并进入job文件夹。</p><pre><code>mkdir jobcd job/touch flume-telnet-logger.conf# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# Describe the sinka1.sinks.k1.type = logger # Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1</code></pre><ol start="4"><li>先开启flume监听端口</li></ol><pre><code></code></pre><p>参数说明：</p><p>​    –conf conf/  ：表示配置文件存储在conf/目录</p><p>​    –name a1    ：表示给agent起名为a1</p><p>​    –conf-file job/flume-telnet.conf ：flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。</p><p>​    -Dflume.root.logger==INFO,console ：-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。</p><p>5．使用telnet工具向本机的44444端口发送内容</p><p>[atguigu@hadoop102 ~]$ telnet localhost 44444</p><p><img src="/2020/01/16/flume-da-shu-ju-cai-ji-gong-ju/wps11.jpg" alt> </p><p>6．在Flume监听页面观察接收数据情况</p><p><img src="/2020/01/16/flume-da-shu-ju-cai-ji-gong-ju/wps28.jpg" alt> </p><p><strong>具体使用时可以根据自己的输入输出去官网查询对一个配置文件书写方式.</strong></p><h1 id="Flume监控之Ganglia"><a href="#Flume监控之Ganglia" class="headerlink" title="Flume监控之Ganglia"></a>Flume监控之Ganglia</h1><h2 id="操作Flume测试监控"><a href="#操作Flume测试监控" class="headerlink" title="操作Flume测试监控"></a>操作Flume测试监控</h2><p>样式如图：</p><p><img src="/2020/01/16/flume-da-shu-ju-cai-ji-gong-ju/wps27.jpg" alt> </p><p>图例说明：</p><table><thead><tr><th>字段（图表名称）</th><th>字段含义</th></tr></thead><tbody><tr><td>EventPutAttemptCount</td><td>source尝试写入channel的事件总数量</td></tr><tr><td>EventPutSuccessCount</td><td>成功写入channel且提交的事件总数量</td></tr><tr><td>EventTakeAttemptCount</td><td>sink尝试从channel拉取事件的总数量。这不意味着每次事件都被返回，因为sink拉取的时候channel可能没有任何数据。</td></tr><tr><td>EventTakeSuccessCount</td><td>sink成功读取的事件的总数量</td></tr><tr><td>StartTime</td><td>channel启动的时间（毫秒）</td></tr><tr><td>StopTime</td><td>channel停止的时间（毫秒）</td></tr><tr><td>ChannelSize</td><td>目前channel中事件的总数量</td></tr><tr><td>ChannelFillPercentage</td><td>channel占用百分比</td></tr><tr><td>ChannelCapacity</td><td>channel的容量</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>流式处理框架后起之秀Flink</title>
      <link href="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/"/>
      <url>/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink简介"><a href="#Flink简介" class="headerlink" title="Flink简介"></a>Flink简介</h1><h2 id="初识Flink"><a href="#初识Flink" class="headerlink" title="初识Flink"></a>初识Flink</h2><p>Apache Flink是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。Flink被设计在所有常见的集群环境中运行，以内存执行速度和任意规模来执行计算。</p><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/1.jpg" alt> </p><p>Flink起源于Stratosphere项目，Stratosphere是在2010~2014年由3所地处柏林的大学和欧洲的一些其他的大学共同进行的研究项目，2014年4月Stratosphere的代码被复制并捐赠给了Apache软件基金会，参加这个孵化项目的初始成员是Stratosphere系统的核心开发人员，2014年12月，Flink一跃成为Apache软件基金会的顶级项目。</p><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/2.jpg" alt> </p><p>在德语中，Flink一词表示快速和灵巧，项目采用一只松鼠的彩色图案作为logo，这不仅是因为松鼠具有快速和灵巧的特点，还因为柏林的松鼠有一种迷人的红棕色，而Flink的松鼠logo拥有可爱的尾巴，尾巴的颜色与Apache软件基金会的logo颜色相呼应，也就是说，这是一只Apache风格的松鼠。</p><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/3.jpg" alt></p><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/4.jpg" alt="4"></p><p>Flink虽然诞生的早(2010年)，但是其实是起大早赶晚集，直到2015年才开始突然爆发热度。 </p><p>在Flink被apache提升为顶级项目之后，阿里实时计算团队决定在阿里内部建立一个 Flink 分支 Blink，并对 Flink 进行大量的修改和完善，让其适应阿里巴巴这种超大规模的业务场景。</p><p>Blink由2016年上线，服务于阿里集团内部搜索、推荐、广告和蚂蚁等大量核心实时业务。与2019年1月Blink正式开源，目前阿里70%的技术部门都有使用该版本。</p><p>Blink比起Flink的优势就是对SQL语法的更完善的支持以及执行SQL的性能提升。</p><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/wps5.jpg" alt> </p><h2 id="Flink的重要特点"><a href="#Flink的重要特点" class="headerlink" title="Flink的重要特点"></a><strong>Flink</strong>的重要特点</h2><h3 id="事件驱动型-Event-driven"><a href="#事件驱动型-Event-driven" class="headerlink" title="事件驱动型(Event-driven)"></a>事件驱动型(Event-driven)</h3><p>事件驱动型应用是一类具有状态的应用，它从一个或多个事件流提取数据，并根据到来的事件触发计算、状态更新或其他外部动作。比较典型的就是以kafka为代表的消息队列几乎都是事件驱动型应用。</p><p>与之不同的就是SparkStreaming微批次，如图：</p><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/wps6.jpg" alt> </p><p>   事件驱动型：</p><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/wps7.jpg" alt> </p><h3 id="流与批的世界观"><a href="#流与批的世界观" class="headerlink" title="流与批的世界观"></a>流与批的世界观</h3><p>​     <strong>批处理</strong>的特点是有界、持久、大量，非常适合需要访问全套记录才能完成的计算工作，一般用于离线统计。</p><p><strong>流处理</strong>的特点是无界、实时,  无需针对整个数据集执行操作，而是对通过系统传输的每个数据项执行操作，一般用于实时统计。</p><p>在<strong>spark</strong>的世界观中，一切都是由批次组成的，离线数据是一个大批次，而实时数据是由一个一个无限的小批次组成的。</p><p> 而在<strong>flink</strong>的世界观中，一切都是由流组成的，离线数据是有界限的流，实时数据是一个没有界限的流，这就是所谓的有界流和无界流。</p><p><strong>无界数据流</strong>：<strong>无界数据流有一个开始但是没有结束</strong>，它们不会在生成时终止并提供数据，必须连续处理无界流，也就是说必须在获取后立即处理event。对于无界数据流我们无法等待所有数据都到达，因为输入是无界的，并且在任何时间点都不会完成。处理无界数据通常要求以特定顺序（例如事件发生的顺序）获取event，以便能够推断结果完整性。</p><p><strong>有界数据流</strong>：<strong>有界数据流有明确定义的开始和结束</strong>，可以在执行任何计算之前通过获取所有数据来处理有界流，处理有界流不需要有序获取，因为可以始终对有界数据集进行排序，有界流的处理也称为批处理。</p><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/wps8.png" alt></p><p>​    这种以流为世界观的架构，获得的最大好处就是具有极低的延迟。</p><h3 id="分层api"><a href="#分层api" class="headerlink" title="分层api"></a>分层api</h3><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/wps9.jpg" alt> </p><p>最底层级的抽象仅仅提供了有状态流，它将通过过程函数（Process Function）被嵌入到DataStream API中。底层过程函数（Process Function） 与 DataStream API 相集成，使其可以对某些特定的操作进行底层的抽象，它允许用户可以自由地处理来自一个或多个数据流的事件，并使用一致的容错的状态。除此之外，用户可以注册事件时间并处理时间回调，从而使程序可以处理复杂的计算。</p><p>实际上，<strong>大多数应用并不需要上述的底层抽象，而是针对核心API（Core APIs） 进行编程，比如DataStream API（有界或无界流数据）以及DataSet API（有界数据集）</strong>。这些API为数据处理提供了通用的构建模块，比如由用户定义的多种形式的转换（transformations），连接（joins），聚合（aggregations），窗口操作（windows）等等。DataSet API 为有界数据集提供了额外的支持，例如循环与迭代。这些API处理的数据类型以类（classes）的形式由各自的编程语言所表示。</p><p>Table API 是以表为中心的声明式编程，其中表可能会动态变化（在表达流数据时）。Table API遵循（扩展的）关系模型：表有二维数据结构（schema）（类似于关系数据库中的表），同时API提供可比较的操作，例如select、project、join、group-by、aggregate等。Table API程序声明式地定义了什么逻辑操作应该执行，而不是准确地确定这些操作代码的看上去如何 。 尽管Table API可以通过多种类型的用户自定义函数（UDF）进行扩展，其仍不如核心API更具表达能力，但是使用起来却更加简洁（代码量更少）。除此之外，Table API程序在执行之前会经过内置优化器进行优化。</p><p><strong>你可以在表与 DataStream/DataSet 之间无缝切换，以允许程序将 Table API 与 DataStream 以及 DataSet 混合使用</strong>。</p><p>Flink提供的最高层级的抽象是 SQL 。这一层抽象在语法与表达能力上与 Table API 类似，但是是以SQL查询表达式的形式表现程序。SQL抽象与Table API交互密切，同时SQL查询可以直接在Table API定义的表上执行。</p><h3 id="支持有状态计算"><a href="#支持有状态计算" class="headerlink" title="支持有状态计算"></a>支持有状态计算</h3><p>Flink在1.4版本中实现了状态管理，所谓状态管理就是在流失计算过程中将算子的中间结果保存在内存或者文件系统中，等下一个事件进入算子后可以让当前事件的值与历史值进行汇总累计。</p><h3 id="支持exactly-once语义"><a href="#支持exactly-once语义" class="headerlink" title="支持exactly-once语义"></a>支持exactly-once语义</h3><p>在分布式系统中，组成系统的各个计算机是独立的。这些计算机有可能fail。</p><p>一个sender发送一条message到receiver。根据receiver出现fail时sender如何处理fail，可以将message delivery分为三种语义:</p><p><strong>At Most once:</strong> 对于一条message,receiver最多收到一次(0次或1次).</p><p>可以达成At Most Once的策略:</p><p>sender把message发送给receiver.无论receiver是否收到message,sender都不再重发message.</p><p><strong>At Least once:</strong> 对于一条message,receiver最少收到一次(1次及以上).</p><p>可以达成At Least Once的策略:</p><p>sender把message发送给receiver.当receiver在规定时间内没有回复ACK或回复了error信息,那么sender重发这条message给receiver,直到sender收到receiver的ACK.</p><p><strong>Exactly once:</strong> 对于一条message,receiver确保只收到一次</p><h3 id="支持事件时间（EventTime"><a href="#支持事件时间（EventTime" class="headerlink" title="支持事件时间（EventTime)"></a>支持事件时间（EventTime)</h3><p>目前大多数框架时间窗口计算，都是采用当前系统时间，以时间为单位进行的聚合计算只能反应数据到达计算引擎的时间，而并不是实际业务时间</p><h1 id="Flink实战入门"><a href="#Flink实战入门" class="headerlink" title="Flink实战入门"></a>Flink实战入门</h1><h2 id="批处理wordcount编程实战"><a href="#批处理wordcount编程实战" class="headerlink" title="批处理wordcount编程实战"></a><strong>批处理wordcount编程实战</strong></h2><p><strong>添加依赖</strong></p><pre><code>        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt;            &lt;version&gt;1.7.0&lt;/version&gt;        &lt;/dependency&gt;</code></pre><p><strong>编写程序</strong></p><pre><code>def main(args: Array[String]): Unit = {  //构造执行环境  val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment  //读取文件  val input = &quot;file:///d:/temp/hello.txt&quot;  val ds: DataSet[String] = env.readTextFile(input)  // 其中flatMap 和Map 中  需要引入隐式转换  import org.apache.flink.api.scala.createTypeInformation  //经过groupby进行分组，sum进行聚合  val aggDs: AggregateDataSet[(String, Int)] = ds.flatMap(_.split(&quot; &quot;)).map((_, 1)).groupBy(0).sum(1)  // 打印  aggDs.print()}</code></pre><h2 id="流处理wordcount编程"><a href="#流处理wordcount编程" class="headerlink" title="流处理wordcount编程"></a>流处理wordcount编程</h2><p><strong>导入依赖</strong></p><pre><code>&lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-scala_2.11&lt;/artifactId&gt;            &lt;version&gt;1.7.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-scala --&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;            &lt;artifactId&gt;flink-streaming-scala_2.11&lt;/artifactId&gt;            &lt;version&gt;1.7.0&lt;/version&gt;        &lt;/dependency&gt;</code></pre><p><strong>编写程序</strong></p><pre><code>import org.apache.flink.api.java.utils.ParameterToolimport org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}object StreamWcApp {  def main(args: Array[String]): Unit = {    //从外部命令中获取参数    val tool: ParameterTool = ParameterTool.fromArgs(args)    val host: String = tool.get(&quot;host&quot;)    val port: Int = tool.get(&quot;port&quot;).toInt    //创建流处理环境    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment    //接收socket文本流    val textDstream: DataStream[String] = env.socketTextStream(host,port)   // flatMap和Map需要引用的隐式转换    import org.apache.flink.api.scala._   //处理 分组并且sum聚合    val dStream: DataStream[(String, Int)] = textDstream.flatMap(_.split(&quot; &quot;)).filter(_.nonEmpty).map((_,1)).keyBy(0).sum(1)   //打印    dStream.print()    env.execute()  }</code></pre><h1 id="Flink部署"><a href="#Flink部署" class="headerlink" title="Flink部署"></a>Flink部署</h1><h2 id="standalone模式"><a href="#standalone模式" class="headerlink" title="standalone模式"></a>standalone模式</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a><strong>安装</strong></h3><p>解压缩  flink-1.7.0-bin-hadoop27-scala_2.11.tgz</p><p><strong>修改 flink/conf/flink-conf.yaml 文件</strong></p><pre><code>jobmanager.rpc.address:master</code></pre><p><strong>修改/conf/slave文件,添加上集群主机</strong></p><pre><code>slave1slave2slave3</code></pre><p><strong>分发给 另外两台机子(<a href>分发脚本可通过如下地址获取</a>)</strong></p><pre><code>xsync flink-1.7.0</code></pre><p><strong>启动</strong></p><pre><code>start-cluster.sh</code></pre><h3 id="提交任务"><a href="#提交任务" class="headerlink" title="提交任务"></a>提交任务</h3><p><strong>准备数据文件</strong> </p><p><strong>把含数据文件的文件夹，分发到taskmanage 机器中</strong> </p><p><strong>注:</strong>由于读取数据是从本地磁盘读取，实际任务会被分发到taskmanage的机器中，所以要把目标文件分发。</p><p><strong>执行程序</strong> </p><pre><code>..flink run -c 全类名 /jar包位置 --input 输入文件位置 --output 输出文件位置</code></pre><p> <strong>到目标文件夹中查看计算结果</strong></p><p><strong>注：</strong>计算结果根据会保存到taskmanage的机器下，不会再jobmanage下。</p><p>在<strong>web控制台</strong>查看计算过程</p><pre><code>在浏览器打开http://master:8081 </code></pre><h2 id="yarn模式"><a href="#yarn模式" class="headerlink" title="yarn模式"></a>yarn模式</h2><p><strong>启动hadoop集群</strong></p><p><strong>启动yarn-session</strong></p><pre><code>./yarn-sessin.sh -n 2 -s 6 -jm 1024 -tm test -d</code></pre><p>其中：</p><ul><li><p>-n(–container)：TaskManager的数量。<br>   -s(–slots)：    每个TaskManager的slot数量，默认一个slot一个core，默认每个taskmanager的slot的个数为1，有时可以多一些taskmanager，做冗余。</p></li><li><p>-jm：JobManager的内存（单位MB)。</p></li><li><p>-tm：每个taskmanager的内存（单位MB)。</p></li><li><p>-nm：yarn 的appName(现在yarn的ui上的名字)。 </p></li><li><p>-d：后台执行。</p><p><strong>执行任务</strong></p></li></ul><pre><code>./flink run -m yarn-cluster -c 主类名 jar包位置 --input 输入文件位置 --output 输出文件位置 </code></pre><p><strong>去yarn控制台查看任务状态</strong></p><pre><code>去浏览器打开http://slave1:8088</code></pre><h1 id="Flink架构介绍"><a href="#Flink架构介绍" class="headerlink" title="Flink架构介绍"></a>Flink架构介绍</h1><h2 id="基本组件栈"><a href="#基本组件栈" class="headerlink" title="基本组件栈"></a>基本组件栈</h2><p>了解Spark的朋友会发现Flink的架构和Spark是非常类似的，在整个软件架构体系中，同样遵循着分层的架构设计理念，在降低系统耦合度的同时，也为上层用户构建Flink应用提供了丰富且友好的接口。</p><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/6.png" alt="file"></p><p>Flink分为架构分为三层，由上往下依次是API&amp;Libraries层、Runtime核心层以及物理部署层</p><p>​        <strong>API&amp;Libraries层</strong></p><p>作为分布式数据处理框架，Flink同时提供了支撑计算和批计算的接口，同时在此基础上抽象出不同的应用类型的组件库，如基于流处理的CEP(复杂事件处理库)、SQL&amp;Table库和基于批处理的FlinkML(机器学习库)等、Gelly(图处理库)等。API层包括构建流计算应用的DataStream API和批计算应用的DataSet API，两者都提供给用户丰富的数据处理高级API，例如Map、FlatMap操作等，同时也提供比较低级的Process Function API，用户可以直接操作状态和时间等底层数据。</p><p>　　<strong>Runtime核心层</strong></p><p>　　该层主要负责对上层不同接口提供基础服务，也是Flink分布式计算框架的核心实现层，支持分布式Stream作业的执行、JobGraph到ExecutionGraph的映射转换、任务调度等。将DataSteam和DataSet转成统一的可执行的Task Operator，达到在流式引擎下同时处理批量计算和流式计算的目的。</p><p>　　<strong>物理部署层</strong></p><p>　　该层主要涉及Flink的部署模式，目前Flink支持多种部署模式：本地、集群(Standalone、YARN)、云(GCE/EC2)、Kubenetes。Flink能够通过该层能够支持不同平台的部署，用户可以根据需要选择使用对应的部署模式。</p><h2 id="运行架构"><a href="#运行架构" class="headerlink" title="运行架构"></a>运行架构</h2><h2 id="任务提交流程（yarn模式）"><a href="#任务提交流程（yarn模式）" class="headerlink" title="任务提交流程（yarn模式）"></a>任务提交流程（yarn模式）</h2><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/wps23.jpg" alt="Yarn模式任务提交流程"> </p><p>Flink任务提交后，Client向HDFS上传Flink的Jar包和配置，之后向Yarn ResourceManager提交任务，ResourceManager分配Container资源并通知对应的NodeManager启动ApplicationMaster，ApplicationMaster启动后加载Flink的Jar包和配置构建环境，然后启动JobManager，之后ApplicationMaster向ResourceManager申请资源启动TaskManager，ResourceManager分配Container资源后，由ApplicationMaster通知资源所在节点的NodeManager启动TaskManager，NodeManager加载Flink的Jar包和配置构建环境并启动TaskManager，TaskManager启动后向JobManager发送心跳包，并等待JobManager向其分配任务。</p><h2 id="2-任务调度原理"><a href="#2-任务调度原理" class="headerlink" title="2 任务调度原理"></a><strong>2</strong> <strong>任务调度原理</strong></h2><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/wps24.jpg" alt="任务调度原理"></p><p>客户端不是运行时和程序执行的一部分，但它用于准备并发送dataflow(JobGraph)给Master(JobManager)，然后，客户端断开连接或者维持连接以等待接收计算结果。</p><p>当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。</p><p><strong>Client</strong> 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境连通即可）。提交 Job 后，Client 可以结束进程（Streaming的任务），也可以不结束并等待结果返回。</p><p><strong>JobManager</strong> 主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。</p><p><strong>TaskManager</strong> 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理。</p><p><strong>关于执行图</strong></p><p>Flink 中的执行图可以分成四层：<strong>StreamGraph</strong> -&gt; <strong>JobGraph</strong> -&gt; <strong>ExecutionGraph</strong> -&gt; <strong>物理执行图</strong>。</p><p><strong>StreamGraph</strong>：是根据用户通过 Stream API 编写的代码生成的最初的图。用来表示程序的拓扑结构。</p><p><strong>JobGraph</strong>：StreamGraph经过优化后生成了 JobGraph，提交给 JobManager 的数据结构。主要的优化为，将多个符合条件的节点 chain 在一起作为一个节点，这样可以减少数据在节点之间流动所需要的序列化/反序列化/传输消耗。</p><p><strong>ExecutionGraph</strong>：JobManager 根据 JobGraph 生成ExecutionGraph。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。</p><p><strong>物理执行图</strong>：JobManager 根据 ExecutionGraph 对 Job 进行调度后，在各个TaskManager 上部署 Task 后形成的“图”，并不是一个具体的数据结构。</p><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/wps25.jpg" alt></p><h2 id="3-Worker与Slots"><a href="#3-Worker与Slots" class="headerlink" title="3 Worker与Slots"></a><strong>3</strong> <strong>Worker与Slots</strong></h2><p><strong>每一个worker(TaskManager)是一个JVM进程，它可能会在独立的线程上执行一个或多个subtask</strong>。为了控制一个worker能接收多少个task，worker通过task slot来进行控制（一个worker至少有一个task slot）。·</p><p>每个task slot表示TaskManager拥有资源的一个固定大小的子集。假如一个TaskManager有三个slot，那么它会将其管理的内存分成三份给各个slot。<strong>资源slot化意味着一个subtask将不需要跟来自其他job的subtask竞争被管理的内存，取而代之的是它将拥有一定数量的内存储备</strong>。需要注意的是，这里不会涉及到CPU的隔离，slot目前仅仅用来隔离task的受管理的内存。</p><p><strong>通过调整task slot的数量，允许用户定义subtask之间如何互相隔离</strong>。如果一个TaskManager一个slot，那将意味着每个task group运行在独立的JVM中（该JVM可能是通过一个特定的容器启动的），而一个TaskManager多个slot意味着更多的subtask可以共享同一个JVM。而在同一个JVM进程中的task将共享TCP连接（基于多路复用）和心跳消息。它们也可能共享数据集和数据结构，因此这减少了每个task的负载。</p><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/wps26.jpg" alt="TaskManager与Slot"></p><p><strong>Task **Slot是静态的概念，是指TaskManager具有的并发执行能力</strong>，可以通过参数taskmanager.numberOfTaskSlots进行配置，而<strong>并行度parallelism是动态概念，即TaskManager运行程序时实际使用的并发能力</strong>，可以通过参数parallelism.default进行配置。</p><p>也就是说，假设一共有3个TaskManager，每一个TaskManager中的分配3个TaskSlot，也就是每个TaskManager可以接收3个task，一共9个TaskSlot，如果我们设置parallelism.default=1，即运行程序默认的并行度为1，9个TaskSlot只用了1个，有8个空闲，因此，设置合适的并行度才能提高效率。</p><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/wps27.jpg" alt> </p><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/wps28.jpg" alt> </p><h2 id="4-并行数据流"><a href="#4-并行数据流" class="headerlink" title="4 并行数据流"></a><strong>4</strong> <strong>并行数据流</strong></h2><p><strong>Flink程序的执行具有并行、分布式的特性</strong>。在执行过程中，一个 stream 包含一个或多个 stream partition ，而每一个 operator 包含一个或多个 operator subtask，这些operator subtasks在不同的线程、不同的物理机或不同的容器中彼此互不依赖得执行。</p><p><strong>一个特定operator的subtask的个数被称之为其parallelism(并行度)</strong>。一个stream的并行度总是等同于其producing operator的并行度。一个程序中，不同的operator可能具有不同的并行度。</p><p><img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/wps29.jpg" alt="并行数据流"></p><p>Stream在operator之间传输数据的形式可以是one-to-one(forwarding)的模式也可以是redistributing的模式，具体是哪一种形式，取决于operator的种类。</p><p><strong>One-to-one</strong>：<strong>stream(比如在source和map operator之间)维护着分区以及元素的顺序</strong>。那意味着map operator的subtask看到的元素的个数以及顺序跟source operator的subtask生产的元素的个数、顺序相同，map、fliter、flatMap等算子都是one-to-one的对应关系。</p><p>Ø <strong>类似于s**</strong>park<strong>**中的窄依赖</strong></p><p><strong>Redistributing</strong>：<strong>stream(map()跟keyBy/window之间或者keyBy/window跟sink之间)的分区会发生改变</strong>。每一个operator subtask依据所选择的transformation发送数据到不同的目标subtask。例如，keyBy() 基于hashCode重分区、broadcast和rebalance会随机重新分区，这些算子都会引起redistribute过程，而redistribute过程就类似于Spark中的shuffle过程。</p><p>Ø <strong>类似于s**</strong>park<strong>**中的宽依赖</strong></p><h2 id="5-task与operator-chains"><a href="#5-task与operator-chains" class="headerlink" title="5 task与operator chains"></a><strong>5</strong> <strong>task与operator</strong> <strong>chains</strong></h2><p>相同并行度的one to one操作，Flink这样相连的operator 链接在一起形成一个task，原来的operator成为里面的subtask。将operators链接成task是非常有效的优化：<strong>它能减少线程之间的切换和基于缓存区的数据交换，在减少时延的同时提升吞吐量</strong>。链接的行为可以在编程API中进行指定。</p><p> <img src="/2020/01/16/liu-shi-chu-li-kuang-jia-hou-qi-zhi-xiu-flink/wps30.jpg" alt=" task与operatorchains"></p><p><strong>OperatorChain的优点</strong></p><ul><li>减少线程切换</li><li>减少序列化与反序列化</li><li>减少延迟并且提高吞吐能力</li></ul><p><strong>OperatorChain 组成条件</strong></p><ul><li>上下游算子并行度一致</li><li>上下游算子之间没有数据shuffle</li></ul><h1 id="流计算框架Flink与Storm-的性能对比"><a href="#流计算框架Flink与Storm-的性能对比" class="headerlink" title="流计算框架Flink与Storm 的性能对比"></a>流计算框架Flink与Storm 的性能对比</h1><table><thead><tr><th></th><th>Storm</th><th>Flink</th></tr></thead><tbody><tr><td>状态管理</td><td>无状态，需用户自行进行状态管理</td><td>有状态</td></tr><tr><td>窗口支持</td><td>对事件窗口支持较弱，缓存整个窗口的所有 数据，窗口结束时一起计算</td><td>窗口支持较为完善，自带一些窗口聚合方法，并 且会自动管理窗口状态。</td></tr><tr><td>消息投递</td><td>At Most Once At Least Once</td><td>At Most Once At Least Once Exactly Once</td></tr><tr><td>容错方式</td><td>ACK机制：对每个消息进行全链路跟踪，失败 或超时进行重发。</td><td>检查点机制：通过分布式一致性快照机制，对数 据流和算子状态进行保存。在发生错误时，使系 统能够进行回滚。</td></tr><tr><td>应用现状</td><td>在美团点评实时计算业务中已有较为成熟的 运用，有管理平台、常用 API 和相应的文档， 大量实时作业基于 Storm 构建。</td><td>在美团点评实时计算业务中已有一定应用，但 是管理平台、API 及文档等仍需进一步完善。</td></tr></tbody></table><h2 id="框架本身性能"><a href="#框架本身性能" class="headerlink" title="框架本身性能"></a>框架本身性能</h2><ul><li>Storm 单线程吞吐约为 <strong>8.7 万条/秒</strong>，Flink 单线程吞吐 可达 <strong>35 万条/秒</strong>。Flink 吞吐约为 Storm 的 3-5 倍。</li><li>Storm QPS 接近吞吐时延迟（含 Kafka 读写时间）中位 数约 100 毫秒，99 线约 700 毫秒，Flink 中位数约 50 毫秒，99 线约 300 毫秒。Flink 在 满吞吐时的延迟约为 Storm 的一半，且随着 QPS 逐渐增大，Flink 在延迟上的优势开始体现出来。</li><li>综上可得，<strong>Flink 框架本身性能优于 Storm</strong>。</li></ul><h2 id="复杂用户逻辑对框架差异的削弱"><a href="#复杂用户逻辑对框架差异的削弱" class="headerlink" title="复杂用户逻辑对框架差异的削弱"></a>复杂用户逻辑对框架差异的削弱</h2><ul><li>单个 Bolt Sleep 时长达到 1 毫秒时， Flink 的延迟仍低于 Storm，但吞吐优势已基本无法体现。</li><li>因此，用户逻辑越复杂，本身耗时越长，针对该逻辑的测试体现出来的框架的差异越小。</li></ul><h2 id="不同消息投递语义的差异"><a href="#不同消息投递语义的差异" class="headerlink" title="不同消息投递语义的差异"></a>不同消息投递语义的差异</h2><ul><li>Flink Exactly Once 的吞吐较 At Least Once 而 言下降 6.3%，延迟差异不大；Storm At Most Once 语义下的吞吐较 At Least Once 提升 16.8%，延迟稍有下降。</li><li>由于 Storm 会对每条消息进行 ACK，Flink 是基于一批消息做的检查点，不同的实现原理导 致两者在 At Least Once 语义的花费差异较大，从而影响了性能。而 Flink 实现 Exactly Once 语义仅增加了对齐操作，因此在算子并发量不大、没有出现慢节点的情况下对 Flink 性能的 影响不大。Storm At Most Once 语义下的性能仍然低于 Flink。</li></ul><h2 id="Flink-状态存储后端选择"><a href="#Flink-状态存储后端选择" class="headerlink" title="Flink 状态存储后端选择"></a>Flink 状态存储后端选择</h2><p>• Flink 提供了内存、文件系统、RocksDB 三种 StateBackends，三者的对比如下：</p><p>StateBackend 过程状态存储 检查点存储 吞吐 推荐使用场景 Memory TM Memory JM Memory 高（3-5 倍 Storm） 调试、无状态或对数据是否 丢失重复无要求 FileSystem TM Memory FS/HDFS 高（3-5 倍 Storm） 普通状态、窗口、KV 结构 （建议作为默认 Backend）</p><pre><code>    RocksDB RocksDB on TM FS/HDFS 低（0.3-0.5 倍 Storm） 超大状态、超长窗口、大型 KV 结构 </code></pre><h2 id="推荐使用-Flink-的场景"><a href="#推荐使用-Flink-的场景" class="headerlink" title="推荐使用 Flink 的场景"></a>推荐使用 Flink 的场景</h2><p>综合上述测试结果，以下实时计算场景建议考虑使用 Flink 框架进行计算：</p><ul><li>要求消息投递语义为Exactly Once的场景；</li><li>数据量较大，要求高吞吐低延迟的场景；</li><li>需要进行状态管理或窗口统计的场景。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>真正意义的流式处理框架Strom</title>
      <link href="/2020/01/16/zhen-zheng-yi-yi-de-liu-shi-chu-li-kuang-jia-strom/"/>
      <url>/2020/01/16/zhen-zheng-yi-yi-de-liu-shi-chu-li-kuang-jia-strom/</url>
      
        <content type="html"><![CDATA[<h1 id="Storm基础入门"><a href="#Storm基础入门" class="headerlink" title="Storm基础入门"></a>Storm基础入门</h1><ul><li>Apache Storm是一个分布式实时大数据处理系统。Storm设计用于在容错和水平可扩展方法中处理大量数据。它是一个流数据框架，具有最高的摄取率。</li></ul><h2 id="Storm的核心组件"><a href="#Storm的核心组件" class="headerlink" title="Storm的核心组件"></a>Storm的核心组件</h2><ul><li>Nimbus：即Storm的Master，负责资源分配和任务调度。一个Storm集群只有一个Nimbus。</li><li>Supervisor：即Storm的Slave，负责接收Nimbus分配的任务，管理所有Worker，一个Supervisor节点中包含多个Worker进程。</li><li>Worker：工作进程，每个工作进程中都有多个Task。 Task：任务，在 Storm 集群中每个 Spout 和 Bolt 都由若干个任务（tasks）来执行。每个任务都与一个执行线程相对应。</li><li>Topology：计算拓扑，Storm 的拓扑是对实时计算应用逻辑的封装，它的作用与 MapReduce 的任务（Job）很相似，区别在于 MapReduce 的一个 Job 在得到结果之后总会结束，而拓扑会一直在集群中运行，直到你手动去终止它。拓扑还可以理解成由一系列通过数据流（Stream Grouping）相互关联的 Spout 和 Bolt 组成的的拓扑结构。</li><li>Stream：数据流（Streams）是 Storm 中最核心的抽象概念。一个数据流指的是在分布式环境中并行创建、处理的一组元组（tuple）的无界序列。数据流可以由一种能够表述数据流中元组的域（fields）的模式来定义。</li><li>Spout：数据源（Spout）是拓扑中数据流的来源。一般 Spout 会从一个外部的数据源读取元组然后将他们发送到拓扑中。根据需求的不同，Spout 既可以定义为可靠的数据源，也可以定义为不可靠的数据源。一个可靠的 Spout能够在它发送的元组处理失败时重新发送该元组，以确保所有的元组都能得到正确的处理；相对应的，不可靠的 Spout 就不会在元组发送之后对元组进行任何其他的处理。一个 Spout可以发送多个数据流。</li><li>Bolt：拓扑中所有的数据处理均是由 Bolt 完成的。通过数据过滤（filtering）、函数处理（functions）、聚合（aggregations）、联结（joins）、数据库交互等功能，Bolt 几乎能够完成任何一种数据处理需求。一个 Bolt 可以实现简单的数据流转换，而更复杂的数据流变换通常需要使用多个 Bolt 并通过多个步骤完成。</li><li>Stream grouping：为拓扑中的每个 Bolt 的确定输入数据流是定义一个拓扑的重要环节。数据流分组定义了在 Bolt 的不同任务（tasks）中划分数据流的方式。在 Storm 中有八种内置的数据流分组方式。</li><li>Reliability：可靠性。Storm 可以通过拓扑来确保每个发送的元组都能得到正确处理。通过跟踪由 Spout 发出的每个元组构成的元组树可以确定元组是否已经完成处理。每个拓扑都有一个“消息延时”参数，如果 Storm 在延时时间内没有检测到元组是否处理完成，就会将该元组标记为处理失败，并会在稍后重新发送该元组。</li></ul><h2 id="storm-Linux安装"><a href="#storm-Linux安装" class="headerlink" title="storm Linux安装"></a>storm Linux安装</h2><p>首先需要3台虚拟机,并且安装好JDK1.8,python2.6.6以上版本,还要在3台虚拟机上安装好zookeeper. 下载地址:</p><ul><li>Zookeeper：<a href="https://zookeeper.apache.org/releases.html#download" target="_blank" rel="noopener">zookeeper.apache.org/releases.ht…</a></li><li>Storm: <a href="http://storm.apache.org/downloads.html" target="_blank" rel="noopener">storm.apache.org/downloads.h…</a></li><li>将下载下来的storm保存到/usr/local/目录,将apache-storm-1.2.2.tar.gz文件进行解压 在linux上输入:</li></ul><pre class="line-numbers language-复制代码"><code class="language-复制代码">tar -zxvf apache-storm-1.2.2.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>得到apache-storm-1.2.2。配置环境变量编辑 /etc/profile 文件</li></ul><pre><code>export STORM_HOME=/opt/storm/storm1.1export PATH=.:${JAVA_HOME}/bin:${ZK_HOME}/bin:${STORM_HOME}/bin:$PATH</code></pre><p>编辑 /usr/local/apache-storm-1.2.2/conf 的 storm.yarm</p><pre><code> storm.zookeeper.servers:     - &quot;192.168.1.21&quot;     - &quot;192.168.1.23&quot;     - &quot;192.168.1.24&quot; storm.local.dir: &quot;/usr/local/apache-storm-1.2.2/data&quot; nimbus.host: &quot;192.168.70.68&quot; ui.port: 10088 supervisor.slots.ports:     - 6700     - 6701     - 6702     - 6703</code></pre><p>注意的是每个参数前面必须有空格，-的后面也需要</p><ul><li>storm.zookeeper.servers是指定zookeeper的服务地址。</li><li>storm.local.dir 表示存储目录。</li><li>nimbus.host 表示主机节点。</li><li>ui.port 主机端口</li><li>supervisor.slots.ports 表示worker 端口。</li></ul><h2 id="启动Storm"><a href="#启动Storm" class="headerlink" title="启动Storm"></a>启动Storm</h2><p>进入到/usr/local/apache-storm-1.2.2/bin 目录下 首先启动主节点</p><pre><code>nohup ./strom nimbus &amp; </code></pre><p>启动图形界面</p><pre><code>nohup ./storm ui &amp;</code></pre><p>启动其他节点</p><pre><code>nohup ./storm supervisor &amp;</code></pre><p>在浏览器访问主节点<a href="https://juejin.im/post/5c9aeb95f265da610849b2b8" target="_blank" rel="noopener">http://192.168.70.68:10088</a> 看到界面表示成功。</p><h1 id="Storm-核心概念详解"><a href="#Storm-核心概念详解" class="headerlink" title="Storm 核心概念详解"></a>Storm 核心概念详解</h1><h2 id="一、Storm核心概念"><a href="#一、Storm核心概念" class="headerlink" title="一、Storm核心概念"></a>一、Storm核心概念</h2><p><img src="/2020/01/16/zhen-zheng-yi-yi-de-liu-shi-chu-li-kuang-jia-strom/4.png" alt></p><h3 id="1-1-Topologies（拓扑）"><a href="#1-1-Topologies（拓扑）" class="headerlink" title="1.1 Topologies（拓扑）"></a>1.1 Topologies（拓扑）</h3><p>一个完整的 Storm 流处理程序被称为 Storm topology(拓扑)。它是一个是由 <code>Spouts</code> 和 <code>Bolts</code>通过 <code>Stream</code> 连接起来的有向无环图，Storm 会保持每个提交到集群的 topology 持续地运行，从而处理源源不断的数据流，直到你将主动其杀死 (kill) 为止。</p><h3 id="1-2-Streams（流）"><a href="#1-2-Streams（流）" class="headerlink" title="1.2 Streams（流）"></a>1.2 Streams（流）</h3><p><code>Stream</code> 是 Storm 中的核心概念。一个 <code>Stream</code> 是一个无界的、以分布式方式并行创建和处理的 <code>Tuple</code> 序列。Tuple 可以包含大多数基本类型以及自定义类型的数据。简单来说，Tuple 就是流数据的实际载体，而 Stream 就是一系列 Tuple。</p><h3 id="1-3-Spouts"><a href="#1-3-Spouts" class="headerlink" title="1.3 Spouts"></a>1.3 Spouts</h3><p><code>Spouts</code> 是流数据的源头，一个 Spout 可以向不止一个 <code>Streams</code> 中发送数据。<code>Spout</code> 通常分为<strong>可靠</strong>和<strong>不可靠</strong>两种：可靠的 <code>Spout</code> 能够在失败时重新发送 Tuple, 不可靠的 <code>Spout</code> 一旦把 Tuple 发送出去就置之不理了。</p><h3 id="1-4-Bolts"><a href="#1-4-Bolts" class="headerlink" title="1.4 Bolts"></a>1.4 Bolts</h3><p><code>Bolts</code> 是流数据的处理单元，它可以从一个或者多个 <code>Streams</code> 中接收数据，处理完成后再发射到新的 <code>Streams</code> 中。<code>Bolts</code> 可以执行过滤 (filtering)，聚合 (aggregations)，连接 (joins) 等操作，并能与文件系统或数据库进行交互。</p><h3 id="1-5-Stream-groupings（分组策略）"><a href="#1-5-Stream-groupings（分组策略）" class="headerlink" title="1.5 Stream groupings（分组策略）"></a>1.5 Stream groupings（分组策略）</h3><p><img src="/2020/01/16/zhen-zheng-yi-yi-de-liu-shi-chu-li-kuang-jia-strom/1.png" alt></p><p><code>spouts</code> 和 <code>bolts</code> 在集群上执行任务时，是由多个 Task 并行执行 (如上图，每一个圆圈代表一个 Task)。当一个 Tuple 需要从 Bolt A 发送给 Bolt B 执行的时候，程序如何知道应该发送给 Bolt B 的哪一个 Task 执行呢？</p><p>这是由 Stream groupings 分组策略来决定的，Storm 中一共有如下 8 个内置的 Stream Grouping。当然你也可以通过实现 <code>CustomStreamGrouping</code> 接口来实现自定义 Stream 分组策略。</p><ol><li><p><strong>Shuffle grouping</strong></p><p>Tuples 随机的分发到每个 Bolt 的每个 Task 上，每个 Bolt 获取到等量的 Tuples。</p></li><li><p><strong>Fields grouping</strong></p><p>Streams 通过 grouping 指定的字段 (field) 来分组。假设通过 <code>user-id</code> 字段进行分区，那么具有相同 <code>user-id</code> 的 Tuples 就会发送到同一个 Task。</p></li><li><p><strong>Partial Key grouping</strong></p><p>Streams 通过 grouping 中指定的字段 (field) 来分组，与 <code>Fields Grouping</code> 相似。但是对于两个下游的 Bolt 来说是负载均衡的，可以在输入数据不平均的情况下提供更好的优化。</p></li><li><p><strong>All grouping</strong></p><p>Streams 会被所有的 Bolt 的 Tasks 进行复制。由于存在数据重复处理，所以需要谨慎使用。</p></li><li><p><strong>Global grouping</strong></p><p>整个 Streams 会进入 Bolt 的其中一个 Task，通常会进入 id 最小的 Task。</p></li><li><p><strong>None grouping</strong></p><p>当前 None grouping 和 Shuffle grouping 等价，都是进行随机分发。</p></li><li><p><strong>Direct grouping</strong></p><p>Direct grouping 只能被用于 direct streams 。使用这种方式需要由 Tuple 的生产者直接指定由哪个 Task 进行处理。</p></li><li><p><strong>Local or shuffle grouping</strong></p><p>如果目标 Bolt 有 Tasks 和当前 Bolt 的 Tasks 处在同一个 Worker 进程中，那么则优先将 Tuple Shuffled 到处于同一个进程的目标 Bolt 的 Tasks 上，这样可以最大限度地减少网络传输。否则，就和普通的 <code>Shuffle Grouping</code> 行为一致。</p></li></ol><h2 id="二、Storm架构详解"><a href="#二、Storm架构详解" class="headerlink" title="二、Storm架构详解"></a>二、Storm架构详解</h2><p><img src="/2020/01/16/zhen-zheng-yi-yi-de-liu-shi-chu-li-kuang-jia-strom/2.png" alt></p><h3 id="2-1-Nimbus进程"><a href="#2-1-Nimbus进程" class="headerlink" title="2.1 Nimbus进程"></a>2.1 Nimbus进程</h3><p>也叫做 Master Node，是 Storm 集群工作的全局指挥官。主要功能如下：</p><ol><li>通过 Thrift 接口，监听并接收 Client 提交的 Topology；</li><li>根据集群 Workers 的资源情况，将 Client 提交的 Topology 进行任务分配，分配结果写入 Zookeeper;</li><li>通过 Thrift 接口，监听 Supervisor 的下载 Topology 代码的请求，并提供下载 ;</li><li>通过 Thrift 接口，监听 UI 对统计信息的读取，从 Zookeeper 上读取统计信息，返回给 UI;</li><li>若进程退出后，立即在本机重启，则不影响集群运行。</li></ol><h3 id="2-2-Supervisor进程"><a href="#2-2-Supervisor进程" class="headerlink" title="2.2 Supervisor进程"></a>2.2 Supervisor进程</h3><p>也叫做 Worker Node , 是 Storm 集群的资源管理者，按需启动 Worker 进程。主要功能如下：</p><ol><li>定时从 Zookeeper 检查是否有新 Topology 代码未下载到本地 ，并定时删除旧 Topology 代码 ;</li><li>根据 Nimbus 的任务分配计划，在本机按需启动 1 个或多个 Worker 进程，并监控所有的 Worker 进程的情况；</li><li>若进程退出，立即在本机重启，则不影响集群运行。</li></ol><h3 id="2-3-zookeeper的作用"><a href="#2-3-zookeeper的作用" class="headerlink" title="2.3 zookeeper的作用"></a>2.3 zookeeper的作用</h3><p>Nimbus 和 Supervisor 进程都被设计为<strong>快速失败</strong>（遇到任何意外情况时进程自毁）和<strong>无状态</strong>（所有状态保存在 Zookeeper 或磁盘上）。 这样设计的好处就是如果它们的进程被意外销毁，那么在重新启动后，就只需要从 Zookeeper 上获取之前的状态数据即可，并不会造成任何数据丢失。</p><h3 id="2-4-Worker进程"><a href="#2-4-Worker进程" class="headerlink" title="2.4 Worker进程"></a>2.4 Worker进程</h3><p>Storm 集群的任务构造者 ，构造 Spoult 或 Bolt 的 Task 实例，启动 Executor 线程。主要功能如下：</p><ol><li>根据 Zookeeper 上分配的 Task，在本进程中启动 1 个或多个 Executor 线程，将构造好的 Task 实例交给 Executor 去运行；</li><li>向 Zookeeper 写入心跳 ；</li><li>维持传输队列，发送 Tuple 到其他的 Worker ；</li><li>若进程退出，立即在本机重启，则不影响集群运行。</li></ol><h3 id="2-5-Executor线程"><a href="#2-5-Executor线程" class="headerlink" title="2.5 Executor线程"></a>2.5 Executor线程</h3><p>Storm 集群的任务执行者 ，循环执行 Task 代码。主要功能如下：</p><ol><li>执行 1 个或多个 Task；</li><li>执行 Acker 机制，负责发送 Task 处理状态给对应 Spout 所在的 worker。</li></ol><h3 id="2-6-并行度"><a href="#2-6-并行度" class="headerlink" title="2.6 并行度"></a>2.6 并行度</h3><p><img src="/2020/01/16/zhen-zheng-yi-yi-de-liu-shi-chu-li-kuang-jia-strom/3.png" alt></p><p>1 个 Worker 进程执行的是 1 个 Topology 的子集，不会出现 1 个 Worker 为多个 Topology 服务的情况，因此 1 个运行中的 Topology 就是由集群中多台物理机上的多个 Worker 进程组成的。1 个 Worker 进程会启动 1 个或多个 Executor 线程来执行 1 个 Topology 的 Component(组件，即 Spout 或 Bolt)。</p><p>Executor 是 1 个被 Worker 进程启动的单独线程。每个 Executor 会运行 1 个 Component 中的一个或者多个 Task。</p><p>Task 是组成 Component 的代码单元。Topology 启动后，1 个 Component 的 Task 数目是固定不变的，但该 Component 使用的 Executor 线程数可以动态调整（例如：1 个 Executor 线程可以执行该 Component 的 1 个或多个 Task 实例）。这意味着，对于 1 个 Component 来说，<code>#threads&lt;=#tasks</code>（线程数小于等于 Task 数目）这样的情况是存在的。默认情况下 Task 的数目等于 Executor 线程数，即 1 个 Executor 线程只运行 1 个 Task。</p><p><strong>总结如下：</strong></p><ul><li>一个运行中的 Topology 由集群中的多个 Worker 进程组成的；</li><li>在默认情况下，每个 Worker 进程默认启动一个 Executor 线程；</li><li>在默认情况下，每个 Executor 默认启动一个 Task 线程；</li><li>Task 是组成 Component 的代码单元。</li></ul><h1 id="Strom实战入门"><a href="#Strom实战入门" class="headerlink" title="Strom实战入门"></a>Strom实战入门</h1><p><strong>第一步引入依赖</strong></p><pre><code>       &lt;--使用storm有这个就够了--&gt;       &lt;dependency&gt;            &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;            &lt;artifactId&gt;storm-client&lt;/artifactId&gt;            &lt;version&gt;2.0.0&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;            &lt;artifactId&gt;storm-hbase&lt;/artifactId&gt;            &lt;version&gt;2.0.0&lt;/version&gt;        &lt;/dependency&gt;</code></pre><p><strong>第二步编写Spout获取从HBase中获取数据,类实现IRichSpout接口(也可以继承BaseRichSpout,里面实现了一些基础功能)</strong></p><pre><code>ublic class EventSpout implements IRichSpout {    private SpoutOutputCollector spoutOutputCollector;    private HBaseDao hBaseDao = null;    private Long startRowKey = 1566377894517L;    private Long endRowKey = startRowKey + 300000L;    private List&lt;Result&gt; result = null;    public void open(Map map, TopologyContext topologyContext, SpoutOutputCollector spoutOutputCollector) {        this.spoutOutputCollector = spoutOutputCollector;        hBaseDao = new HBaseDaoImpl();    }    public void nextTuple() {        if (result == null || result.size() &lt;= 0){            result = hBaseDao.getRows(&quot;mjw:tb_event&quot;, startRowKey.toString(),endRowKey.toString());            startRowKey = endRowKey + 1;            long current = System.currentTimeMillis();            endRowKey = endRowKey + 300000 &gt; current ? current : endRowKey + 300000;        }        if (result != null &amp;&amp; result.size() &gt;0){            Result result = this.result.remove(0);            Cell[] cells = result.rawCells();            for (Cell cell : cells) {                if (Bytes.toString(CellUtil.cloneQualifier(cell)).equals(&quot;name&quot;)){                    String name = Bytes.toString(CellUtil.cloneValue(cell));                    spoutOutputCollector.emit(new Values(name));                }            }        } }</code></pre><p><strong>第三步编写Bolt获取处理数据,编写类实现IRichBolt接口</strong></p><pre><code>public class SelectEventBlot implements IRichBolt {    private OutputCollector outputCollector;    private HBaseDaoImpl hBaseDao;    private String event = null;    private String type = null;    public void prepare(Map map, TopologyContext topologyContext, OutputCollector outputCollector) {        this.outputCollector = outputCollector;        hBaseDao = new HBaseDaoImpl();    }    public void execute(Tuple tuple) {        String name = tuple.getString(0);        hBaseDao.insert(&quot;mjw:tb_event_count1&quot;,&quot;test&quot;,&quot;track&quot;,&quot;test&quot;,name);        try{            traveseJson(new JSONObject(name));            hBaseDao.insert(&quot;mjw:tb_event_count1&quot;,type,&quot;track&quot;,type,type);            outputCollector.emit(new Values(event,type));        }catch (Exception e){            e.printStackTrace();        }    }    public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {        outputFieldsDeclarer.declare(new Fields(&quot;event&quot;,&quot;type&quot;));    }    public void traveseJson(Object json){        if(json == null){            return;        }        if(json instanceof JSONObject){//json 是一个map            //将json转换为JsonObject对象            JSONObject jsonStr = (JSONObject) json;            //迭代器迭代 map集合所有的keys            Iterator it = jsonStr.keys();            while(it.hasNext()){                //获取map的key                String key = (String) it.next();                //得到value的值                Object value = jsonStr.get(key);                if (key.equals(&quot;type&quot;)){                    type = (String) value;                }                if (key.equals(&quot;event&quot;)){                    event = (String) value;                }                //递归遍历               traveseJson(value);            }        }else if(json instanceof JSONArray){// if  json 是 数组            JSONArray jsonStr = (JSONArray) json;            //获取Array 的长度            int length = jsonStr.length();            for (int i = 0; i &lt;length; i++) {                traveseJson(jsonStr.get(i));            }        }else {//其他类型            return ;        }    }}</code></pre><p><strong>第四步编写Bolt保存数据(如果逻辑简单,一个bolt就可以了)</strong></p><pre><code>public class SaveEventBlot implements IRichBolt {    private Map eventMap = new HashMap&lt;String,Integer&gt;();    private Map typeMap = new HashMap&lt;String,String&gt;();    private Long beginTime = 0L;    private Long endTime = 0L;    private HBaseDao hBaseDao = null;    public void execute(Tuple tuple) {        if (tuple != null){            String event = tuple.getString(0);            if (eventMap.containsKey(event)){                eventMap.put(event,(Integer)eventMap.get(event) + 1);            }else {                eventMap.put(event,1);            }            String type = tuple.getString(1);            if (!typeMap.containsKey(event)){                typeMap.put(event,type);            }            endTime = System.currentTimeMillis();            if (endTime - beginTime &gt;= 5000) {                // 5s 写一次库                for (Object key : eventMap.keySet()) {                    Result result = hBaseDao.getOneRow(&quot;mjw:tb_event_count1&quot;, typeMap.get(key) + &quot;&quot; + key.toString());                    Integer value = null;                    if (result != null) {                        Cell[] cells = result.rawCells();                        int add_value = 0;                        for (Cell cell : cells) {                            add_value = Integer.parseInt(Bytes.toString(CellUtil.cloneValue(cell)));                        }                         value = add_value +(Integer) eventMap.get(key);                    }                    hBaseDao.insert(&quot;mjw:tb_event_count1&quot;,typeMap.get(key) + &quot;&quot; + key.toString(),(String) typeMap.get(key),key.toString(),value.toString());                }                //重置时间map                eventMap = new HashMap&lt;String,Integer&gt;();                // 需要重置初始时间                beginTime = System.currentTimeMillis();            }        }    }    public Map&lt;String, Object&gt; getComponentConfiguration() {        return null;    }    public void prepare(Map&lt;String, Object&gt; map, TopologyContext topologyContext, OutputCollector outputCollector) {        hBaseDao = new HBaseDaoImpl();        beginTime = System.currentTimeMillis();    }}</code></pre><p><strong>第五步编写启动类,构建整个拓扑结构,然后提交任务</strong></p><pre><code>public class EventTopology {    public static void main(String[] args) {        TopologyBuilder builder = new TopologyBuilder();        builder.setSpout(&quot;spout&quot;,new EventSpout());        builder.setBolt(&quot;select_blot&quot;, new SelectEventBlot()).shuffleGrouping(&quot;spout&quot;);        builder.setBolt(&quot;save_blot&quot;, new SaveEventBlot()).shuffleGrouping(&quot;select_blot&quot;);        Config config = new Config();        if (args.length &gt; 0){            try {                StormSubmitter.submitTopology(args[0],config,builder.createTopology());            } catch (Exception e) {                e.printStackTrace();            }        }    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark到底好在哪里?</title>
      <link href="/2020/01/16/spark-dao-di-hao-zai-na-li/"/>
      <url>/2020/01/16/spark-dao-di-hao-zai-na-li/</url>
      
        <content type="html"><![CDATA[<h2 id="Spark简介"><a href="#Spark简介" class="headerlink" title="Spark简介"></a>Spark简介</h2><p>Apache Spark 是当今最流行的开源大数据处理框架。和人们耳熟能详的 MapReduce 一样，Spark 用于进行分布式、大规模的数据处理，但 Spark 作为 MapReduce 的接任者，提供了更高级的编程接口、更高的性能。除此之外，Spark 不仅能进行常规的批处理计算，还提供了流式计算支持。</p><p>Apache Spark 诞生于大名鼎鼎的 AMPLab（这里还诞生过 Mesos 和 Alluxio），从创立之初就带有浓厚的学术气质，其设计目标是为各种大数据处理需求提供一个统一的技术栈。如今 Spark 背后的商业公司 Databricks 创始人也是来自 AMPLab 的博士毕业生。</p><p>Spark 本身使用 Scala 语言编写，Scala 是一门融合了面向对象与函数式的“双范式”语言，运行在 JVM 之上。Spark 大量使用了它的函数式、即时代码生成等特性。Spark 目前提供了 Java、Scala、Python、R 四种语言的 API，前两者因为同样运行在 JVM 上可以达到更原生的支持。</p><h2 id="MapReduce-的问题所在"><a href="#MapReduce-的问题所在" class="headerlink" title="MapReduce 的问题所在"></a>MapReduce 的问题所在</h2><p>Hadoop 是大数据处理领域的开创者。严格来说，Hadoop 不只是一个软件，而是一整套生态系统，例如 MapReduce 负责进行分布式计算，而 HDFS 负责存储大量文件。</p><p>MapReduce 模型的诞生是大数据处理从无到有的飞跃。但随着技术的进步，对大数据处理的需求也变得越来越复杂，MapReduce 的问题也日渐凸显。通常，我们将 MapReduce 的输入和输出数据保留在 HDFS 上，很多时候，<strong>复杂的 ETL、数据清洗等工作无法用一次 MapReduce 完成，所以需要将多个 MapReduce 过程连接起来</strong>：</p><p><img src="/2020/01/16/spark-dao-di-hao-zai-na-li/1.png" alt="multi-stage-mapreduce"></p><p>▲ <em>上图中只有两个 MapReduce 串联，实际上可能有几十个甚至更多，依赖关系也更复杂。</em></p><p>这种方式下，<strong>每次中间结果都要写入 HDFS 落盘保存，代价很大</strong>（别忘了，HDFS 的每份数据都需要冗余若干份拷贝）。另外，由于本质上是多次 MapReduce 任务，调度也比较麻烦，实时性无从谈起。</p><h2 id="Spark-与-RDD-模型"><a href="#Spark-与-RDD-模型" class="headerlink" title="Spark 与 RDD 模型"></a>Spark 与 RDD 模型</h2><p>针对上面的问题，如果能把中间结果保存在内存里，岂不是快的多？之所以不能这么做，最大的障碍是：分布式系统必须能容忍一定的故障，所谓 fault-tolerance。如果只是放在内存中，一旦某个计算节点宕机，其他节点无法恢复出丢失的数据，只能重启整个计算任务，这对于动辄成百上千节点的集群来说是不可接受的。</p><p>一般来说，想做到 fault-tolerance 只有两个方案：要么存储到外部（例如 HDFS），要么拷贝到多个副本。<strong>Spark 大胆地提出了第三种——重算一遍。但是之所以能做到这一点，是依赖于一个额外的假设：所有计算过程都是确定性的（deterministic）。</strong>Spark 借鉴了函数式编程思想，提出了 RDD（Resilient Distributed Datasets），译作“弹性分布式数据集”。</p><p><img src="/2020/01/16/spark-dao-di-hao-zai-na-li/2.png" alt="rdd-example"></p><p><strong>RDD 是一个只读的、分区的（partitioned）数据集合</strong>。RDD 要么来源于不可变的外部文件（例如 HDFS 上的文件），要么由确定的算子由其他 RDD 计算得到。<strong>RDD 通过算子连接构成有向无环图（DAG）</strong>，上图演示了一个简单的例子，其中节点对应 RDD，边对应算子。</p><p>回到刚刚的问题，RDD 如何做到 fault-tolerance？很简单，RDD 中的每个分区都能被确定性的计算出来，所以<strong>一旦某个分区丢失了，另一个计算节点可以从它的前继节点出发、用同样的计算过程重算一次，即可得到完全一样的 RDD 分区</strong>。这个过程可以递归的进行下去。</p><p><img src="/2020/01/16/spark-dao-di-hao-zai-na-li/3.png" alt="rdd-example-crash"></p><p>▲ <em>上图演示了 RDD 分区的恢复。为了简洁并没有画出分区，实际上恢复是以分区为单位的。</em></p><p>Spark 的编程接口和 Java 8 的 Stream 很相似：RDD 作为数据，在多种算子间变换，构成对执行计划 DAG 的描述。最后，一旦遇到类似 <code>collect()</code> 这样的输出命令，执行计划会被发往 Spark 集群、开始计算。不难发现，算子分成两类：</p><ul><li><code>map()</code>、<code>filter()</code>、<code>join()</code> 等算子称为 Transformation，它们输入一个或多个 RDD，输出一个 RDD。</li><li><code>collect()</code>、<code>count()</code>、<code>save()</code> 等算子称为 Action，它们通常是将数据收集起来返回；</li></ul><p><img src="/2020/01/16/spark-dao-di-hao-zai-na-li/4.png" alt="spark-rdd-api-example"></p><p>▲ <em>上图的例子用来收集包含“HDFS”关键字的错误日志时间戳。当执行到 collect() 时，右边的执行计划开始运行。</em></p><p>像之前提到的，RDD 的数据由多个分区（partition）构成，这些分区可以分布在集群的各个机器上，这也就是 RDD 中 “distributed” 的含义。熟悉 DBMS 的同学可以把 RDD 理解为逻辑执行计划，partition 理解为物理执行计划。</p><p>此外，RDD 还包含它的每个分区的依赖分区（dependency），以及一个函数指出如何计算出本分区的数据。Spark 的设计者发现，依赖关系依据执行方式的不同可以很自然地分成两种：<strong>窄依赖（Narrow Dependency）和宽依赖（Wide Dependency）</strong>，举例来说：</p><ul><li><code>map()</code>、<code>filter()</code> 等算子构成窄依赖：生产的每个分区只依赖父 RDD 中的一个分区。</li><li><code>groupByKey()</code> 等算子构成宽依赖：生成的每个分区依赖父 RDD 中的多个分区（往往是全部分区）。</li></ul><p><img src="/2020/01/16/spark-dao-di-hao-zai-na-li/5.png" alt="2-kinds-of-dependencies"></p><p>▲ <em>左图展示了宽依赖和窄依赖，其中 Join 算子因为 Join key 分区情况不同二者皆有；右图展示了执行过程，由于宽依赖的存在，执行计划被分成 3 个阶段。</em></p><p>在执行时，窄依赖可以很容易的按流水线（pipeline）的方式计算：对于每个分区从前到后依次代入各个算子即可。<strong>然而，宽依赖需要等待前继 RDD 中所有分区计算完成；换句话说，宽依赖就像一个栅栏（barrier）会阻塞到之前的所有计算完成。</strong>整个计算过程被宽依赖分割成多个阶段（stage），如上右图所示。</p><blockquote><p>了解 MapReduce 的同学可能已经发现，宽依赖本质上就是一个 MapReduce 过程。但是相比 MapReduce 自己写 Map 和 Reduce 函数的编程接口，Spark 的接口要容易的多；并且在 Spark 中，多个阶段的 MapReduce 只需要构造一个 DAG 即可。</p></blockquote><h2 id="声明式接口：Spark-SQL"><a href="#声明式接口：Spark-SQL" class="headerlink" title="声明式接口：Spark SQL"></a>声明式接口：Spark SQL</h2><p>Spark 诞生后，大幅简化了 MapReduce 编程模型，但人们并不满足于此。我们知道，<strong>与命令式（imperative）编程相对的是声明式（declarative）编程，前者需要告诉程序怎样得到我需要的结果，后者则是告诉程序我需要的结果是什么</strong>。举例而言：你想知道，各个部门 <code>&lt;dept_id, dept_name&gt;</code> 中性别为女 <code>&#39;female&#39;</code> 的员工分别有多少？</p><p>命令式编程中，你需要编写一个程序。下面给出了一种伪代码实现：</p><pre><code>employees = db.getAllEmployees()countByDept = dict() // 统计各部门女生人数 (dept_id -&gt; count)for employee in employees:    if (employee.gender == &#39;female&#39;)        countByDept[employee.dept_id] += 1results = list() // 加上 dept.name 列depts = db.getAllDepartments()for dept in depts:    if (countByDept containsKey dept.id)        results.add(row(dept.id, dept.name, countByDept[dept.id]))return results;</code></pre><p>声明式编程中，你只要用关系代数的运算表达出结果：</p><pre><code>employees.join(dept, employees.deptId == dept.id)         .where(employees.gender == &#39;female&#39;)         .groupBy(dept.id, dept.name)         .agg()</code></pre><blockquote><p>等价地，如果你更熟悉 SQL，也可以写成这样：</p><pre><code>SELECT dept.id, dept.name, COUNT(*)FROM employees JOIN dept ON employees.dept_id == dept.idWHERE employees.gender = &#39;female&#39;GROUP BY dept.id, dept.name</code></pre></blockquote><p>显然，声明式的要简洁的多！但声明式编程依赖于执行者产生真正的程序代码，所以除了上面这段程序，还需要把数据模型（即 schema）一并告知执行者。声明式编程最广为人知的形式就是 SQL。</p><p>Spark SQL 就是这样一个基于 SQL 的声明式编程接口。<strong>你可以将它看作在 Spark 之上的一层封装，在 RDD 计算模型的基础上，提供了 DataFrame API 以及一个内置的 SQL 执行计划优化器 Catalyst。</strong></p><p><img src="/2020/01/16/spark-dao-di-hao-zai-na-li/6.png" alt="spark-sql-arch"></p><p>▲ <em>上图黄色部分是 Spark SQL 中新增的部分。</em></p><p><strong>DataFrame 就像数据库中的表，除了数据之外它还保存了数据的 schema 信息。</strong>计算中，schema 信息也会经过算子进行相应的变换。DataFrame 的数据是行（row）对象组成的 RDD，对 DataFrame 的操作最终会变成对底层 RDD 的操作。</p><p><strong>Catalyst 是一个内置的 SQL 优化器，负责把用户输入的 SQL 转化成执行计划。</strong>Catelyst 强大之处是它利用了 Scala 提供的代码生成（codegen）机制，物理执行计划经过编译，产出的执行代码效率很高，和直接操作 RDD 的命令式代码几乎没有分别。</p><p><img src="/2020/01/16/spark-dao-di-hao-zai-na-li/7.png" alt="spark-sql-catalyst"></p><p>▲ <em>上图是 Catalyst 的工作流程，与大多数 SQL 优化器一样是一个 Cost-Based Optimizer (CBO)，但最后使用代码生成（codegen）转化成直接对 RDD 的操作。</em></p><h2 id="流计算框架：Spark-Streaming"><a href="#流计算框架：Spark-Streaming" class="headerlink" title="流计算框架：Spark Streaming"></a>流计算框架：Spark Streaming</h2><p>以往，批处理和流计算被看作大数据系统的两个方面。我们常常能看到这样的架构——以 Kafka、Storm 为代表的流计算框架用于实时计算，而 Spark 或 MapReduce 则负责每天、每小时的数据批处理。在 ETL 等场合，这样的设计常常导致同样的计算逻辑被实现两次，耗费人力不说，保证一致性也是个问题。</p><p>Spark Streaming 正是诞生于此类需求。传统的流计算框架大多注重于低延迟，采用了持续的（continuous）算子模型；而 Spark Streaming 基于 Spark，另辟蹊径提出了 <strong>D-Stream（Discretized Streams）方案：将流数据切成很小的批（micro-batch），用一系列的短暂、无状态、确定性的批处理实现流处理。</strong></p><p><img src="/2020/01/16/spark-dao-di-hao-zai-na-li/8.png" alt="spark-streaming"></p><p>Spark Streaming 的做法在流计算框架中很有创新性，它虽然牺牲了低延迟（一般流计算能做到 100ms 级别，Spark Streaming 延迟一般为 1s 左右），但是带来了三个诱人的优势：</p><ul><li>更高的吞吐量（大约是 Storm 的 2-5 倍）</li><li>更快速的失败恢复（通常只要 1-2s），因此对于 straggler（性能拖后腿的节点）直接杀掉即可</li><li>开发者只需要维护一套 ETL 逻辑即可同时用于批处理和流计算</li></ul><p><img src="/2020/01/16/spark-dao-di-hao-zai-na-li/9.png" alt="continuous-vs-spark-d-stream"></p><p>▲ <em>上左图中，为了在持续算子模型的流计算系统中保证一致性，不得不在主备机之间使用同步机制，导致性能损失，Spark Streaming 完全没有这个问题；右图是 D-Stream 的原理示意图。</em></p><p>你可能会困惑，流计算中的状态一直是个难题。但我们刚刚提到 D-Stream 方案是无状态的，那诸如 word count 之类的问题，怎么做到保持 count 算子的状态呢？</p><p>答案是通过 RDD：<strong>将前一个时间步的 RDD 作为当前时间步的 RDD 的前继节点，就能造成状态不断更替的效果</strong>。实际上，新的状态 RDD 总是不断生成，而旧的 RDD 并不会被“替代”，而是作为新 RDD 的前继依赖。对于底层的 Spark 框架来说，并没有时间步的概念，有的只是不断扩张的 DAG 图和新的 RDD 节点。</p><p><img src="/2020/01/16/spark-dao-di-hao-zai-na-li/10.png" alt="d-stream-state-rdd"></p><p>▲ <em>上图是流式计算 word count 的例子，count 结果在不同时间步中不断累积。</em></p><p>那么另一个问题也随之而来：随着时间的推进，上图中的状态 RDD <code>counts</code> 会越来越多，他的祖先（lineage）变得越来越长，极端情况下，恢复过程可能溯源到很久之前。这是不可接受的！<strong>因此，Spark Streming 会定期地对状态 RDD 做 checkpoint，将其持久化到 HDFS 等存储中，这被称为 lineage cut</strong>，在它之前更早的 RDD 就可以没有顾虑地清理掉了。</p><h2 id="流计算与-SQL：Spark-Structured-Streaming"><a href="#流计算与-SQL：Spark-Structured-Streaming" class="headerlink" title="流计算与 SQL：Spark Structured Streaming"></a>流计算与 SQL：Spark Structured Streaming</h2><p>Spark 通过 Spark Streaming 拥有了流计算能力，那 Spark SQL 是否也能具有类似的流处理能力呢？答案是肯定的，只要将<strong>数据流建模成一张不断增长、没有边界的表</strong>，在这样的语义之下，很多 SQL 操作等就能直接应用在流数据上。</p><blockquote><p>出人意料的是，Spark Structured Streaming 的流式计算引擎并没有复用 Spark Streaming，而是在 Spark SQL 上设计了新的一套引擎。因此，从 Spark SQL 迁移到 Spark Structured Streaming 十分容易，但从 Spark Streaming 迁移过来就要困难得多。</p></blockquote><p>很自然的，基于这样的模型，Spark SQL 中的大部分接口、实现都得以在 Spark Structured Streaming 中直接复用。将用户的 SQL 执行计划转化成流计算执行计划的过程被称为<strong>增量化</strong>（incrementalize），这一步是由 Spark 框架自动完成的。对于用户来说只要知道：每次计算的输入是某一小段时间的流数据，而输出是对应数据产生的计算结果。</p><p><img src="/2020/01/16/spark-dao-di-hao-zai-na-li/11.png" alt="spark-structured-streaming-mode"></p><p>▲ <em>左图是 Spark Structured Streaming 模型示意图；右图展示了同一个任务的批处理、流计算版本，可以看到，除了输入输出不同，内部计算过程完全相同。</em></p><p>与 Spark SQL 相比，流式 SQL 计算还有两个额外的特性，分别是窗口（window）和水位（watermark）。</p><p><strong>窗口（window）是对过去某段时间的定义。</strong>批处理中，查询通常是全量的（例如：总用户量是多少）；而流计算中，我们通常关心近期一段时间的数据（例如：最近24小时新增的用户量是多少）。用户通过选用合适的窗口来获得自己所需的计算结果，常见的窗口有滑动窗口（Sliding Window）、滚动窗口（Tumbling Window）等。</p><p><strong>水位（watermark）用来丢弃过早的数据。</strong>在流计算中，上游的输入事件可能存在不确定的延迟，而流计算系统的内存是有限的、只能保存有限的状态，一定时间之后必须丢弃历史数据。以双流 A JOIN B 为例，假设窗口为 1 小时，那么 A 中比当前时间减 1 小时更早的数据（行）会被丢弃；如果 B 中出现 1 小时前的事件，因为无法处理只能忽略。</p><p><img src="/2020/01/16/spark-dao-di-hao-zai-na-li/12.png" alt="spark-structured-streaming-watermark"></p><p>▲ <em>上图为水位的示意图，“迟到”太久的数据（行）由于已经低于当前水位无法处理，将被忽略。</em></p><p>水位和窗口的概念都是因时间而来。在其他流计算系统中，也存在相同或类似的概念。</p><blockquote><p>关于 SQL 的流计算模型，常常被拿来对比的还有另一个流计算框架 <a href="https://flink.apache.org/" target="_blank" rel="noopener">Apache Flink</a>。与 Spark 相比，它们的实现思路有很大不同，但在模型上是很相似的。</p></blockquote><h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><p>Spark 中有三个角色：Driver, Worker 和 Cluster Manager。</p><p><img src="/2020/01/16/spark-dao-di-hao-zai-na-li/13.png" alt="cluster-overview"></p><p><strong>驱动程序（Driver）</strong>即用户编写的程序，对应一个 <code>SparkContext</code>，负责任务的构造、调度、故障恢复等。驱动程序可以直接运行在客户端，例如用户的应用程序中；也可以托管在 Master 上，这被称为集群模式（cluster mode），通常用于流计算等长期任务。</p><p><strong>Cluster Manager</strong> 顾名思义负责集群的资源分配，Spark 自带的 Spark Master 支持任务的资源分配，并包含一个 Web UI 用来监控任务运行状况。多个 Master 可以构成一主多备，通过 ZooKeeper 进行协调和故障恢复。通常 Spark 集群使用 Spark Master 即可，但如果用户的集群中不仅有 Spark 框架、还要承担其他任务，官方推荐使用 Mesos 作为集群调度器。</p><p><strong>Worker</strong> 节点负责执行计算任务，上面保存了 RDD 等数据。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hive-基于MapReduce的数据分析工具</title>
      <link href="/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/"/>
      <url>/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/</url>
      
        <content type="html"><![CDATA[<h2 id="Hive概述"><a href="#Hive概述" class="headerlink" title="Hive概述"></a>Hive概述</h2><p>Hive 的底层执行引擎有 ：MapReduce，Tez，Spark- Hive on MapReduce- Hive on Tez- Hive on spark</p><p>压缩：GZIP,LZO,Snappy,Bzip2…存储：Textfile，SequenceFile，RcFile，ORC，ParquetUDF：自定义函数</p><p><img src="/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/1.png" alt></p><blockquote><p>为什么要使用Hive：简单，容易上手(提供了类SQL的查询语言HQL)为超大数据集设计的计算/存储扩展能力（MR计算,HDFS存储）统一的元数据管理（可与Pretso/Impala/SparkSQL数据共享）</p></blockquote><h2 id="Hive-的体系结构"><a href="#Hive-的体系结构" class="headerlink" title="Hive 的体系结构"></a>Hive 的体系结构</h2><h3 id="1-Hive的元数据"><a href="#1-Hive的元数据" class="headerlink" title="1.Hive的元数据"></a>1.Hive的元数据</h3><p><img src="/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/2.png" alt></p><h3 id="2-HQL-的执行过程"><a href="#2-HQL-的执行过程" class="headerlink" title="2.HQL 的执行过程"></a>2.HQL 的执行过程</h3><ul><li>解释器、编译器、优化器完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划（PLAN）的生产，生产的查询计划存储在HDFS中，并在随后有MapReduce调用执行</li></ul><p><img src="/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/3.png" alt></p><h3 id="3-体系结构"><a href="#3-体系结构" class="headerlink" title="3.体系结构"></a>3.体系结构</h3><p><img src="/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/4.png" alt></p><p><img src="/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/5.png" alt></p><h3 id="4-Hive-生产环境部署架构"><a href="#4-Hive-生产环境部署架构" class="headerlink" title="4.Hive 生产环境部署架构"></a>4.Hive 生产环境部署架构</h3><p><img src="/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/6.png" alt></p><h2 id="Hive-安装"><a href="#Hive-安装" class="headerlink" title="Hive 安装"></a>Hive 安装</h2><p><img src="/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/8.png" alt></p><h3 id="1-嵌入入模式-元数据保存在自己维护的dirbe数据库"><a href="#1-嵌入入模式-元数据保存在自己维护的dirbe数据库" class="headerlink" title="1.嵌入入模式(元数据保存在自己维护的dirbe数据库)"></a>1.嵌入入模式(元数据保存在自己维护的dirbe数据库)</h3><pre><code>解压好文件夹后直接进入bin目录执行hive脚本${HIVE_HOME}/bin/hive</code></pre><h3 id="2-本地模式或者远程模式-元数据保存在本地或者远程的mysql库"><a href="#2-本地模式或者远程模式-元数据保存在本地或者远程的mysql库" class="headerlink" title="2.本地模式或者远程模式(元数据保存在本地或者远程的mysql库)"></a>2.本地模式或者远程模式(元数据保存在本地或者远程的mysql库)</h3><p>修改hive-site.xml</p><pre><code>&lt;!-- jdbc 参数 --&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;    &lt;value&gt;jdbc:mysql://localhost:3306/hive&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;    &lt;value&gt;root&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;    &lt;value&gt;root&lt;/value&gt;  &lt;/property&gt;</code></pre><h2 id="Hive-管理"><a href="#Hive-管理" class="headerlink" title="Hive 管理"></a>Hive 管理</h2><h3 id="1-cli模式"><a href="#1-cli模式" class="headerlink" title="1.cli模式"></a>1.cli模式</h3><pre><code># 进入cli${HIVE_HOME}/bin/hive --service cli# 1. Hive -S进入静默模式，不会打印MapReduce作业调试信息# 2. 一般情况下，hive执行SQL都会转换成MapReduce作业进行执行，但是如果是使用select * 则不会转换成mr任务${HIVE_HOME}/bin/hive -S# 不进入交互模式${HIVE_HOME}/bin/hive -e {sql语句}</code></pre><h3 id="2-web管理界面模式-只能做查询"><a href="#2-web管理界面模式-只能做查询" class="headerlink" title="2.web管理界面模式(只能做查询)"></a>2.web管理界面模式(只能做查询)</h3><ol><li>进入hive的源代码目录的hwi目录 <code>${HIVE_SRC_HOME}/hwi</code></li><li>将其打包编译<code>mvn package(需要安装mvn环境)</code></li><li>将打好的包放入<code>${HIVE_HOME}/lib/</code> 目录下</li><li>修改 hive_site.xml</li></ol><pre><code>&lt;!-- web界面监听的主机地址 --&gt;  &lt;property&gt;  &lt;name&gt;hive.hwi.listen.host&lt;/name&gt;  &lt;value&gt;0.0.0.0&lt;/value&gt;  &lt;description&gt;This is the host address the Hive Web Interface will listen on&lt;/description&gt;&lt;/property&gt; &lt;!-- web界面监听的端口 --&gt;&lt;property&gt;  &lt;name&gt;hive.hwi.listen.port&lt;/name&gt;  &lt;value&gt;9999&lt;/value&gt;  &lt;description&gt;This is the port the Hive Web Interface will listen on&lt;/description&gt;&lt;/property&gt; &lt;!-- war包的位置 --&gt;&lt;property&gt;  &lt;name&gt;hive.hwi.war.file&lt;/name&gt;  &lt;value&gt;${HIVE_HOME}/lib/hive-hwi-&lt;version&gt;.war&lt;/value&gt;  &lt;description&gt;This is the WAR file with the jsp content for Hive Web Interface&lt;/description&gt;&lt;/property&gt;</code></pre><p>6.拷贝jdk目录下的tools.jar 到hive的lib下</p><pre><code>cp ${JAVA_HOME}/lib/tools.jar ${HIVE_HOME}/lib</code></pre><ol><li>启动web服务</li></ol><pre><code>${HIVE_HOME}/bin/hive --service hwi</code></pre><p>验证：浏览器访问 <a href="http://localhost:9999/hwi/" target="_blank" rel="noopener">http://localhost:9999/hwi/</a></p><h3 id="3-远程连接"><a href="#3-远程连接" class="headerlink" title="3.远程连接"></a>3.远程连接</h3><pre><code>${HIVE_HOME}/bin/hive --service hiveserver</code></pre><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h3 id="1-基本数据类型"><a href="#1-基本数据类型" class="headerlink" title="1.基本数据类型"></a>1.基本数据类型</h3><p><img src="/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/9.png" alt></p><pre><code>hive新版本中，新增了两种字符串类型 varchar和charvarchar(20) 最大长度是20 ，可伸缩char(20) 固定长度20</code></pre><h3 id="2-复杂数据类型"><a href="#2-复杂数据类型" class="headerlink" title="2.复杂数据类型"></a>2.复杂数据类型</h3><p><img src="/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/10.png" alt></p><pre><code>create table student1( sid int ,  sname string,  score array&lt;float&gt;)create table studetnt2( sid int ,  sname string,  score map&lt;string,float&gt;)create table student3( sid int ,  info struct&lt;name:string,age:int,sex:string&gt;)</code></pre><h3 id="3-时间类型"><a href="#3-时间类型" class="headerlink" title="3.时间类型"></a>3.时间类型</h3><p><img src="/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/11.png" alt></p><pre><code>timestamp 与时区无关，是自从有了unix以来的偏移量date 描述的是特定的日期 YYYY-MM-DD</code></pre><h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h2><h3 id="1-数据存储"><a href="#1-数据存储" class="headerlink" title="1.数据存储"></a>1.数据存储</h3><ul><li>基于HDFS的默认存储在 <code>/user/hive/warehouse/</code> 下</li><li>没有专门的数据存储格式</li></ul><table><thead><tr><th>sid</th><th>sname</th></tr></thead><tbody><tr><td>1</td><td>Tom</td></tr><tr><td>2</td><td>Mary</td></tr></tbody></table><p>这张表在文件中默认存储为文件，使用垂直制表符分割</p><pre><code>1 Tom2 Mary</code></pre><ul><li>存储结构主要包括：数据库 文件 表 视图</li><li>可以直接加载文本文件（.txt等）进行数据添加</li><li>创建表时，可以指定Hive数据的列分隔符和行分隔符</li><li>表</li></ul><pre><code>    · Table 内部表    · Partition 分区表    · External 外部表    · Bucket Table 桶表</code></pre><h3 id="2-详解表"><a href="#2-详解表" class="headerlink" title="2.详解表"></a>2.详解表</h3><ul><li><h4 id="Table-内部表"><a href="#Table-内部表" class="headerlink" title="Table 内部表"></a>Table 内部表</h4></li></ul><p><img src="/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/12.png" alt></p><pre><code>create table student1( sid int ,  sname string)location &#39;${目录}&#39; row format  delimited fields terminated by &#39;列分隔符&#39;</code></pre><ul><li><h4 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h4></li></ul><p><img src="/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/13.png" alt></p><pre><code>create table partition_table(    sid int,    sname string) partitioned by (gender string)row format  delimited fields terminated by &#39;,&#39;;</code></pre><ul><li><h4 id="外部表"><a href="#外部表" class="headerlink" title="外部表"></a>外部表</h4></li></ul><p><img src="/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/14.png" alt></p><p><img src="https://user-gold-cdn.xitu.io/2018/12/12/167a101df8d521cd?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt></p><pre><code>create external table partition_table(    sid int,    sname string) row format  delimited fields terminated by &#39;,&#39;location &#39;/input&#39;;-- input 目录中有相关数据</code></pre><ul><li><h4 id="桶表"><a href="#桶表" class="headerlink" title="桶表"></a>桶表</h4></li></ul><p><img src="https://user-gold-cdn.xitu.io/2018/12/12/167a101e0beef1ad?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt></p><p><img src="https://user-gold-cdn.xitu.io/2018/12/12/167a101e0fa30c69?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt></p><pre><code>create external table partition_table(    sid int,    sname string) clustered by({hash的字段}) into {桶的数量} buckets-- input 目录中有相关数据</code></pre><h3 id="3-视图"><a href="#3-视图" class="headerlink" title="3.视图"></a>3.视图</h3><p><img src="/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/15.png" alt></p><p><img src="/2020/01/16/hive-ji-yu-mapreduce-de-shu-ju-fen-xi-gong-ju/16.png" alt></p><h2 id="Hive-的数据导入"><a href="#Hive-的数据导入" class="headerlink" title="Hive 的数据导入"></a>Hive 的数据导入</h2><h3 id="使用load语句导入"><a href="#使用load语句导入" class="headerlink" title="使用load语句导入"></a>使用load语句导入</h3><pre><code>LOAD DATE [LOCAL] INPATH &#39;filepath&#39; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1,partcol2=val2)]-- [LOCAL] 代表从本地文件系统导入，否则从HDFS中导入-- [OVERWRITE] 代表覆盖原有的数据-- [PARTITION] 代表分区-- 如果filepah是一个文件则导入一个文件的数据，如果是一个目录，则导入该目录下所有的文件</code></pre><h2 id="Hive-调优"><a href="#Hive-调优" class="headerlink" title="Hive 调优"></a>Hive 调优</h2><pre><code>-- 动态分区，根据插入的记录自动分区SET hive.exec.dynamic.partition=true;SET hive.exec.dynamic.partition.mode=nonstrict;-- 并行执行，子查询可以并行执行SET hive.exec.parallel=true;-- 计算结束以后将小文件合并SET hive.merge.mapredfiles=true;-- 如果某个维表小于100000000B（100M),就做MAP关联，不用到reduce阶段SET hive.mapjoin.smalltable.filesize=100000000;-- 超时时间SET mapred.task.timeout=1800000;-- 添加自定义jar包ADD jar viewfs://hadoop-meituan/user/hadoop-hotel/user_upload/gaowenfeng02_hive-udf-zhaoxiang.jar;-- 创建UDFCREATE TEMPORARY FUNCTION get_tag_list as &#39;com.meituan.hive.udf.common.ResolveTagUdf&#39;;-- map jvm内存设置3G-- SET mapred.map.child.java.opts=&quot;-Xmx3072m&quot;; -- map task 的内存 约等于4G-- SET mapreduce.map.memory.mb=4000;-- reduce jvm内存设置3G-- SET mapred.reduce.child.java.opts=&quot;-Xmx3072m&quot;;-- reduce task 的内存 约等于4G</code></pre><p>Hive教程：<a href="https://www.yiibai.com/hive/" target="_blank" rel="noopener">www.yiibai.com/hive/</a></p><h2 id="ETL的优化"><a href="#ETL的优化" class="headerlink" title="ETL的优化"></a>ETL的优化</h2><p>hive.exec.reducers.bytes.per.reducer    这个参数控制一个job会有多少个reducer来处理，依据的是输入文件的总大小。默认1GB。（即每个reduce任务处理的数据量。）</p><p>hive.exec.reducers.max     这个参数控制最大的reducer的数量， 如果 input / bytes per reduce &gt; max  则会启动这个参数所指定的reduce个数。  这个并不会影响mapre.reduce.tasks参数的设置。默认的max是999。</p><p>mapred.reduce.tasks  这个参数如果指定了，hive就不会用它的estimation函数来自动计算reduce的个数，而是用这个参数来启动reducer。默认是-1.</p><p>reduce的个数设置其实对执行效率有很大的影响：1、如果reduce太少： 如果数据量很大，会导致这个reduce异常的慢，从而导致这个任务不能结束，也有可能会OOM2、如果reduce太多： 产生的小文件太多，合并起来代价太高，namenode的内存占用也会增大。</p><p>如果我们不指定mapred.reduce.tasks， hive会自动计算需要多少个reducer。计算的公式：  reduce个数 =  InputFileSize   /   bytes per reducer</p><p>mapreduce.map.memory.mb    每个Map Task需要的内存量mapreduce.reduce.memory.mb    每个Reduce Task需要的内存量</p><p>查看任务执行的日志：XT平台生产运维栏目中，调度管理下的执行日志测试参数：</p><ul><li>-delta 1 -v</li></ul><p>测试的表名：ba_hotel_test.topic_log_mt_order_trade_entrance线上的表名：ba_hotel.topic_log_mt_order_trade_entrance测试流量：页面流量，模块流量（某个页面之前前的页面流量一定是大于该页面的流量）</p><p>任务流程—测试及上线：测试完再上线，测试包括线下测试和线上测试提交审核，审核通过后就自动上线了在XT平台中，该任务下点执行计划，再进行线上测试</p><p>map、reduce java代码讲解ba_hotel.topic_log_mt_order_trade_entrance.mpt_trackba_hotel.topic_log_mt_order_trade_entrance.patch_trackba_hotel.topic_log_mt_order_trade_entrance.mge_track</p><p>ba_travel.topic_log_tag_moudlefact_log_tag_pv</p><p>优化排查：1.最后一个map少，时间长2.reduce一直在99%，发生了数据倾斜3.job交接时间长，说明碎片多</p><p>优化：1.ETL语句执行问题：问Hadoop小客服2.子查询，精简数据3.子查询之间的关联，是否数据倾斜4.参数调高</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Kafka-大数据的标准数据源</title>
      <link href="/2020/01/16/kafka-da-shu-ju-de-biao-zhun-shu-ju-yuan/"/>
      <url>/2020/01/16/kafka-da-shu-ju-de-biao-zhun-shu-ju-yuan/</url>
      
        <content type="html"><![CDATA[<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><blockquote><p>消息队列中的基本概念尤为重要,当对基本概念有了深入的理解之后,消息队列的原理以及常见的问题都将更浅显明了。</p></blockquote><ol><li><p>Broker:一个单独的Kafka server就是一个Broker,Broker的主要工作就是接收生产者发送来的消息,分配offset,然后将包装过的数据保存到磁盘上;此外,Broker还会接收消费者和其他Broker的请求,根据请求的类型进行相应的处理然后返回响应。多个Broker可以做成一个Cluster(集群)对外提供服务,每个Cluster当中会选出一个Broker来担任Controller,Controller是Kafka集群的指挥中心,其他的Broker则听从Controller指挥实现相应的功能。Controller负责管理分区的状态、管理每个分区的副本状态、监听zookeeper中数据的变化等。Controller也是一主多从的实现,所有的Broker都会监听Controller Leader的状态,当Leader Controller出现了故障的时候就重新选举新的Controller Leader。</p></li><li><p>消息:消息是Kafka中最基本的消息单元。消息由一串字节组成,其中主要由key和value构成,key和value都是字节数组。key的主要作用是根据一定的策略,将这个消息路由到指定的分区中,这样就可以保证包含同一个key的消息全部写入同一个分区</p></li><li><p>Topic:Topic是用于存储消息的逻辑概念,Topic可以看做是一个消息的集合。每个Topic可以有多个生产者向其中push消息,也可以有多个消费者向其中pull消息。</p></li><li><p>分区(partition):每一个Topic都可以划分成多个分区(每一个Topic都至少有一个分区),不同的分区会分配在不同的Broker上以对Kafka进行水平扩展从而增加Kafka的并行处理能力。同一个Topic下的不同分区包含的消息是不同的。每一个消息在被添加到分区的时候,都会被分配一个offset,他是消息在此分区中的唯一编号,此外,Kafka通过offset保证消息在分区中的顺序,offset的顺序性不跨分区,也就是说在Kafka的同一个分区中的消息是有序的,不同分区的消息可能不是有序的。Partitions概念图</p><p>​</p><p><img src="/2020/01/16/kafka-da-shu-ju-de-biao-zhun-shu-ju-yuan/D:%5CMyBlot%5Cbolt%5Csource_posts%5CKafka-%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E6%BA%90%5C1.png" alt></p><p>​</p></li><li><p>Log:分区在逻辑上对应着一个Log,当生产者将消息写入分区的时候,实际上就是写入到了一个Log中。Log是一个逻辑概念,对应的是一个磁盘上的文件夹。Log由多个Segment组成,每一个Segment又对应着一个日志文件和一个索引文件。</p></li><li><p>副本:Kafka对消息进行了冗余备份,每一个分区都可以有多个副本,每一个副本中包含的消息是相同的(但不保证同一时刻下完全相同)。副本的类型分为Leader和Follower,当分区只有一个副本的时候,该副本属于Leader,没有Follower。Kafka的副本具有一定的同步机制,在每个副本集合中,都会选举出一个副本作为Leader副本,Kafka在不同的场景中会采用不同的选举策略。Kafka中所有的读写请求都由选举出的Leader副本处理,其他的都作为Follower副本,Follower副本仅仅是从Leader副本中把数据拉取到本地之后,同步更新到自己的Log中。</p><p>分区副本:</p><p>​</p><p><img src="/2020/01/16/kafka-da-shu-ju-de-biao-zhun-shu-ju-yuan/D:%5CMyBlot%5Cbolt%5Csource_posts%5CKafka-%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A0%87%E5%87%86%E6%95%B0%E6%8D%AE%E6%BA%90%5C2.png" alt></p><p>​</p></li><li><p>生产者:生产者主要是生产消息,并将消息按照一定的规则推送到Topic的分区中</p></li><li><p>消费者:消费者主要是从Topic中拉取消息,并对消息进行消费。Consumer维护消费者消费者消费到Partition的哪一个位置(offset的值)这一信息。<strong>在Kafka中,多个Consumer可以组成一个Consumer Group,一个Consumer只能属于一个Consumer Group。Consumer Group保证其订阅的Topic中每一个分区只被分配给此Consumer Group中的一个消费者处理,所以如果需要实现消息的广播消费,则将消费者放在多个不同的Consumer Group中即可实现。</strong>通过向Consumer Group中动态的添加适量的Consumer,可以出发Kafka的Rebalance操作重新分配分区与消费者的对应关系,从而实现了水平扩展的能力。</p></li><li><p>ISR集合:ISR集合表示的是目前可用(alive)且消息量与Leader相差不多的副本集合,即整个副本集合的子集。ISR集合中副本所在的节点都与ZK保持着连接,此外,副本的最后一条消息的offset与Leader副本的最后一条消息的offset之间的差值不能超出指定的阈值。每一个分区的Leader副本都维护此分区的ISR集合。如上面所述,Leader副本进行了消息的写请求,Follower副本会从Leader上拉取写入的消息,第二个过程中会存在Follower副本中的消息数量少于Leader副本的状态,只要差值少于指定的阈值,那么此时的副本集合就是ISR集合。</p></li></ol><h2 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h2><h3 id="Java调用API使用Kafka"><a href="#Java调用API使用Kafka" class="headerlink" title="Java调用API使用Kafka:"></a>Java调用API使用Kafka:</h3><pre><code>        public class ProducerDemo {            public static void main(String[] args) {                //构造Kafka的配置项                Properties properties=new Properties();                //定义Kafka服务端的主机名和端口号                properties.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);                //定义客户端的ID                properties.put(&quot;client.id&quot;, &quot;DemoProducer&quot;);                //定义消息的key和value的数据类型都是字节数组                properties.put(&quot;key.serializer&quot;,&quot;org.apache.kafka.common.serialization.IntegerSerializer&quot;);                properties.put(&quot;value.serializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);                //创建生产者的核心类                KafkaProducer producer=new KafkaProducer&lt;&gt;(properties);                //指定topic的名称                String topic = &quot;demo&quot;;                //定义消息的key                int messageNo=1;                while(true){                    //定义消息的value                    String messageStr=&quot;Message_&quot;+messageNo;                    long startTime=System.currentTimeMillis();                    //异步的发送消息                    producer.send(new ProducerRecord&lt;&gt;(topic, messageNo,messageStr,new Callback() {                        //消息发送成功之后收到了Kafka服务端发来的ACK确认消息之后,就回调下面的方法                        //metadata保存着生产者发送过来的消息的元数据,如果消息的发送过程中出现了异常,则改参数的值为null                        @Override                        public void onCompletion(RecordMetadata metadata, Exception exception) {                            long elapsedTime=System.currentTimeMillis()-startTime;                            if(null!=metadata){                                System.out.println(&quot;消息发送给的分区是:&quot;+metadata.partition()+&quot;,消息的发送一共用了:&quot;+elapsedTime+&quot;ms&quot;);                            }else{                                exception.printStackTrace();                            }                        }                    }));                }            }        }        public class ConsumerDemo {            public static void main(String[] args) {                Properties properties=new Properties();                properties.put(&quot;bootstrap.servers&quot;,&quot;localhost:9092&quot;);                //指定Consumer Group的id                properties.put(&quot;group.id&quot;, &quot;BeautifulSoup&quot;);                //自动提交offset                properties.put(&quot;enable.auto.commit&quot;, &quot;true&quot;);                //自动提交offset的时间间隔                properties.put(&quot;auto.commit.interval.ms&quot;,&quot;1000&quot;);                properties.put(&quot;session.timeout.ms&quot;, &quot;30000&quot;);                properties.put(&quot;key.deserializer&quot;,&quot;org.apache.kafka.common.serialization.IntegerDeserializer&quot;);                properties.put(&quot;value.deserializer&quot;,&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);                KafkaConsumer consumer=new KafkaConsumer&lt;&gt;(properties);                //指定消费者订阅的topic                consumer.subscribe(Arrays.asList(&quot;demo&quot;,&quot;test&quot;));                try{                    while(true){                        //从服务端开始拉取消息,每次的poll都会拉取多个消息                        ConsumerRecords&lt;String, String&gt; records=consumer.poll(100);                        for (ConsumerRecord&lt;String,String&gt; consumerRecord : records) {                            System.out.println(&quot;消息记录的位置:&quot;+consumerRecord.offset()+&quot;,消息的键:&quot;+consumerRecord.key()+&quot;,消息的值:&quot;+consumerRecord.value());                        }                    }                }finally{                    //关闭consumer                    consumer.close();                }            }        }</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>HBase的rowkey设计的理解</title>
      <link href="/2020/01/16/hbase-de-rowkey-she-ji-de-li-jie/"/>
      <url>/2020/01/16/hbase-de-rowkey-she-ji-de-li-jie/</url>
      
        <content type="html"><![CDATA[<p>HBase是三维有序存储的，通过rowkey（行键），column key（column family和qualifier）和TimeStamp（时间戳）这个三个维度可以对HBase中的数据进行快速定位。</p><p>HBase中rowkey可以唯一标识一行记录，在HBase查询的时候，有以下几种方式：</p><ol><li>通过get方式，指定rowkey获取唯一一条记录</li><li>通过scan方式，设置startRow和stopRow参数进行范围匹配</li><li>全表扫描，即直接扫描整张表中所有行记录</li></ol><h2 id="rowkey长度原则"><a href="#rowkey长度原则" class="headerlink" title="rowkey长度原则"></a>rowkey长度原则</h2><p>rowkey是一个二进制码流，可以是任意字符串，最大长度 <em>64kb</em> ，实际应用中一般为10-100bytes，以 <code>byte[]</code> 形式保存，一般设计成定长。</p><p>建议越短越好，不要超过16个字节，原因如下：</p><ol><li>数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长，比如超过100字节，1000w行数据，光rowkey就要占用100*1000w=10亿个字节，将近1G数据，这样会极大影响HFile的存储效率；</li><li>MemStore将缓存部分数据到内存，如果rowkey字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率。</li><li>目前操作系统都是64位系统，内存8字节对齐，控制在16个字节，8字节的整数倍利用了操作系统的最佳特性。</li></ol><h2 id="rowkey散列原则"><a href="#rowkey散列原则" class="headerlink" title="rowkey散列原则"></a>rowkey散列原则</h2><p>如果rowkey按照时间戳的方式递增，不要将时间放在二进制码的前面，建议将rowkey的高位作为散列字段，由程序随机生成，低位放时间字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息，所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。</p><h2 id="rowkey唯一原则"><a href="#rowkey唯一原则" class="headerlink" title="rowkey唯一原则"></a>rowkey唯一原则</h2><p>必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的，因此，设计rowkey的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。</p><h2 id="什么是热点"><a href="#什么是热点" class="headerlink" title="什么是热点"></a>什么是热点</h2><p>HBase中的行是按照rowkey的字典顺序排序的，这种设计优化了scan操作，可以将相关的行以及会被一起读取的行存取在临近位置，便于scan。然而糟糕的rowkey设计是热点的源头。 热点发生在大量的client直接访问集群的一个或极少数个节点（访问可能是读，写或者其他操作）。大量访问会使热点region所在的单个机器超出自身承受能力，引起性能下降甚至region不可用，这也会影响同一个RegionServer上的其他region，由于主机无法服务其他region的请求。 设计良好的数据访问模式以使集群被充分，均衡的利用。</p><p>为了避免写热点，设计rowkey使得不同行在同一个region，但是在更多数据情况下，数据应该被写入集群的多个region，而不是一个。</p><p>下面是一些常见的避免热点的方法以及它们的优缺点：</p><h4 id="加盐"><a href="#加盐" class="headerlink" title="加盐"></a>加盐</h4><p>这里所说的加盐不是密码学中的加盐，而是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同。分配的前缀种类数量应该和你想使用数据分散到不同的region的数量一致。加盐之后的rowkey就会根据随机生成的前缀分散到各个region上，以避免热点。</p><h4 id="哈希"><a href="#哈希" class="headerlink" title="哈希"></a>哈希</h4><p>哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据</p><h4 id="反转"><a href="#反转" class="headerlink" title="反转"></a>反转</h4><p>第三种防止热点的方法时反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。</p><p>反转rowkey的例子以手机号为rowkey，可以将手机号反转后的字符串作为rowkey，这样的就避免了以手机号那样比较固定开头导致热点问题</p><h4 id="时间戳反转"><a href="#时间戳反转" class="headerlink" title="时间戳反转"></a>时间戳反转</h4><p>一个常见的数据处理问题是快速获取数据的最近版本，使用反转的时间戳作为rowkey的一部分对这个问题十分有用，可以用 <code>Long.Max_Value - timestamp</code> 追加到key的末尾，例如 <code>[key][reverse_timestamp]</code> , <code>[key]</code> 的最新值可以通过scan [key]获得[key]的第一条记录，因为HBase中rowkey是有序的，第一条记录是最后录入的数据。</p><p>比如需要保存一个用户的操作记录，按照操作时间倒序排序，在设计rowkey的时候，可以这样设计</p><p>[userId反转][Long.Max_Value - timestamp]，在查询用户的所有操作记录数据的时候，直接指定反转后的userId，startRow是[userId反转][000000000000],stopRow是[userId反转][Long.Max_Value - timestamp]</p><p>如果需要查询某段时间的操作记录，startRow是[user反转][Long.Max_Value - 起始时间]，stopRow是[userId反转][Long.Max_Value - 结束时间]</p><p>其他一些建议</p><ul><li>尽量减少行和列的大小在HBase中，value永远和它的key一起传输的。当具体的值在系统间传输时，它的rowkey，列名，时间戳也会一起传输。如果你的rowkey和列名很大，甚至可以和具体的值相比较，那么你将会遇到一些有趣的问题。HBase storefiles中的索引（有助于随机访问）最终占据了HBase分配的大量内存，因为具体的值和它的key很大。可以增加block大小使得storefiles索引再更大的时间间隔增加，或者修改表的模式以减小rowkey和列名的大小。压缩也有助于更大的索引。</li><li>列族尽可能越短越好，最好是一个字符</li><li>冗长的属性名虽然可读性好，但是更短的属性名存储在HBase中会更好</li></ul>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop之HDFS上传文件源码分析</title>
      <link href="/2020/01/16/hadoop-zhi-hdfs-shang-chuan-wen-jian-yuan-ma-fen-xi/"/>
      <url>/2020/01/16/hadoop-zhi-hdfs-shang-chuan-wen-jian-yuan-ma-fen-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>hdfs中每个block默认情况下是128M，由于每个块比较大，所以在写数据的过程中是把数据块拆分成一个个的数据包以管道的形式发送的，所以hdfs文件的写入会涉及到客户端、namenode、datanode多个模块的交互。</p><h3 id="操作代码"><a href="#操作代码" class="headerlink" title="操作代码"></a>操作代码</h3><pre><code>Configuration conf = new Configuration();  FileSystem fs = FileSystem.get(conf);  Path file = new Path(&quot;hdfs://127.0.0.1:9000/example.txt&quot;);  FSDataOutputStream outStream = fs.create(file);  out.write(&quot;java api write data&quot;.getBytes(&quot;UTF-8&quot;));   outStream.close();  </code></pre><p>通过 FileSystem.get(conf); 来构造了一个FileSystem 实例，这里对应的是DistributedFileSystem，通过调用DistributedFileSystem里面的create方法创建了一个文件，并且返回了这个文件的输出流，用于写入数据。</p><p>DistributedFileSystem的create方法有很多重载的方法，最终调用了DistributedFileSystem的下面的这个create方法</p><pre><code>@Override  public FSDataOutputStream create(final Path f, final FsPermission permission,    final EnumSet&lt;CreateFlag&gt; cflags, final int bufferSize,    final short replication, final long blockSize, final Progressable progress,    final ChecksumOpt checksumOpt) throws IOException {    statistics.incrementWriteOps(1);    Path absF = fixRelativePart(f);    return new FileSystemLinkResolver&lt;FSDataOutputStream&gt;() {      @Override      public FSDataOutputStream doCall(final Path p)throws IOException, UnresolvedLinkException {        final DFSOutputStream dfsos = dfs.create(getPathName(p), permission,                cflags, replication, blockSize, progress, bufferSize,                checksumOpt);        return dfs.createWrappedOutputStream(dfsos, statistics); }      @Override      public FSDataOutputStream next(final FileSystem fs, final Path p) throws IOException {        return fs.create(p, permission, cflags, bufferSize,replication, blockSize, progress, checksumOpt);      }        }.resolve(this, absF);  }</code></pre><p>在这里，调用了DFSClient的create方法来创建文件</p><pre><code>create(String, FsPermission, EnumSet, boolean, short, long, Progressable, int, ChecksumOpt, InetSocketAddress[])</code></pre><p>在这里create方法里，通过DFSOutputStream的静态方法newStreamForCreate构建了一个对象，并且返回了一个DFSOutputStream对象。</p><pre><code>  static DFSOutputStream newStreamForCreate(DFSClient dfsClient, String src,      FsPermission masked, EnumSet&lt;CreateFlag&gt; flag, boolean createParent,      short replication, long blockSize, Progressable progress, int buffersize,      DataChecksum checksum, String[] favoredNodes) throws IOException {    TraceScope scope = dfsClient.getPathTraceScope(&quot;newStreamForCreate&quot;, src);    try {      HdfsFileStatus stat = null;      // Retry the create if we get a RetryStartFileException up to a maximum      // number of times      boolean shouldRetry = true;      int retryCount = CREATE_RETRY_COUNT;      while (shouldRetry) {        shouldRetry = false;        try {          //通过调用namenode的create方法来创建文件          stat = dfsClient.namenode.create(src, masked, dfsClient.clientName,              new EnumSetWritable&lt;CreateFlag&gt;(flag), createParent, replication,              blockSize, SUPPORTED_CRYPTO_VERSIONS);          break;        } catch (RemoteException re) {          IOException e = re.unwrapRemoteException(              AccessControlException.class,              DSQuotaExceededException.class,              FileAlreadyExistsException.class,              FileNotFoundException.class,              ParentNotDirectoryException.class,              NSQuotaExceededException.class,              RetryStartFileException.class,              SafeModeException.class,              UnresolvedPathException.class,              SnapshotAccessControlException.class,              UnknownCryptoProtocolVersionException.class);          if (e instanceof RetryStartFileException) {            if (retryCount &gt; 0) {              shouldRetry = true;              retryCount--;            } else {              throw new IOException(&quot;Too many retries because of encryption&quot; +                  &quot; zone operations&quot;, e);            }          } else {            throw e;          }        }      }  Preconditions.checkNotNull(stat, &quot;HdfsFileStatus should not be null!&quot;);  //构造了一个DFSOutputStream对象，即刚刚创建的文件的输出流.  final DFSOutputStream out = new DFSOutputStream(dfsClient, src, stat,      flag, progress, checksum, favoredNodes); //start方法启动了DFSOutputStream的内部类DataStreamer，用于接收要写入的数据包  out.start();  return out;} finally {  scope.close();  }}</code></pre><p>通过dfsClient.namenode.create在hdfs的目录树上创建了一个文件，然后通过new DFSOutputStream创建了一个该文件的输出流实例，在DFSOutputStream构造方法中,初始化了用于数据处理的DFSOutputStream类的内部类DataStreamer，用于启动DataStreamer线程,接受客户端写入数据包的请求。</p><p>DataStreamer是一个线程，它的启动是通过DFSOutputStream的start方法来启动的</p><pre><code> /** Construct a new output stream for creating a file. */  private DFSOutputStream(DFSClient dfsClient, String src, HdfsFileStatus stat,      EnumSet&lt;CreateFlag&gt; flag, Progressable progress,      DataChecksum checksum, String[] favoredNodes) throws IOException {      this(dfsClient, src, progress, stat, checksum);      this.shouldSyncBlock = flag.contains(CreateFlag.SYNC_BLOCK);      computePacketChunkSize(dfsClient.getConf().writePacketSize, bytesPerChecksum);      streamer = new DataStreamer(stat, null);      if (favoredNodes != null &amp;&amp; favoredNodes.length != 0) {          streamer.setFavoredNodes(favoredNodes);      }}</code></pre><h3 id="namenode创建文件"><a href="#namenode创建文件" class="headerlink" title="namenode创建文件"></a>namenode创建文件</h3><p>上述dfsClient.namenode.create是调用了客户端和namenode交互的接口ClientProtocol中的create方法来创建文件，之后由ClientProtoco的实现类<br>org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB中的create方法封装了创建文件所需的信息，通过rpc的方式发送到了namenode来处理。</p><p>最终的实现方法是NameNodeRpcServer类的create方法，之后经过FSNamesystem的startFile、startFileInt，最后在方法startFileInternal中实现具体的逻辑。</p><p>1.首先检查是否是一个目录，如果是的话抛出异常.<br>2.检查是否有写的权限。<br>3.检查是否创建父目录<br>4.检查create字段，用户是否创建为文件<br>5.检查是否覆盖源文件，如果true的话，则删除原来的旧文件。</p><p>最后调用了FSDirectory的addFile方法来创建文件。</p><pre><code>iip = dir.addFile(parent.getKey(), parent.getValue(), permissions,replication, blockSize, holder, clientMachine);</code></pre><p>具体的操作就是找到该文件的父目录，然后在父目录的List类型的对象children中添加一条数据。</p><p>具体的代码如下：</p><pre><code> private BlocksMapUpdateInfo startFileInternal(FSPermissionChecker pc,  INodesInPath iip, PermissionStatus permissions, String holder,  String clientMachine, boolean create, boolean overwrite,   boolean createParent, short replication, long blockSize,   boolean isLazyPersist, CipherSuite suite, CryptoProtocolVersion version,  EncryptedKeyVersion edek, boolean logRetryEntry)  throws IOException {    assert hasWriteLock();    // Verify that the destination does not exist as a directory already.    final INode inode = iip.getLastINode();    final String src = iip.getPath();    //检查是否存在，并且是不是一个目录    if (inode != null &amp;&amp; inode.isDirectory()) {      throw new FileAlreadyExistsException(src +          &quot; already exists as a directory&quot;);    }    //检查是否有写的权限    final INodeFile myFile = INodeFile.valueOf(inode, src, true);    if (isPermissionEnabled) {      if (overwrite &amp;&amp; myFile != null) {        dir.checkPathAccess(pc, iip, FsAction.WRITE);      }      /*       * To overwrite existing file, need to check &#39;w&#39; permission        * of parent (equals to ancestor in this case)       */      dir.checkAncestorAccess(pc, iip, FsAction.WRITE);    }    //是否创建父目录    if (!createParent) {      dir.verifyParentDir(iip, src);    }    FileEncryptionInfo feInfo = null;    final EncryptionZone zone = dir.getEZForPath(iip);    if (zone != null) {      // The path is now within an EZ, but we&#39;re missing encryption parameters      if (suite == null || edek == null) {        throw new RetryStartFileException();      }      // Path is within an EZ and we have provided encryption parameters.      // Make sure that the generated EDEK matches the settings of the EZ.      final String ezKeyName = zone.getKeyName();      if (!ezKeyName.equals(edek.getEncryptionKeyName())) {        throw new RetryStartFileException();      }      feInfo = new FileEncryptionInfo(suite, version,          edek.getEncryptedKeyVersion().getMaterial(),          edek.getEncryptedKeyIv(),          ezKeyName, edek.getEncryptionKeyVersionName());    }    try {      BlocksMapUpdateInfo toRemoveBlocks = null;      if (myFile == null) {      //是否创建文件        if (!create) {          throw new FileNotFoundException(&quot;Can&#39;t overwrite non-existent &quot; +              src + &quot; for client &quot; + clientMachine);        }      } else {         //是否覆盖        if (overwrite) {          toRemoveBlocks = new BlocksMapUpdateInfo();          List&lt;INode&gt; toRemoveINodes = new ChunkedArrayList&lt;INode&gt;();          //删除旧文件          long ret = FSDirDeleteOp.delete(dir, iip, toRemoveBlocks,                                          toRemoveINodes, now());          if (ret &gt;= 0) {            iip = INodesInPath.replace(iip, iip.length() - 1, null);            FSDirDeleteOp.incrDeletedFileCount(ret);            removeLeasesAndINodes(src, toRemoveINodes, true);          }        } else {          // If lease soft limit time is expired, recover the lease          recoverLeaseInternal(RecoverLeaseOp.CREATE_FILE,              iip, src, holder, clientMachine, false);          throw new FileAlreadyExistsException(src + &quot; for client &quot; +              clientMachine + &quot; already exists&quot;);        }      }      checkFsObjectLimit();      INodeFile newNode = null;      // Always do an implicit mkdirs for parent directory tree.      Map.Entry&lt;INodesInPath, String&gt; parent = FSDirMkdirOp          .createAncestorDirectories(dir, iip, permissions);      if (parent != null) {        //具体的操作，创建文件        iip = dir.addFile(parent.getKey(), parent.getValue(), permissions,            replication, blockSize, holder, clientMachine);        newNode = iip != null ? iip.getLastINode().asFile() : null;      }      if (newNode == null) {        throw new IOException(&quot;Unable to add &quot; + src +  &quot; to namespace&quot;);      }      leaseManager.addLease(newNode.getFileUnderConstructionFeature()          .getClientName(), src);      // Set encryption attributes if necessary      if (feInfo != null) {        dir.setFileEncryptionInfo(src, feInfo);        newNode = dir.getInode(newNode.getId()).asFile();      }      setNewINodeStoragePolicy(newNode, iip, isLazyPersist);      // record file record in log, record new generation stamp      getEditLog().logOpenFile(src, newNode, overwrite, logRetryEntry);      NameNode.stateChangeLog.debug(&quot;DIR* NameSystem.startFile: added {}&quot; +          &quot; inode {} holder {}&quot;, src, newNode.getId(), holder);      return toRemoveBlocks;    } catch (IOException ie) {      NameNode.stateChangeLog.warn(&quot;DIR* NameSystem.startFile: &quot; + src + &quot; &quot; +          ie.getMessage());      throw ie;    }</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>多线程之缓存与主存一致性</title>
      <link href="/2020/01/15/duo-xian-cheng-zhi-huan-cun-yu-zhu-cun-yi-zhi-xing/"/>
      <url>/2020/01/15/duo-xian-cheng-zhi-huan-cun-yu-zhu-cun-yi-zhi-xing/</url>
      
        <content type="html"><![CDATA[<h2 id="一、总线锁定和缓存一致性"><a href="#一、总线锁定和缓存一致性" class="headerlink" title="一、总线锁定和缓存一致性"></a>一、总线锁定和缓存一致性</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>这是两个操作系统层面的概念。随着多核时代的到来，并发操作已经成了很正常的现象，操作系统必须要有一些机制和原语，以保证某些基本操作的原子性，比如处理器需要保证读一个字节或写一个字节是原子的，那么它是如何实现的呢?有两种机制：总线锁定和缓存一致性。</p><p>我们知道，CPU和物理内存之间的通信速度远慢于CPU的处理速度，所以CPU有自己的内部缓存，根据一些规则将内存中的数据读取到内部缓存中来，以加快频繁读取的速度。我们假设在一台PC上只有一个CPU和一份内部缓存，那么所有进程和线程看到的数都是缓存里的数，不会存在问题;但现在服务器通常是多 CPU，更普遍的是，每块CPU里有多个内核，而每个内核都维护了自己的缓存，那么这时候多线程并发就会存在缓存不一致性，这会导致严重问题。</p><p>以 i++为例，i的初始值是0.那么在开始每块缓存都存储了i的值0，当第一块内核做i++的时候，其缓存中的值变成了1，即使马上回写到主内存，那么在回写之后第二块内核缓存中的i值依然是0，其执行i++，回写到内存就会覆盖第一块内核的操作，使得最终的结果是1，而不是预期中的2.</p><p>那么怎么解决整个问题呢?操作系统提供了总线锁定的机制。前端总线(也叫CPU总线)是所有CPU与芯片组连接的主干道，负责CPU与外界所有部件的通信，包括高速缓存、内存、北桥，其控制总线向各个部件发送控制信号、通过地址总线发送地址信号指定其要访问的部件、通过数据总线双向传输。在CPU1要做 i++操作的时候，其在总线上发出一个LOCK#信号，其他处理器就不能操作缓存了该共享变量内存地址的缓存，也就是阻塞了其他CPU，使该处理器可以独享此共享内存。</p><p>但我们只需要对此共享变量的操作是原子就可以了，而总线锁定把CPU和内存的通信给锁住了，使得在锁定期间，其他处理器不能操作其他内存地址的数据，从而开销较大，所以后来的CPU都提供了缓存一致性机制，Intel的奔腾486之后就提供了这种优化。</p><p>缓存一致性机制整体来说，是当某块CPU对缓存中的数据进行操作了之后，就通知其他CPU放弃储存在它们内部的缓存，或者从主内存中重新读取，如下图：</p><p><img src="/2020/01/15/duo-xian-cheng-zhi-huan-cun-yu-zhu-cun-yi-zhi-xing/20160613151013467.png" alt></p><p>这里以在Intel系列中广泛使用的MESI协议详细阐述下其原理。</p><h3 id="MESI协议"><a href="#MESI协议" class="headerlink" title="MESI协议"></a>MESI协议</h3><p>MESI 协议是以缓存行(缓存的基本数据单位，在Intel的CPU上一般是64字节)的几个状态来命名的(全名是Modified、Exclusive、 Share or Invalid)。该协议要求在每个缓存行上维护两个状态位，使得每个数据单位可能处于M、E、S和I这四种状态之一，各种状态含义如下：</p><p>M：被修改的。处于这一状态的数据，只在本CPU中有缓存数据，而其他CPU中没有。同时其状态相对于内存中的值来说，是已经被修改的，且没有更新到内存中。</p><p>E：独占的。处于这一状态的数据，只有在本CPU中有缓存，且其数据没有修改，即与内存中一致。</p><p>S：共享的。处于这一状态的数据在多个CPU中都有缓存，且与内存一致。</p><p>I：无效的。本CPU中的这份缓存已经无效。</p><p>这里首先介绍该协议约定的缓存上对应的监听：</p><p>一个处于M状态的缓存行，必须时刻监听所有试图读取该缓存行对应的主存地址的操作，如果监听到，则必须在此操作执行前把其缓存行中的数据写回CPU。</p><p>一个处于S状态的缓存行，必须时刻监听使该缓存行无效或者独享该缓存行的请求，如果监听到，则必须把其缓存行状态设置为I。</p><p>一个处于E状态的缓存行，必须时刻监听其他试图读取该缓存行对应的主存地址的操作，如果监听到，则必须把其缓存行状态设置为S。</p><p>当CPU需要读取数据时，如果其缓存行的状态是I的，则需要从内存中读取，并把自己状态变成S，如果不是I，则可以直接读取缓存中的值，但在此之前，必须要等待其他CPU的监听结果，如其他CPU也有该数据的缓存且状态是M，则需要等待其把缓存更新到内存之后，再读取。</p><p>当CPU需要写数据时，只有在其缓存行是M或者E的时候才能执行，否则需要发出特殊的RFO指令(Read Or Ownership，这是一种总线事务)，通知其他CPU置缓存无效(I)，这种情况下会性能开销是相对较大的。在写入完成后，修改其缓存状态为M。</p><p>所以如果一个变量在某段时间只被一个线程频繁地修改，则使用其内部缓存就完全可以办到，不涉及到总线事务，如果缓存一会被这个CPU独占、一会被那个CPU 独占，这时才会不断产生RFO指令影响到并发性能。这里说的缓存频繁被独占并不是指线程越多越容易触发，而是这里的CPU协调机制，这有点类似于有时多线程并不一定提高效率，原因是线程挂起、调度的开销比执行任务的开销还要大，这里的多CPU也是一样，如果在CPU间调度不合理，也会形成RFO指令的开销比任务开销还要大。当然，这不是编程者需要考虑的事，操作系统会有相应的内存地址的相关判断，这不在本文的讨论范围之内。</p><p>并非所有情况都会使用缓存一致性的，如被操作的数据不能被缓存在CPU内部或操作数据跨越多个缓存行(状态无法标识)，则处理器会调用总线锁定;另外当CPU不支持缓存锁定时，自然也只能用总线锁定了，比如说奔腾486以及更老的CPU。</p><h2 id="二、CAS-Compare-and-Swap"><a href="#二、CAS-Compare-and-Swap" class="headerlink" title="二、CAS(Compare and Swap)"></a>二、CAS(Compare and Swap)</h2><p>有了上一章的总线锁定和缓存一致性的介绍，对CAS就比较好理解了，这不是java特有的，而是操作系统需要保证的。CAS指令在Intel CPU上称为CMPXCHG指令，它的作用是将指定内存地址的内容与所给的某个值相比，如果相等，则将其内容替换为指令中提供的新值，如果不相等，则更新失败。这一比较并交换的操作是原子的，不可以被中断，而其保证原子性的原理就是上一节提到的“总线锁定和缓存一致性”。初一看，CAS也包含了读取、比较 (这也是种操作)和写入这三个操作，和之前的i++并没有太大区别，是的，的确在操作上没有区别，但CAS是通过硬件命令保证了原子性，而i++没有，且硬件级别的原子性比i++这样高级语言的软件级别的运行速度要快地多。虽然CAS也包含了多个操作，但其的运算是固定的(就是个比较)，这样的锁定性能开销很小。</p><p>随着互联网行业的兴起和硬件多CPU/多内核的进步，高并发已经成为越来越普遍的现象，CAS已经被越来越广泛地使用，在Java领域也是如此。JDK1.4是2002年2月发布的，当时的硬件设备远没有如今这么先进，多CPU和多核还没有普及，所以在JDK1.5之前的synchronized是使用挂起线程、等待调度的方式来实现线程同步，开销较大;而随着硬件的不断升级，在2004年9月发布的JDK5中引入了CAS机制——比较并交换——来彻底解决此问题，在一般情况下不再需要挂起(参考后文对锁级别的描述，只有进入重量级锁的时候才会使用挂起)，而是多次尝试，其利用底层CPU命令实现的乐观锁机制。从内存领域来说这是乐观锁，因为它在对共享变量更新之前会先比较当前值是否与更新前的值一致，如果是，则更新，如果不是，则无限循环执行(称为自旋)，直到当前值与更新前的值一致为止，才执行更新。</p><p>以concurrent中的AtomicInteger的代码为例，其的getAndIncrement()方法(获得并且自增，即i++)源代码如下：</p><pre><code>   /**      * Atomically increments by one the current value.      *      * @return the previous value      */     public final int getAndIncrement() {         for (;;) {             int current = get();             int next = current + 1 ;             if (compareAndSet(current, next))                 return current;         }     } /**  * Atomically sets the value to the given updated value  * if the current value {@code ==} the expected value.  *  * @param expect the expected value  * @param update the new value  * @return true if successful. False return indicates that  * the actual value was not equal to the expected value.  */ public final boolean compareAndSet( int expect, int update) {     return unsafe.compareAndSwapInt( this , valueOffset, expect, update); }</code></pre><p>其调用了compareAndSet(int expect,int update)方法，其中expect是期望值，即操作前的原始值，而update是操作后的值，以i=2为例，则这里的 expect=2，update=3，它调用了sun.misc.Unsafe的compareAndSwapInt方法来执行，此方法代码如下：</p><pre><code>/***    * Compares the value of the integer field at the specified offset    * in the supplied object with the given expected value, and updates    * it if they match.  The operation of this method should be atomic,    * thus providing an uninterruptible way of updating an integer field.    *    * @param obj the object containing the field to modify.    * @param offset the offset of the integer field within &lt;code&gt;obj&lt;/code&gt;.    * @param expect the expected value of the field.    * @param update the new value of the field if it equals &lt;code&gt;expect&lt;/code&gt;.    * @return true if the field was changed.    */   public native boolean compareAndSwapInt(Object obj, long offset,                                           int expect, int update);</code></pre><p>这是一个本地方法，即利用CAS保证其原子性，同时如果失败了则通过循环不断地进行运算直到成功为止，这是和JDK5以前最大的区别，失败的线程不再需要被挂起、重新调度，而是可以无障碍地再度执行，这又极大减少了挂起调度的开销(当然如果CAS长时间不成功，也会造成耗费CPU，这取决于具体应用场景)。</p><p>CAS策略有如下需要注意的事项：</p><p>在线程抢占资源特别频繁的时候(相对于CPU执行效率而言)，会造成长时间的自旋，耗费CPU性能。</p><p>有ABA问题(即在更新前的值是A，但在操作过程中被其他线程更新为B，又更新为 A)，这时当前线程认为是可以执行的，其实是发生了不一致现象，如果这种不一致对程序有影响(真正有这种影响的场景很少，除非是在变量操作过程中以此变量为标识位做一些其他的事，比如初始化配置)，则需要使用AtomicStampedReference(除了对更新前的原值进行比较，也需要用更新前的 stamp标志位来进行比较)。</p><p>只能对一个变量进行原子性操作。如果需要把多个变量作为一个整体来做原子性操作，则应该使用AtomicReference来把这些变量放在一个对象里，针对这个对象做原子性操作。</p><p>CAS在JDK5中被J.U.C包广泛使用，在JDK6中被应用到synchronized的 JVM实现中，因此在JDK5中J.U.C的效率是比synchronized高不少的，而到了JDK6，两者效率相差无几，而synchronized 使用更简单、更不容易出错，所以其是专家组推荐的首选，除非需要用到J.U.C的特殊功能(如阻塞一段时间后放弃，而不是继续等待)。</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Netty原理</title>
      <link href="/2020/01/14/netty-yuan-li/"/>
      <url>/2020/01/14/netty-yuan-li/</url>
      
        <content type="html"><![CDATA[<p>对于高性能的 RPC 框架，Netty 作为异步通信框架，几乎成为必备品。例如，Dubbo 框架中通信组件，还有 RocketMQ 中生产者和消费者的通信，都使用了 Netty。今天，我们来看看 Netty 的基本架构和原理。</p><p>Netty 的特点与 NIO</p><p>Netty 是一个异步的、基于事件驱动的网络应用框架，它可以用来开发高性能服务端和客户端。</p><p>以前编写网络调用程序的时候，我们都会在客户端创建一个 Socket，通过这个 Socket 连接到服务端。</p><p>服务端根据这个 Socket 创建一个 Thread，用来发出请求。客户端在发起调用以后，需要等待服务端处理完成，才能继续后面的操作。这样线程会出现等待的状态。</p><p>如果客户端请求数越多，服务端创建的处理线程也会越多，JVM 如此多的线程并不是一件容易的事。</p><p><img src="/2020/01/14/netty-yuan-li/1.png" alt="使用阻赛 I/O 处理多个连接"></p><p>为了解决上述的问题，推出了 NIO 的概念，也就是（Non-blocking I/O）。其中，Selector 机制就是 NIO 的核心。</p><p>当每次客户端请求时，会创建一个 Socket Channel，并将其注册到 Selector 上（多路复用器）。</p><p>然后，Selector 关注服务端 IO 读写事件，此时客户端并不用等待 IO 事件完成，可以继续做接下来的工作。</p><p>一旦，服务端完成了 IO 读写操作，Selector 会接到通知，同时告诉客户端 IO 操作已经完成。</p><p>接到通知的客户端，就可以通过 SocketChannel 获取需要的数据了。</p><p><img src="/2020/01/14/netty-yuan-li/2.png" alt="NIO 机制与 Selector"></p><p>上面描述的过程有点异步的意思，不过，Selector 实现的并不是真正意义上的异步操作。</p><p>因为 Selector 需要通过线程阻塞的方式监听 IO 事件变更，只是这种方式没有让客户端等待，是 Selector 在等待 IO 返回，并且通知客户端去获取数据。真正“异步 IO”（AIO）这里不展开介绍，有兴趣可以自行查找。</p><p>说好了 NIO 再来谈谈 Netty，Netty 作为 NIO 的实现，它适用于服务器/客户端通讯的场景，以及针对于 TCP 协议下的高并发应用。</p><p>对于开发者来说，它具有以下特点：</p><ul><li>对 NIO 进行封装，开发者不需要关注 NIO 的底层原理，只需要调用 Netty 组件就能够完成工作。</li><li>对网络调用透明，从 Socket 建立 TCP 连接到网络异常的处理都做了包装。</li><li>对数据处理灵活， Netty 支持多种序列化框架，通过“ChannelHandler”机制，可以自定义“编/解码器”。</li><li>对性能调优友好，Netty 提供了线程池模式以及 Buffer 的重用机制（对象池化），不需要构建复杂的多线程模型和操作队列。</li></ul><h2 id="组件作用与之间关系"><a href="#组件作用与之间关系" class="headerlink" title="组件作用与之间关系"></a>组件作用与之间关系</h2><p>开篇讲到了，为了满足高并发下网络请求，引入了 NIO 的概念。Netty 是针对 NIO 的实现，在 NIO 封装，网络调用，数据处理以及性能优化等方面都有不俗的表现。</p><p>学习架构最容易的方式就是从实例入手，从客户端访问服务端的代码来看看 Netty 是如何运作的。再一次介绍代码中调用的组件以及组件的工作原理。</p><p>假设有一个客户端去调用一个服务端，假设服务端叫做 EchoServer，客户端叫做 EchoClient，用 Netty 架构实现代码如下。</p><p><strong>服务端代码</strong></p><p>构建服务器端，假设服务器接受客户端传来的信息，然后在控制台打印。首先，生成 EchoServer，在构造函数中传入需要监听的端口号。</p><p><img src="/2020/01/14/netty-yuan-li/3.webp" alt="构造函数中传入需要监听的端口号"></p><p>接下来就是服务的启动方法：</p><p><img src="/2020/01/14/netty-yuan-li/4.jpg" alt="启动 NettyServer 的 Start 方法"></p><p>Server 的启动方法涉及到了一些组件的调用，例如 EventLoopGroup，Channel。这些会在后面详细讲解。</p><p>这里有个大致的印象就好：</p><ul><li>创建 EventLoopGroup。</li><li>创建 ServerBootstrap。</li><li>指定所使用的 NIO 传输 Channel。</li><li>使用指定的端口设置套接字地址。</li><li>添加一个 ServerHandler 到 Channel 的 ChannelPipeline。</li><li>异步地绑定服务器；调用 sync() 方法阻塞等待直到绑定完成。</li><li>获取 Channel 的 CloseFuture，并且阻塞当前线程直到它完成。</li><li>关闭 EventLoopGroup，释放所有的资源。</li></ul><p>NettyServer 启动以后会监听某个端口的请求，当接受到了请求就需要处理了。在 Netty 中客户端请求服务端，被称为“入站”操作。</p><p>可以通过 ChannelInboundHandlerAdapter 实现，具体内容如下：</p><p><img src="/2020/01/14/netty-yuan-li/5.jpg" alt="处理来自客户端的请求"></p><p>从上面的代码可以看出，服务端处理的代码包含了三个方法。这三个方法都是根据事件触发的。</p><p>他们分别是：</p><ul><li>当接收到消息时的操作，channelRead。</li><li>消息读取完成时的方法，channelReadComplete。</li><li>出现异常时的方法，exceptionCaught。</li></ul><p><strong>客户端代码</strong></p><p>客户端和服务端的代码基本相似，在初始化时需要输入服务端的 IP 和 Port。</p><p><img src="/2020/01/14/netty-yuan-li/6.webp" alt="Netty原理/3.webp"></p><p>同样在客户端启动函数中包括以下内容：</p><p><img src="/2020/01/14/netty-yuan-li/7.webp" alt="Netty原理/3.webp"></p><p>客户端启动程序的顺序：</p><ul><li>创建 Bootstrap。</li><li>指定 EventLoopGroup 用来监听事件。</li><li>定义 Channel 的传输模式为 NIO（Non-BlockingInputOutput）。</li><li>设置服务器的 InetSocketAddress。</li><li>在创建 Channel 时，向 ChannelPipeline 中添加一个 EchoClientHandler 实例。</li><li>连接到远程节点，阻塞等待直到连接完成。</li><li>阻塞，直到 Channel 关闭。</li><li>关闭线程池并且释放所有的资源。</li></ul><p>客户端在完成以上操作以后，会与服务端建立连接从而传输数据。同样在接受到 Channel 中触发的事件时，客户端会触发对应事件的操作。</p><p><img src="/2020/01/14/netty-yuan-li/8.jpg" alt="Netty原理/3.webp"></p><p>例如 Channel 激活，客户端接受到服务端的消息，或者发生异常的捕获。</p><p>从代码结构上看还是比较简单的。服务端和客户端分别初始化创建监听和连接。然后分别定义各自的 Handler 处理对方的请求。</p><p><img src="/2020/01/14/netty-yuan-li/9.png" alt="Netty原理/3.webp"></p><p><em>服务端/客户端初始化和事件处理</em></p><p>Netty 核心组件</p><p>通过上面的简单例子，发现有些 Netty 组件在服务初始化以及通讯时被用到，下面就来介绍一下这些组件的用途和关系。</p><h3 id="Channel"><a href="#Channel" class="headerlink" title="Channel"></a>Channel</h3><p>通过上面例子可以看出，当客户端和服务端连接的时候会建立一个 Channel。</p><p>这个 Channel 我们可以理解为 Socket 连接，它负责基本的 IO 操作，例如：bind（），connect（），read（），write（） 等等。</p><p>简单的说，Channel 就是代表连接，实体之间的连接，程序之间的连接，文件之间的连接，设备之间的连接。同时它也是数据入站和出站的载体。</p><h3 id="EventLoop-和-EventLoopGroup"><a href="#EventLoop-和-EventLoopGroup" class="headerlink" title="EventLoop 和 EventLoopGroup"></a>EventLoop 和 EventLoopGroup</h3><p>既然有了 Channel 连接服务，让信息之间可以流动。如果服务发出的消息称作“出站”消息，服务接受的消息称作“入站”消息。那么消息的“出站”/“入站”就会产生事件（Event）。</p><p>例如：连接已激活；数据读取；用户事件；异常事件；打开链接；关闭链接等等。</p><p>顺着这个思路往下想，有了数据，数据的流动产生事件，那么就有一个机制去监控和协调事件。</p><p>这个机制（组件）就是 EventLoop。在 Netty 中每个 Channel 都会被分配到一个 EventLoop。一个 EventLoop 可以服务于多个 Channel。</p><p>每个 EventLoop 会占用一个 Thread，同时这个 Thread 会处理 EventLoop 上面发生的所有 IO 操作和事件（Netty 4.0）。</p><p><img src="/2020/01/14/netty-yuan-li/10.png" alt="EventLoop 与 Channel 关系"></p><p>理解了 EventLoop，再来说 EventLoopGroup 就容易了，EventLoopGroup 是用来生成 EventLoop 的，还记得例子代码中第一行就 new 了 EventLoopGroup 对象。</p><p>一个 EventLoopGroup 中包含了多个 EventLoop 对象。</p><p><img src="/2020/01/14/netty-yuan-li/11.webp" alt="创建 EventLoopGroup"></p><p>EventLoopGroup 要做的就是创建一个新的 Channel，并且给它分配一个 EventLoop。</p><p><img src="/2020/01/14/netty-yuan-li/12.webp" alt="EventLoopGroup，EventLoop 和 Channel 的关系"></p><p>在异步传输的情况下，一个 EventLoop 是可以处理多个 Channel 中产生的事件的，它主要的工作就是事件的发现以及通知。</p><p>相对于以前一个 Channel 就占用一个 Thread 的情况。Netty 的方式就要合理多了。</p><p>客户端发送消息到服务端，EventLoop 发现以后会告诉服务端：“你去获取消息”，同时客户端进行其他的工作。</p><p>当 EventLoop 检测到服务端返回的消息，也会通知客户端：“消息返回了，你去取吧“。客户端再去获取消息。整个过程 EventLoop 就是监视器+传声筒。</p><h3 id="ChannelHandler，ChannelPipeline-和ChannelHandlerContext"><a href="#ChannelHandler，ChannelPipeline-和ChannelHandlerContext" class="headerlink" title="ChannelHandler，ChannelPipeline 和ChannelHandlerContext"></a>ChannelHandler，ChannelPipeline 和ChannelHandlerContext</h3><p>如果说 EventLoop 是事件的通知者，那么 ChannelHandler 就是事件的处理者。</p><p>在 ChannelHandler 中可以添加一些业务代码，例如数据转换，逻辑运算等等。</p><p>正如上面例子中展示的，Server 和 Client 分别都有一个 ChannelHandler 来处理，读取信息，网络可用，网络异常之类的信息。</p><p>并且，针对出站和入站的事件，有不同的 ChannelHandler，分别是：</p><ul><li><strong>ChannelInBoundHandler（入站事件处理器）</strong></li><li><strong>ChannelOutBoundHandler（出站事件处理器）</strong></li></ul><p><img src="/2020/01/14/netty-yuan-li/13.webp" alt="Netty原理/3.webp"></p><p>假设每次请求都会触发事件，而由 ChannelHandler 来处理这些事件，这个事件的处理顺序是由 ChannelPipeline 来决定的。</p><p><img src="/2020/01/14/netty-yuan-li/14.png" alt="ChannelHanlder 处理，出站/入站的事件"></p><p>ChannelPipeline 为 ChannelHandler 链提供了容器。到 Channel 被创建的时候，会被 Netty 框架自动分配到 ChannelPipeline 上。</p><p>ChannelPipeline 保证 ChannelHandler 按照一定顺序处理事件，当事件触发以后，会将数据通过 ChannelPipeline 按照一定的顺序通过 ChannelHandler。</p><p>说白了，ChannelPipeline 是负责“排队”的。这里的“排队”是处理事件的顺序。</p><p>同时，ChannelPipeline 也可以添加或者删除 ChannelHandler，管理整个队列。</p><p><img src="/2020/01/14/netty-yuan-li/15.webp" alt></p><p>如上图，ChannelPipeline 使 ChannelHandler 按照先后顺序排列，信息按照箭头所示方向流动并且被 ChannelHandler 处理。</p><p>说完了 ChannelPipeline 和 ChannelHandler，前者管理后者的排列顺序。那么它们之间的关联就由 ChannelHandlerContext 来表示了。</p><p>每当有 ChannelHandler 添加到 ChannelPipeline 时，同时会创建 ChannelHandlerContext 。</p><p>ChannelHandlerContext 的主要功能是管理 ChannelHandler 和 ChannelPipeline 的交互。</p><p>不知道大家注意到没有，开始的例子中 ChannelHandler 中处理事件函数，传入的参数就是 ChannelHandlerContext。</p><p><img src="/2020/01/14/netty-yuan-li/16.jpg" alt></p><p>ChannelHandlerContext 参数贯穿 ChannelPipeline，将信息传递给每个 ChannelHandler，是个合格的“通讯员”。</p><p><img src="/2020/01/14/netty-yuan-li/17.png" alt="ChannelHandlerContext 负责传递消息"></p><p>把上面提到的几个核心组件归纳一下，用下图表示方便记忆他们之间的关系。</p><p><img src="/2020/01/14/netty-yuan-li/18.png" alt="Netty 核心组件关系图"></p><p>Netty 的数据容器</p><p>前面介绍了 Netty 的几个核心组件，服务器在数据传输的时候，产生事件，并且对事件进行监控和处理。</p><p>接下来看看数据是如何存放以及是如何读写的。Netty 将 ByteBuf 作为数据容器，来存放数据。</p><h2 id="ByteBuf-工作原理"><a href="#ByteBuf-工作原理" class="headerlink" title="ByteBuf 工作原理"></a><strong>ByteBuf 工作原理</strong></h2><p>从结构上来说，ByteBuf 由一串字节数组构成。数组中每个字节用来存放信息。</p><p>ByteBuf 提供了两个索引，一个用于读取数据，一个用于写入数据。这两个索引通过在字节数组中移动，来定位需要读或者写信息的位置。</p><p>当从 ByteBuf 读取时，它的 readerIndex（读索引）将会根据读取的字节数递增。</p><p>同样，当写 ByteBuf 时，它的 writerIndex 也会根据写入的字节数进行递增。</p><p><img src="/2020/01/14/netty-yuan-li/19.webp" alt="ByteBuf 读写索引图例"></p><p>需要注意的是极限的情况是 readerIndex 刚好读到了 writerIndex 写入的地方。</p><p>如果 readerIndex 超过了 writerIndex 的时候，Netty 会抛出 IndexOutOf-BoundsException 异常。</p><h3 id="ByteBuf-使用模式"><a href="#ByteBuf-使用模式" class="headerlink" title="ByteBuf 使用模式"></a>ByteBuf 使用模式</h3><p>谈了 ByteBuf 的工作原理以后，再来看看它的使用模式。</p><p>根据存放缓冲区的不同分为三类：</p><ul><li><p><strong>堆缓冲区，</strong>ByteBuf 将数据存储在 JVM 的堆中，通过数组实现，可以做到快速分配。</p><p>由于在堆上被 JVM 管理，在不被使用时可以快速释放。可以通过 ByteBuf.array() 来获取 byte[] 数据。</p></li><li><p><strong>直接缓冲区，</strong>在 JVM 的堆之外直接分配内存，用来存储数据。其不占用堆空间，使用时需要考虑内存容量。</p><p>它在使用 Socket 传递时性能较好，因为间接从缓冲区发送数据，在发送之前 JVM 会先将数据复制到直接缓冲区再进行发送。</p><p>由于，直接缓冲区的数据分配在堆之外，通过 JVM 进行垃圾回收，并且分配时也需要做复制的操作，因此使用成本较高。</p></li><li><p><strong>复合缓冲区，</strong>顾名思义就是将上述两类缓冲区聚合在一起。Netty 提供了一个 CompsiteByteBuf，可以将堆缓冲区和直接缓冲区的数据放在一起，让使用更加方便。</p></li></ul><h3 id="ByteBuf-的分配"><a href="#ByteBuf-的分配" class="headerlink" title="ByteBuf 的分配"></a>ByteBuf 的分配</h3><p>聊完了结构和使用模式，再来看看 ByteBuf 是如何分配缓冲区的数据的。</p><p>Netty 提供了两种 ByteBufAllocator 的实现，他们分别是：</p><ul><li><strong>PooledByteBufAllocator，</strong>实现了 ByteBuf 的对象的池化，提高性能减少内存碎片。</li><li><strong>Unpooled-ByteBufAllocator，</strong>没有实现对象的池化，每次会生成新的对象实例。</li></ul><p>对象池化的技术和线程池，比较相似，主要目的是提高内存的使用率。池化的简单实现思路，是在 JVM 堆内存上构建一层内存池，通过 allocate 方法获取内存池中的空间，通过 release 方法将空间归还给内存池。</p><p>对象的生成和销毁，会大量地调用 allocate 和 release 方法，因此内存池面临碎片空间回收的问题，在频繁申请和释放空间后，内存池需要保证连续的内存空间，用于对象的分配。</p><p>基于这个需求，有两种算法用于优化这一块的内存分配：伙伴系统和 slab 系统。</p><p>伙伴系统，用完全二叉树管理内存区域，左右节点互为伙伴，每个节点代表一个内存块。内存分配将大块内存不断二分，直到找到满足所需的最小内存分片。</p><p>内存释放会判断释放内存分片的伙伴（左右节点）是否空闲，如果空闲则将左右节点合成更大块内存。</p><p>slab 系统，主要解决内存碎片问题，将大块内存按照一定内存大小进行等分，形成相等大小的内存片构成的内存集。</p><p>按照内存申请空间的大小，申请尽量小块内存或者其整数倍的内存，释放内存时，也是将内存分片归还给内存集。</p><p>Netty 内存池管理以 Allocate 对象的形式出现。一个 Allocate 对象由多个 Arena 组成，每个 Arena 能执行内存块的分配和回收。</p><p>Arena 内有三类内存块管理单元：</p><ul><li><strong>TinySubPage</strong></li><li><strong>SmallSubPage</strong></li><li><strong>ChunkList</strong></li></ul><p>Tiny 和 Small 符合 Slab 系统的管理策略，ChunkList 符合伙伴系统的管理策略。</p><p>当用户申请内存介于 tinySize 和 smallSize 之间时，从 tinySubPage 中获取内存块。</p><p>申请内存介于 smallSize 和 pageSize 之间时，从 smallSubPage 中获取内存块；介于 pageSize 和 chunkSize 之间时，从 ChunkList 中获取内存；大于 ChunkSize（不知道分配内存的大小）的内存块不通过池化分配。</p><h2 id="Netty-的-Bootstrap"><a href="#Netty-的-Bootstrap" class="headerlink" title="Netty 的 Bootstrap"></a>Netty 的 Bootstrap</h2><p>说完了 Netty 的核心组件以及数据存储。再回到最开始的例子程序，在程序最开始的时候会 new 一个 Bootstrap 对象，后面所有的配置都是基于这个对象展开的。</p><p><img src="/2020/01/14/netty-yuan-li/20.jpg" alt="生成 Bootstrap 对象"></p><p>Bootstrap 的作用就是将 Netty 核心组件配置到程序中，并且让他们运行起来。</p><p>从 Bootstrap 的继承结构来看，分为两类分别是 Bootstrap 和 ServerBootstrap，一个对应客户端的引导，另一个对应服务端的引导。</p><p><img src="/2020/01/14/netty-yuan-li/21.webp" alt="支持客户端和服务端的程序引导"></p><p>客户端引导 Bootstrap，主要有两个方法 bind（） 和 connect（）。Bootstrap 通过 bind（） 方法创建一个 Channel。</p><p>在 bind（） 之后，通过调用 connect（） 方法来创建 Channel 连接。</p><p><img src="/2020/01/14/netty-yuan-li/22.png" alt="Bootstrap 通过 bind 和 connect 方法创建连接"></p><p>服务端引导 ServerBootstrap，与客户端不同的是在 Bind（） 方法之后会创建一个 ServerChannel，它不仅会创建新的 Channel 还会管理已经存在的 Channel。</p><p><img src="/2020/01/14/netty-yuan-li/23.png" alt></p><p>ServerBootstrap 通过 bind 方法创建/管理连接</p><p>通过上面的描述，服务端和客户端的引导存在两个区别：</p><ul><li><p>ServerBootstrap（服务端引导）绑定一个端口，用来监听客户端的连接请求。而 Bootstrap（客户端引导）只要知道服务端 IP 和 Port 建立连接就可以了。</p></li><li><p>Bootstrap（客户端引导）需要一个 EventLoopGroup，但是 ServerBootstrap（服务端引导）则需要两个 EventLoopGroup。</p><p>因为服务器需要两组不同的 Channel。第一组 ServerChannel 自身监听本地端口的套接字。第二组用来监听客户端请求的套接字。</p></li></ul><p><img src="/2020/01/14/netty-yuan-li/24.png" alt="ServerBootstrap 有两组 EventLoopGroup"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我们从 NIO 入手，谈到了 Selector 的核心机制。然后通过介绍 Netty 客户端和服务端源代码运行流程，让大家对 Netty 编写代码有基本的认识。</p><p>在 Netty 的核心组件中，Channel 提供 Socket 的连接通道，EventLoop 会对应 Channel 监听其产生的事件，并且通知执行者。EventloopGroup 的容器，负责生成和管理 EventLoop。</p><p>ChannelPipeline 作为 ChannelHandler 的容器会绑定到 Channel 上，然后由 ChannelHandler 提供具体事件处理。另外，ChannelHandlerContext 为 ChannelHandler 和 ChannelPipeline 提供信息共享。</p><p>ByteBuf 作为 Netty 的数据容器，通过字节数组的方式存储数据，并且通过读索引和写索引来引导读写操作。</p><p>上述的核心组件都是通过 Bootstrap 来配置并且引导启动的，Bootstrap 启动方式虽然一致，但是针对客户端和服务端有些许的区别。</p>]]></content>
      
      
      <categories>
          
          <category> tool </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>java的Object对象</title>
      <link href="/2020/01/13/java-de-object-dui-xiang/"/>
      <url>/2020/01/13/java-de-object-dui-xiang/</url>
      
        <content type="html"><![CDATA[<h2 id="一-Object对象简介"><a href="#一-Object对象简介" class="headerlink" title="一.Object对象简介"></a>一.Object对象简介</h2><p>我们学Java的知道，Java是一门面向对象的语言。无论在Java中出现什么，都可以认为它是对象(<strong>除了</strong>八大基本数据类型。当然了，八大基本数据类型也能<strong>装箱</strong>成为对象)：</p><ul><li>而Object就是这些对象的最高级别的，所有的Java对象都<strong>隐式</strong>地继承了Object对象(不用显示写<code>extends</code>继承)</li><li>所有的Java对象都<strong>拥有Object默认的方法</strong>。</li></ul><p>那么我们看看Object有什么方法：</p><p><img src="/2020/01/13/java-de-object-dui-xiang/1578923575140.png" alt></p><p>其实就可以归纳成几个：</p><ul><li><code>registerNatives()</code>【底层实现、不研究】</li><li><code>hashCode()</code></li><li><code>equals(Object obj)</code></li><li><code>clone()</code></li><li><code>toString()</code></li><li><code>notify()</code></li><li><code>notifyAll()</code></li><li><code>wait(long timeout)</code>【还有重载了两个】</li><li><code>finalize()</code></li></ul><p>Object一共有<strong>11</strong>个方法，其中一个为底层的实现<code>registerNatives()</code>，其中两个<code>wait()</code>和<code>wait(long timeout, int nanos)</code>重载方法。</p><ul><li>所以我们真正需要看的就是<strong>8个</strong>方法</li></ul><p>还有<strong>一个属性</strong>：</p><pre><code> public final native Class&lt;?&gt; getClass();</code></pre><p><img src="/2020/01/13/java-de-object-dui-xiang/1.png" alt></p><h2 id="二、equals和hashCode方法"><a href="#二、equals和hashCode方法" class="headerlink" title="二、equals和hashCode方法"></a>二、equals和hashCode方法</h2><p>equals和hashCode方法可以说是面试的重点题了，配合着String可以说在面试题中<strong>哪都有它们的存在</strong>。</p><p>首先，我们来看看equals和hashCode在Object中<strong>原生</strong>的实现吧：</p><p>hashCode：</p><pre><code>public native int hashCode();</code></pre><p>equals：</p><pre><code>    public boolean equals(Object obj) {        return (this == obj);    }</code></pre><p>看上去都非常简单：</p><ul><li><code>hashCode()</code>由native方法底层实现了。</li><li><code>equals()</code>就直接<code>==</code>判断是否相等了。</li></ul><p>想要更加清晰它们究竟是做什么的，我们来读读它的注释：</p><p><img src="/2020/01/13/java-de-object-dui-xiang/2.png" alt></p><p><img src="/2020/01/13/java-de-object-dui-xiang/3.png" alt></p><p>根据注释我们可以<strong>总结以下的要点</strong>：</p><ul><li><p>重写<code>equals()</code>方法，就必须重写<code>hashCode()</code>的方法</p></li><li><p><code>equals()</code>方法默认是比较对象的地址，使用的是<code>==</code>等值运算符</p></li><li><p><code>hashCode()</code>方法对底层是散列表的对象有提升性能的功能</p></li><li><p>同一个对象(如果该对象没有被修改)：那么重复调用<code>hashCode()</code>那么返回的int是相同的！</p></li><li><p><code>hashCode()</code>方法默认是由对象的地址转换而来的</p></li><li><p>equals()方法还有5个默认的原则：</p><ul><li>自反性—&gt;调用<code>equals()</code>返回的是true，无论这两个对象谁调用<code>equals()</code>都好，返回的都是true</li><li>一致性—&gt;只要对象没有被修改，那么多次调用还是返回对应的结果！</li><li>传递性—&gt;<code>x.equals(y)</code>和<code>y.equals(z)</code>都返回true，那么可以得出：<code>x.equals(z)</code>返回true</li><li>对称性—&gt;<code>x.equals(y)</code>和<code>y.equals(x)</code>结果应该是相等的。</li><li>传入的参数为null，返回的是false</li></ul></li></ul><p>为啥说<code>hashCode()</code>以散列表为底层带来性能的提升是很容易理解的。我们再来<strong>回顾</strong>一下HashMap的插入：</p><p><img src="/2020/01/13/java-de-object-dui-xiang/4.png" alt></p><p>如果hash值都不相等，那么可以直接判断该key是不相等的了！</p><h3 id="2-1equals和hashCode方法重写"><a href="#2-1equals和hashCode方法重写" class="headerlink" title="2.1equals和hashCode方法重写"></a>2.1equals和hashCode方法重写</h3><p><code>equals()</code>方法默认是比较对象的地址，使用的是<code>==</code>等值运算符。但是按我们正常开发来说，<strong>比较的是对象地址是没有意义的</strong>。</p><ul><li>一般地，如果我们有两个Address对象，只要这两个对象的<strong>省号、城市号、街道号相等</strong>，我们就认为这两个对象相等了！</li></ul><p><img src="/2020/01/13/java-de-object-dui-xiang/5.png" alt></p><h3 id="2-2String实现的equals和hashCode方法"><a href="#2-2String实现的equals和hashCode方法" class="headerlink" title="2.2String实现的equals和hashCode方法"></a>2.2String实现的equals和hashCode方法</h3><p>我们在初学的时候可能就听过了：String<strong>已经</strong>实现了equals和hashCode方法了。</p><ul><li>这也就是为什么，我们可以<strong>直接</strong>使用String.equals()来<strong>判断两个字符串</strong>是否相等！</li></ul><p>下面我们就来看看它的实现吧：</p><p><img src="/2020/01/13/java-de-object-dui-xiang/6.png" alt></p><p><img src="/2020/01/13/java-de-object-dui-xiang/7.png" alt></p><h1 id="三、toString方法"><a href="#三、toString方法" class="headerlink" title="三、toString方法"></a>三、toString方法</h1><p>接下来我们看看toString方法，也十分简单：</p><p><img src="/2020/01/13/java-de-object-dui-xiang/8.png" alt></p><p>toString方法主要是用来<strong>标识</strong>该对象的：</p><p><img src="/2020/01/13/java-de-object-dui-xiang/9.png" alt></p><p>从上面的结果我们都可以看出来：<strong>得出的结果我们并不能看到什么东西</strong>~</p><p>于是我们一般都重写toString()，那么<strong>打印出的结果就很方便我们调试了</strong>！</p><pre><code>    @Override    public String toString() {        return &quot;Address{&quot; +                &quot;provinceNo=&quot; + provinceNo +                &quot;, cityNo=&quot; + cityNo +                &quot;, streetNo=&quot; + streetNo +                &#39;}&#39;;    }</code></pre><p>下面的结果看起来就好多了：</p><p><img src="/2020/01/13/java-de-object-dui-xiang/10.png" alt></p><h1 id="四、clone方法"><a href="#四、clone方法" class="headerlink" title="四、clone方法"></a>四、clone方法</h1><p>我们也来看看它的顶部注释：</p><p><img src="/2020/01/13/java-de-object-dui-xiang/11.png" alt></p><p>看了上面的注释我们可以<strong>总结以下的要点</strong>：</p><ul><li>clone方法用于对象的克隆，一般想要克隆出的对象是<strong>独立</strong>的(与原有的对象是分开的)</li><li>深拷贝指的是该对象的成员变量(如果是可变引用)都应该克隆一份，浅拷贝指的是成员变量没有被克隆一份</li></ul><p>下面我们来看一下浅拷贝：<strong>拷贝了Employee对象，但是其成员变量hireday没有被克隆出去，所以指向的还是同一个Date对象</strong>！</p><p><img src="/2020/01/13/java-de-object-dui-xiang/12.png" alt></p><h2 id="4-1clone用法"><a href="#4-1clone用法" class="headerlink" title="4.1clone用法"></a>4.1clone用法</h2><p>那么我们如何克隆对象呢？无论是浅拷贝还是深拷贝都是这两步：</p><ol><li>克隆的对象要<strong>实现Cloneable接口</strong></li><li><strong>重写clone方法</strong>，最好修饰成public</li></ol><p><strong>浅拷贝</strong>：仅仅拷贝了Person对象，而date没有拷贝！</p><pre><code>public class Person implements Cloneable {    // 可变的成员变量    private Date date;    @Override    public Object clone() throws CloneNotSupportedException {        return super.clone();    }}</code></pre><p><strong>深拷贝</strong>：不仅拷贝了Person对象，也拷贝了date成员变量</p><pre><code>public class Person implements Cloneable {    // 可变的成员变量    public  Date date;    @Override    public Object clone() throws CloneNotSupportedException {        // 拷贝Person对象        Person person = (Person) super.clone();        // 将可变的成员变量也拷贝        person.date = (Date) date.clone();        // 返回拷贝的对象        return person;    }}</code></pre><h2 id="4-2clone疑问进一步学习protected"><a href="#4-2clone疑问进一步学习protected" class="headerlink" title="4.2clone疑问进一步学习protected"></a>4.2clone疑问进一步学习protected</h2><p>不知道有没有人跟我有相同的<strong>疑问</strong>：</p><ul><li>我只想要<strong>浅拷贝</strong>，能不能<strong>直接调用该对象.clone()来实现</strong>？</li></ul><p>比如我现在有个Address对象：</p><pre><code>public class Address  {    private int provinceNo;    private int cityNo;    private int streetNo;    public Address() {    }    public Address(int provinceNo, int cityNo, int streetNo) {        this.provinceNo = provinceNo;        this.cityNo = cityNo;        this.streetNo = streetNo;    }}</code></pre><p>下面的代码你们<strong>认为如何</strong>？</p><pre><code>    Address address = new Address(1, 2, 3);    address.clone();</code></pre><p>我们都知道：</p><ul><li><strong>protected修饰的类和属性,对于自己、本包和其子类可见</strong></li></ul><p><strong>可能会想</strong>：<code>clone()</code>方法是定义在Object类上的(以protected来修饰)，而我们自定义的Address对象<strong>隐式</strong>继承着Object(所有的对象都是Object的子类)，那么子类调用Object以protected来修饰<code>clone()</code>是完全没问题的</p><ul><li>但是，IDE现实告诉我，这<strong>编译就不通过了</strong>！</li></ul><p><img src="/2020/01/13/java-de-object-dui-xiang/13.png" alt></p><p>出现错误的原因我立马就想到：<strong>是不是我对protected修饰符出现了偏差？</strong></p><p>protected修饰的类和属性,对于自己、本包和其子类可见，这句话本身是没有错的。但是<strong>还需要补充</strong>：对于protected的成员或方法，要分子类和超类<strong>是否在同一个包中</strong>。与基类<strong>不在同一个包中的子类</strong>，只能<strong>访问自身从基类继承而来的受保护成员，而不能访问基类实例本身的受保护成员</strong>。</p><ul><li>上面的代码就错在：Address与Object<strong>不是在同一个包下</strong>的，而Address直接访问了Object的clone方法。这是不行的。</li></ul><p>下面我截两张图再来给你们看看(看完图再看上面的描述，就能理解了)：</p><p><img src="/2020/01/13/java-de-object-dui-xiang/14.png" alt></p><p><img src="/2020/01/13/java-de-object-dui-xiang/15.png" alt></p><h1 id="五、wait和notify方法"><a href="#五、wait和notify方法" class="headerlink" title="五、wait和notify方法"></a>五、wait和notify方法</h1><p>wait和notify方法其实就是Java给我们提供让<strong>线程之间通信</strong>的API。</p><p>按照惯例我们还是来看注释怎么说吧：</p><p>wait方法：</p><p><img src="/2020/01/13/java-de-object-dui-xiang/16.png" alt></p><p>notify方法：</p><p><img src="/2020/01/13/java-de-object-dui-xiang/17.png" alt></p><p>notifyAll()方法：</p><p><img src="/2020/01/13/java-de-object-dui-xiang/18.png" alt></p><p>看完上面的注释我们可以<strong>总结以下的要点</strong>：</p><ul><li><p>无论是wait、notify还是notifyAll()都需要</p><p>由监听器对象(锁对象)来进行调用</p><ul><li>简单来说：<strong>他们都是在同步代码块中调用的</strong>，否则会抛出异常！</li></ul></li><li><p><code>notify()</code>唤醒的是在等待队列的<strong>某个</strong>线程(不确定会唤醒哪个)，<code>notifyAll()</code>唤醒的是等待队列<strong>所有</strong>线程</p></li><li><p>导致wait()的线程被唤醒可以有4种情况</p><ul><li>该线程被中断</li><li><code>wait()</code>时间到了</li><li>被<code>notify()</code>唤醒</li><li>被<code>notifyAll()</code>唤醒</li></ul></li><li><p>调用<code>wait()</code>的线程会<strong>释放掉锁</strong></p></li></ul><p>其实总结完上面的并不会有比较深刻的印象，可以尝试着回答几个问题来加深对<code>wait()</code>和<code>notify()</code>的理解。</p><h2 id="5-1为什么wait和notify在Object方法上？"><a href="#5-1为什么wait和notify在Object方法上？" class="headerlink" title="5.1为什么wait和notify在Object方法上？"></a>5.1为什么wait和notify在Object方法上？</h2><p>从一开始我们就说了：<code>wait()</code>和<code>notify()</code>是Java给我们提供线程之间通信的API，既然是线程的东西，那什么是在Object类上定义，而不是在Thread类上定义呢？</p><p>因为我们的锁是对象锁，每个对象都可以成为锁。让当前线程等待某个对象的锁，当然应该通过这个对象来操作了。</p><ul><li>锁对象是<strong>任意</strong>的，所以这些方法必须定义在Object类中</li></ul><h2 id="5-2notify方法调用后，会发生什么？"><a href="#5-2notify方法调用后，会发生什么？" class="headerlink" title="5.2notify方法调用后，会发生什么？"></a>5.2notify方法调用后，会发生什么？</h2><p>上面已经说了，notify会唤醒某个处于等待队列的线程。</p><p>但是要<strong>注意</strong>的是：</p><ul><li>notify方法调用后，被唤醒的线程<strong>不会立马获得到锁对象</strong>。而是等待notify的synchronized代码块<strong>执行完之后</strong>才会获得锁对象</li></ul><h2 id="5-3sleep和wait有什么区别？"><a href="#5-3sleep和wait有什么区别？" class="headerlink" title="5.3sleep和wait有什么区别？"></a>5.3sleep和wait有什么区别？</h2><p><code>Thread.sleep()</code>与<code>Object.wait()</code>二者都可以暂停当前线程，释放CPU控制权。</p><ul><li>主要的区别在于<code>Object.wait()</code>在释放CPU同时，<strong>释放了对象锁的控制</strong>。</li><li>而<code>Thread.sleep()</code>没有对锁释放</li></ul><h1 id="六、finalize-方法"><a href="#六、finalize-方法" class="headerlink" title="六、finalize()方法"></a>六、finalize()方法</h1><p><code>finalize()</code>方法将在<strong>垃圾回收器清除对象之前调用</strong>，但该方法不知道何时调用，具有<strong>不定性</strong></p><ul><li>一般我们都不会重写它~</li></ul><blockquote><p>一个对象的finalize()方法<strong>只会被调用一次</strong>，而且finalize()被调用不意味着gc会立即回收该对象，所以有可能调用finalize()后，该对象又不需要被回收了，然后到了真正要被回收的时候，因为前面调用过一次，所以不会调用finalize()，产生问题。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>软件开发六大设计原则</title>
      <link href="/2020/01/10/ruan-jian-kai-fa-liu-da-she-ji-yuan-ze/"/>
      <url>/2020/01/10/ruan-jian-kai-fa-liu-da-she-ji-yuan-ze/</url>
      
        <content type="html"><![CDATA[<h1 id="开闭原则"><a href="#开闭原则" class="headerlink" title="开闭原则"></a>开闭原则</h1><p>定义：软件实体如类、模块和函数应该对扩展开放，对修改关闭。<br>优点：可提高软件系统的可复用性及可维护性<br>举例：一开始需要获取课程的价格，面向接口的开发原则如下</p><pre><code>public interface Course {    Double getPrice();}public class JavaCourse implements Course {    private Double price;    public JavaCourse() {    }    public JavaCourse(Integer id, String name, Double price) {        this.price = price;    }    @Override    public Double getPrice() {        return this.price;    }public class Test {    public static void main(String[] args) {        Course course = new JavaCourse(1,&quot;java&quot;,15.25);        System.out.println(&quot;课程价格&quot;+course.getPrice());    }}}</code></pre><p>后来需求变动，需要显示课程打折后的价格。<br>需要做如下变动：</p><pre><code>public class JavaDiscountCourse extends JavaCourse {    public JavaDiscountCourse(Integer id, String name, Double price) {        super(id, name, price);    }    public Double getDiscountPrice(){        return this.getPrice()*0.8;    }}public class Test {    public static void main(String[] args) {        Course course = new JavaDiscountCourse(1,&quot;java&quot;,15.25);        System.out.println(&quot;课程价格&quot;+course.getPrice()+&quot;\r\n课程折后价格&quot;+((JavaDiscountCourse) course).getDiscountPrice());    }}</code></pre><h1 id="依赖倒置原则"><a href="#依赖倒置原则" class="headerlink" title="依赖倒置原则"></a>依赖倒置原则</h1><p>定义：高层模块不应该依赖底层模块，二者都应该依赖其抽象<br>优点：可以减少类之间的耦合性、提高系统稳定性、提高代码可读性和可维护性，可降低修改程序所造成的风险<br>代码举例：</p><pre><code>public interface Course {    void study();}public class PythonCourse implements Course {    @Override    public void study() {        System.out.println(&quot;学习Python课程&quot;);    }}public class Alice {     Course course;     public Alice(Course course){         this.course = course;     }     public void studyCourse(){         course.study();     }}public class Test {    public static void main(String[] args) {        Alice alice = new Alice(new PythonCourse());        alice.studyCourse();    }}</code></pre><p>以上代码通过将PythonCourse的抽象注入Alice中实现了Alice和Course的具体实现的解耦，实现了依赖倒置。</p><h1 id="单一职责原则"><a href="#单一职责原则" class="headerlink" title="单一职责原则"></a>单一职责原则</h1><p>定义：不要存在多于一个导致类变更的原因<br>优点：降低类的复杂度，提高类的可读性，提高系统的可维护性、降低变更引起的风险<br>理解：一个类只需要实现一类功能，不要有几类功能同时在一个类中，这个微服务的思想有些相似，只是一个微服务需要实现的是一大类功能，目的都是使系统细粒度化，便于开发新功能时写更少的代码（用原有的功能通过不同的调用步骤或顺序或传入不同的参数即可组合出新的功能），也就是便于扩展。</p><h1 id="接口隔离原则"><a href="#接口隔离原则" class="headerlink" title="接口隔离原则"></a>接口隔离原则</h1><p>定义：用多个专门的接口，而不使用单一的总接口，客户端不应该依赖他不需要的接口。<br>优点：符合我们常说的高内聚低耦合的设计思想从而使得类具有很好的可读性、可扩展性和可维护性。</p><h1 id="迪米特法则（最少知道原则）"><a href="#迪米特法则（最少知道原则）" class="headerlink" title="迪米特法则（最少知道原则）"></a>迪米特法则（最少知道原则）</h1><p>定义：一个对象应该对其他对象保持最少的了解。<br>优点：降低类之间的耦合。<br>举例：如下类图并不遵循迪米特原则，因为Boss只需要和TeamLeader交互，而不需要和Course产生依赖。<br><img src="/2020/01/10/ruan-jian-kai-fa-liu-da-she-ji-yuan-ze/1.png" alt></p><p>改进后满足迪米特原则类关系如下<br><img src="/2020/01/10/ruan-jian-kai-fa-liu-da-she-ji-yuan-ze/2.png" alt></p><h1 id="里斯替换原则"><a href="#里斯替换原则" class="headerlink" title="里斯替换原则"></a>里斯替换原则</h1><p>定义1：如果对每一个类型为 T1的对象 o1，都有类型为 T2 的对象o2，使得以 T1定义的所有程序 P 在所有的对象 o1 都代换成 o2 时，程序 P 的行为没有发生变化，那么类型 T2 是类型 T1 的子类型。</p><p>定义2：所有引用基类的地方必须能透明地使用其子类的对象。</p><p>问题由来：有一功能P1，由类A完成。现需要将功能P1进行扩展，扩展后的功能为P，其中P由原有功能P1与新功能P2组成。新功能P由类A的子类B来完成，则子类B在完成新功能P2的同时，有可能会导致原有功能P1发生故障。</p><p>解决方案：当使用继承时，遵循里氏替换原则。类B继承类A时，除添加新的方法完成新增功能P2外，尽量不要重写父类A的方法，也尽量不要重载父类A的方法.</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深入理解JSP</title>
      <link href="/2020/01/09/shen-ru-li-jie-jsp/"/>
      <url>/2020/01/09/shen-ru-li-jie-jsp/</url>
      
        <content type="html"><![CDATA[<h4 id="JSP介绍"><a href="#JSP介绍" class="headerlink" title="JSP介绍"></a>JSP介绍</h4><p> JSP（Java server page）是Java EE规范最基本成员，他是Java Web开发的重点知识，虽然我们一直在用，但其原理知之甚少。今天重点研究一些JSP核心内容以及其工作原理。</p><p>  JSP和Servlet的本质是一样的，因为JSP最终需要编译成Servlet才能运行，换句话说JSP是生成Servler的草稿文件。</p><p>  JSP比较简单，就是在HTML中嵌入Java代码，或者使用JSP标签，包括使用用户自定义标签，从而可以动态的提供内容。早起JSP应用比较广泛，一个web应用可以全部由JSP页面组成，只需要少量的JavaBean即可，但是这样导致了JSP职责过于复杂，这是Java EE标准的出现无疑是雪中送炭，因此JSP慢慢发展成单一的表现技术，不再承担业务逻辑组件以及持久层组件的责任。</p><h4 id="JSP基本原理"><a href="#JSP基本原理" class="headerlink" title="JSP基本原理"></a>JSP基本原理</h4><p>  JSP的本质是servlet，当用户指定servlet发送请求时，servlet利用输出流动态生成HTML页面。由于包含大量的HTML标签。静态文本等格式导致servlet的开发效率极低，所有的表现逻辑，包括布局、色彩及图像等，都必须耦合在Java代码中，静态的部分无需Java程序控制，只有那些需要从数据库读取或者需要动态生成的页面内容才使用Java脚本控制。</p><h4 id="JSP和Servlet是什么关系"><a href="#JSP和Servlet是什么关系" class="headerlink" title="JSP和Servlet是什么关系?"></a>JSP和Servlet是什么关系?</h4><p>　　Servlet是一个特殊的Java程序，它运行于服务器的JVM中，能够依靠服务器的支持向浏览器提供显示内容。JSP本质上是Servlet的一种简易形式，JSP会被服务器处理成一个类似于Servlet的Java程序，可以简化页面内容的生成。Servlet和JSP最主要的不同点在于，Servlet的应用逻辑是在Java文件中，并且完全从表示层中的HTML分离开来。而JSP的情况是Java和HTML可以组合成一个扩展名为.jsp的文件。有人说，Servlet就是在Java中写HTML，而JSP就是在HTML中写Java代码，当然这个说法是很片面且不够准确的。JSP侧重于视图，Servlet更侧重于控制逻辑，在MVC架构模式中，JSP适合充当视图(view)而Servlet适合充当控制器(controller)。</p><p><strong>思考:为什么项目部署后修改jsp不需要重新启动?</strong>(可查阅jTomcat类加载器架构了解)</p><h4 id="JSP中的四种作用域"><a href="#JSP中的四种作用域" class="headerlink" title="JSP中的四种作用域"></a>JSP中的四种作用域</h4><p>JSP中的四种作用域包括page、request、session和application，具体来说：</p><ul><li>page代表与一个页面相关的对象和属性。</li><li>request代表与Web客户机发出的一个请求相关的对象和属性。一个请求可能跨越多个页面，涉及多个Web组件;需要在页面显示的临时数据可以置于此作用域。</li><li>session代表与某个用户与服务器建立的一次会话相关的对象和属性。跟某个用户相关的数据应该放在用户自己的session中。</li><li>application代表与整个Web应用程序相关的对象和属性，它实质上是跨越整个Web应用程序，包括多个页面、请求和会话的一个全局作用域。</li></ul><h4 id="实现会话跟踪的技术"><a href="#实现会话跟踪的技术" class="headerlink" title="实现会话跟踪的技术"></a>实现会话跟踪的技术</h4><p>　　由于HTTP协议本身是无状态的，服务器为了区分不同的用户，就需要对用户会话进行跟踪，简单的说就是为用户进行登记，为用户分配唯一的ID，下一次用户在请求中包含此ID，服务器据此判断到底是哪一个用户。</p><p>　　1)URL 重写：在URL中添加用户会话的信息作为请求的参数，或者将唯一的会话ID添加到URL结尾以标识一个会话。</p><p>　　2) 设置表单隐藏域：将和会话跟踪相关的字段添加到隐式表单域中，这些信息不会在浏览器中显示但是提交表单时会提交给服务器。</p><p>　　这两种方式很难处理跨越多个页面的信息传递，因为如果每次都要修改URL或在页面中添加隐式表单域来存储用户会话相关信息，事情将变得非常麻烦。</p><p>　　3)cookie：cookie有两种，一种是基于窗口的，浏览器窗口关闭后，cookie就没有了;另一种是将信息存储在一个临时文件中，并设置存在的时间。当用户通过浏览器和服务器建立一次会话后，会话ID就会随响应信息返回存储在基于窗口的cookie中，那就意味着只要浏览器没有关闭，会话没有超时，下一次请求时这个会话ID又会提交给服务器让服务器识别用户身份。会话中可以为用户保存信息。会话对象是在服务器内存中的，而基于窗口的cookie是在客户端内存中的。如果浏览器禁用了cookie，那么就需要通过下面两种方式进行会话跟踪。当然，在使用cookie时要注意几点：首先不要在cookie中存放敏感信息;其次cookie存储的数据量有限(4k)，不能将过多的内容存储cookie中;再者浏览器通常只允许一个站点最多存放20个cookie。当然，和用户会话相关的其他信息(除了会话ID)也可以存在cookie方便进行会话跟踪。</p><p>　　4)HttpSession：在所有会话跟踪技术中，HttpSession对象是最强大也是功能最多的。当一个用户第一次访问某个网站时会自动创建HttpSession，每个用户可以访问他自己的HttpSession。可以通过HttpServletRequest对象的getSession方法获得HttpSession，通过HttpSession的setAttribute方法可以将一个值放在HttpSession中，通过调用HttpSession对象的getAttribute方法，同时传入属性名就可以获取保存在HttpSession中的对象。与上面三种方式不同的是，HttpSession放在服务器的内存中，因此不要将过大的对象放在里面，即使目前的Servlet容器可以在内存将满时将HttpSession中的对象移到其他存储设备中，但是这样势必影响性能。添加到HttpSession中的值可以是任意Java对象，这个对象最好实现了Serializable接口，这样Servlet容器在必要的时候可以将其序列化到文件中，否则在序列化时就会出现异常。</p><h4 id="过滤器作用和用法"><a href="#过滤器作用和用法" class="headerlink" title="过滤器作用和用法"></a>过滤器作用和用法</h4><p>　　Java Web开发中的过滤器(filter)是从Servlet 2.3规范开始增加的功能，并在Servlet 2.4规范中得到增强。对Web应用来说，过滤器是一个驻留在服务器端的Web组件，它可以截取客户端和服务器之间的请求与响应信息，并对这些信息进行过滤。当Web容器接受到一个对资源的请求时，它将判断是否有过滤器与这个资源相关联。如果有，那么容器将把请求交给过滤器进行处理。在过滤器中，你可以改变请求的内容，或者重新设置请求的报头信息，然后再将请求发送给目标资源。当目标资源对请求作出响应时候，容器同样会将响应先转发给过滤器，在过滤器中你可以对响应的内容进行转换，然后再将响应发送到客户端。</p><p>　　常见的过滤器用途主要包括：对用户请求进行统一认证、对用户的访问请求进行记录和审核、对用户发送的数据进行过滤或替换、转换图象格式、对响应内容进行压缩以减少传输量、对请求或响应进行加解密处理、触发资源访问事件、对XML的输出应用XSLT等。</p><p>　　过滤器相关的接口主要有：Filter、FilterConfig和FilterChain。</p><p>　　监听器有哪些作用和用法?</p><p>　　Java Web开发中的监听器(listener)就是application、session、request三个对象创建、销毁或者往其中添加修改删除属性时自动执行代码的功能组件，如下所示：</p><p>　　ServletContextListener：对Servlet上下文的创建和销毁进行监听。</p><p>　　ervletContextAttributeListener：监听Servlet上下文属性的添加、删除和替换。</p><p>　　HttpSessionAttributeListener：对Session对象中属性的添加、删除和替换进行监听。</p><p>　　ServletRequestListener：对请求对象的初始化和销毁进行监听。</p><p>　　ServletRequestAttributeListener：对请求对象属性的添加、删除和替换进行监听。</p><p>　　HttpSessionListener：对Session的创建和销毁进行监听。</p><p>　　补充： session的销毁有两种情况：</p><p>　　session超时(可以在web.xml中通过/标签配置超时时间);</p><p>　　通过调用session对象的invalidate()方法使session失效。</p><p>注意:为什么项目部署后修改jsp不需要重新启动?</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>redis简单使用总结</title>
      <link href="/2020/01/08/redis-jian-dan-shi-yong-zong-jie/"/>
      <url>/2020/01/08/redis-jian-dan-shi-yong-zong-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h1><p>Redis 是速度非常快的非关系型（NoSQL）内存键值数据库，可以存储键和五种不同类型的值之间的映射。</p><p>键的类型只能为字符串，值支持五种数据类型：字符串、列表、集合、散列表、有序集合。</p><p>Redis 支持很多特性，例如将内存中的数据持久化到硬盘中，使用复制来扩展读性能，使用分片来扩展写性能。</p><h1 id="二、数据类型"><a href="#二、数据类型" class="headerlink" title="二、数据类型"></a>二、数据类型</h1><table><thead><tr><th>数据类型</th><th>可以存储的值</th><th>操作</th></tr></thead><tbody><tr><td>STRING</td><td>字符串、整数或者浮点数</td><td>对整个字符串或者字符串的其中一部分执行操作对整数和浮点数执行自增或者自减操作</td></tr><tr><td>LIST</td><td>列表</td><td>从两端压入或者弹出元素 对单个或者多个元素进行修剪，只保留一个范围内的元素</td></tr><tr><td>SET</td><td>无序集合</td><td>添加、获取、移除单个元素检查一个元素是否存在于集合中计算交集、并集、差集从集合里面随机获取元素</td></tr><tr><td>HASH</td><td>包含键值对的无序散列表</td><td>添加、获取、移除单个键值对获取所有键值对检查某个键是否存在</td></tr><tr><td>ZSET</td><td>有序集合</td><td>添加、获取、删除元素根据分值范围或者成员来获取元素计算一个键的排名</td></tr></tbody></table><h2 id="STRING"><a href="#STRING" class="headerlink" title="STRING"></a>STRING</h2><p>[<img src="/2020/01/08/redis-jian-dan-shi-yong-zong-jie/1.png" alt></p><pre><code>&gt; set hello worldOK&gt; get hello&quot;world&quot;&gt; del hello(integer) 1&gt; get hello(nil)</code></pre><h2 id="LIST"><a href="#LIST" class="headerlink" title="LIST"></a>LIST</h2><p>[<img src="/2020/01/08/redis-jian-dan-shi-yong-zong-jie/2.png" alt></p><pre><code>&gt; rpush list-key item(integer) 1&gt; rpush list-key item2(integer) 2&gt; rpush list-key item(integer) 3&gt; lrange list-key 0 -11) &quot;item&quot;2) &quot;item2&quot;3) &quot;item&quot;&gt; lindex list-key 1&quot;item2&quot;&gt; lpop list-key&quot;item&quot;&gt; lrange list-key 0 -11) &quot;item2&quot;2) &quot;item&quot;</code></pre><h2 id="SET"><a href="#SET" class="headerlink" title="SET"></a>SET</h2><p>[<img src="/2020/01/08/redis-jian-dan-shi-yong-zong-jie/3.png" alt></p><pre><code>&gt; sadd set-key item(integer) 1&gt; sadd set-key item2(integer) 1&gt; sadd set-key item3(integer) 1&gt; sadd set-key item(integer) 0&gt; smembers set-key1) &quot;item&quot;2) &quot;item2&quot;3) &quot;item3&quot;&gt; sismember set-key item4(integer) 0&gt; sismember set-key item(integer) 1&gt; srem set-key item2(integer) 1&gt; srem set-key item2(integer) 0&gt; smembers set-key1) &quot;item&quot;2) &quot;item3&quot;</code></pre><h2 id="HASH"><a href="#HASH" class="headerlink" title="HASH"></a>HASH</h2><p>[<img src="/2020/01/08/redis-jian-dan-shi-yong-zong-jie/4.png" alt></p><pre><code>&gt; hset hash-key sub-key1 value1(integer) 1&gt; hset hash-key sub-key2 value2(integer) 1&gt; hset hash-key sub-key1 value1(integer) 0&gt; hgetall hash-key1) &quot;sub-key1&quot;2) &quot;value1&quot;3) &quot;sub-key2&quot;4) &quot;value2&quot;&gt; hdel hash-key sub-key2(integer) 1&gt; hdel hash-key sub-key2(integer) 0&gt; hget hash-key sub-key1&quot;value1&quot;&gt; hgetall hash-key1) &quot;sub-key1&quot;2) &quot;value1&quot;</code></pre><h2 id="ZSET"><a href="#ZSET" class="headerlink" title="ZSET"></a>ZSET</h2><p>[<img src="/2020/01/08/redis-jian-dan-shi-yong-zong-jie/5.png" alt></p><pre><code>&gt; zadd zset-key 728 member1(integer) 1&gt; zadd zset-key 982 member0(integer) 1&gt; zadd zset-key 982 member0(integer) 0&gt; zrange zset-key 0 -1 withscores1) &quot;member1&quot;2) &quot;728&quot;3) &quot;member0&quot;4) &quot;982&quot;&gt; zrangebyscore zset-key 0 800 withscores1) &quot;member1&quot;2) &quot;728&quot;&gt; zrem zset-key member1(integer) 1&gt; zrem zset-key member1(integer) 0&gt; zrange zset-key 0 -1 withscores1) &quot;member0&quot;2) &quot;982&quot;</code></pre><h1 id="三、使用场景"><a href="#三、使用场景" class="headerlink" title="三、使用场景"></a>三、使用场景</h1><h2 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h2><p>可以对 String 进行自增自减运算，从而实现计数器功能。</p><p>Redis 这种内存型数据库的读写性能非常高，很适合存储频繁读写的计数量。</p><h2 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h2><p>将热点数据放到内存中，设置内存的最大使用量以及淘汰策略来保证缓存的命中率。</p><h2 id="查找表"><a href="#查找表" class="headerlink" title="查找表"></a>查找表</h2><p>例如 DNS 记录就很适合使用 Redis 进行存储。</p><p>查找表和缓存类似，也是利用了 Redis 快速的查找特性。但是查找表的内容不能失效，而缓存的内容可以失效，因为缓存不作为可靠的数据来源。</p><h2 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h2><p>List 是一个双向链表，可以通过 lpush 和 rpop 写入和读取消息</p><p>不过最好使用 Kafka、RabbitMQ 等消息中间件。</p><h2 id="会话缓存"><a href="#会话缓存" class="headerlink" title="会话缓存"></a>会话缓存</h2><p>可以使用 Redis 来统一存储多台应用服务器的会话信息。</p><p>当应用服务器不再存储用户的会话信息，也就不再具有状态，一个用户可以请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性。</p><h2 id="分布式锁实现"><a href="#分布式锁实现" class="headerlink" title="分布式锁实现"></a>分布式锁实现</h2><p>在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步。</p><p>可以使用 Redis 自带的 SETNX 命令实现分布式锁，除此之外，还可以使用官方提供的 RedLock 分布式锁实现。</p><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p>Set 可以实现交集、并集等操作，从而实现共同好友等功能。</p><p>ZSet 可以实现有序性操作，从而实现排行榜等功能。</p>]]></content>
      
      
      <categories>
          
          <category> tool </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>JVM必须知道的基础</title>
      <link href="/2020/01/07/jvm-bi-xu-zhi-dao-de-ji-chu/"/>
      <url>/2020/01/07/jvm-bi-xu-zhi-dao-de-ji-chu/</url>
      
        <content type="html"><![CDATA[<h2 id="运行时数据区域"><a href="#运行时数据区域" class="headerlink" title="运行时数据区域"></a>运行时数据区域</h2><p>网上有很多描述JVM内存区的图,我觉得这张能表述内容较多</p><p><img src="/2020/01/07/jvm-bi-xu-zhi-dao-de-ji-chu/1.png" alt></p><h4 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h4><pre><code> 程序计数器(Program Counter Register)存储当前线程执行的字节码行号，占用内存较小。字节码解释器就是通过这个计数器的值来选择下一条需要执行的字节码指令。执行Java方法时计数器指向正在执行的虚拟字节码指令的地址，执行Native方法时指向空。</code></pre><h4 id="java虚拟机栈"><a href="#java虚拟机栈" class="headerlink" title="java虚拟机栈"></a>java虚拟机栈</h4><pre><code>java虚拟机栈（Java Virtual Machine Stack）与程序计数器一样，也是线程私有的，生命周期与线程相同。java虚拟机栈描述的是java方法执行的内存模型,每个方法执行的时候都会创建一个栈帧,用于存储局部变量表、操作数栈、动态链接、方法出口等信息。方法的调用和结束分别对应栈帧的入栈和出栈。</code></pre><h4 id="本地方法栈"><a href="#本地方法栈" class="headerlink" title="本地方法栈"></a>本地方法栈</h4><pre><code>本地方法栈（Native Method Stack）与java虚拟机栈作用相似,java虚拟机栈是为java方法服务,本地方法栈是为Native方法服务,甚至有些虚拟机实现时直接将两者合而为一(如:HotSpot).</code></pre><h4 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h4><pre><code>Java堆(Java Heap)是虚拟机管理的内存中最大的一块，该内存区域的唯一目的就是存放对象实例。java堆是垃圾收集区域管理的主要区域。从回收的角度看，可以细分为新生代和老年代；再细致一点就是Eden空间、From Survivor空间、To Survivor空间。线程共享的java堆能划分出多个线程私有的分配缓冲区（Thread Local Allocation Buffer，TLAB）。</code></pre><h4 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h4><pre><code>方法区（Method Area）与java堆一样，是各个线程共享的内存区域，用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译后的代码数据等。这个区域的垃圾回收主要是针对常量池的回收和对类型的卸载。</code></pre><h5 id="运行时常量池"><a href="#运行时常量池" class="headerlink" title="运行时常量池"></a>运行时常量池</h5><pre><code>运行时常量池（Runtime Constant Pool）是方法区的一部分。Class文件的常量池(Constant Pool Table)将在类加载后进入方法区的运行时常量池存放。除此之外，还会将翻译出来的直接引用也存放在运行时常量池。运行期间也可以将新的常量放入运行时常量池。</code></pre><h4 id="直接内存"><a href="#直接内存" class="headerlink" title="直接内存"></a>直接内存</h4><pre><code>直接内存(Direct Memory)并不是java虚拟机运行时数据区的一部分。在JDK1.4中新加入了NIO（New Input/Output）类,引入了一种基于通道(channel)和缓冲区(Buffer)的I/O方式.他可以使用Native函数库直接分配对外内存.然后通过一个内存在java堆中的DirectByteBuffer对象作为这块内存的引用进行操作.</code></pre><h2 id="垃圾收集"><a href="#垃圾收集" class="headerlink" title="垃圾收集"></a>垃圾收集</h2><h3 id="判断一个对象是否可被回收"><a href="#判断一个对象是否可被回收" class="headerlink" title="判断一个对象是否可被回收"></a>判断一个对象是否可被回收</h3><h4 id="引用计数算法"><a href="#引用计数算法" class="headerlink" title="引用计数算法"></a>引用计数算法</h4><p>为对象添加一个引用计数器，当对象增加一个引用时计数器加 1，引用失效时计数器减 1。引用计数为 0 的对象可被回收。</p><p>在两个对象出现循环引用的情况下，此时引用计数器永远不为 0，导致无法对它们进行回收。正是因为循环引用的存在，因此 Java 虚拟机不使用引用计数算法。</p><h4 id="可达性分析算"><a href="#可达性分析算" class="headerlink" title="可达性分析算"></a>可达性分析算</h4><p>以 GC Roots 为起始点进行搜索，可达的对象都是存活的，不可达的对象可被回收。</p><p>Java 虚拟机使用该算法来判断对象是否可被回收，GC Roots 一般包含以下内容：</p><ul><li>虚拟机栈中局部变量表中引用的对象</li><li>本地方法栈中 JNI 中引用的对象</li><li>方法区中类静态属性引用的对象</li><li>方法区中的常量引用的对象</li></ul><h4 id="方法区的回收"><a href="#方法区的回收" class="headerlink" title="方法区的回收"></a>方法区的回收</h4><p>因为方法区主要存放永久代对象，而永久代对象的回收率比新生代低很多，所以在方法区上进行回收性价比不高。</p><p>主要是对常量池的回收和对类的卸载。</p><p>为了避免内存溢出，在大量使用反射和动态代理的场景都需要虚拟机具备类卸载功能。</p><p>类的卸载条件很多，需要满足以下三个条件，并且满足了条件也不一定会被卸载：</p><ul><li>该类所有的实例都已经被回收，此时堆中不存在该类的任何实例。</li><li>加载该类的 ClassLoader 已经被回收。</li><li>该类对应的 Class 对象没有在任何地方被引用，也就无法在任何地方通过反射访问该类方法。</li></ul><h4 id="finalize"><a href="#finalize" class="headerlink" title="finalize()"></a>finalize()</h4><p>当一个对象可被回收时，如果需要执行该对象的 finalize() 方法，那么就有可能在该方法中让对象重新被引用，从而实现自救。自救只能进行一次，如果回收的对象之前调用了 finalize() 方法自救，后面回收时不会再调用该方法。</p><h3 id="引用类型"><a href="#引用类型" class="headerlink" title="引用类型"></a>引用类型</h3><p>无论是通过引用计数算法判断对象的引用数量，还是通过可达性分析算法判断对象是否可达，判定对象是否可被回收都与引用有关。</p><p>Java 提供了四种强度不同的引用类型。</p><h4 id="强引用"><a href="#强引用" class="headerlink" title="强引用"></a>强引用</h4><p>被强引用关联的对象不会被回收。</p><p>使用 new 一个新对象的方式来创建强引用。</p><pre><code>Object obj = new Object();</code></pre><h4 id="软引用"><a href="#软引用" class="headerlink" title="软引用"></a>软引用</h4><p>被软引用关联的对象只有在内存不够的情况下才会被回收。</p><p>使用 SoftReference 类来创建软引用。</p><pre><code>Object obj = new Object();SoftReference&lt;Object&gt; sf = new SoftReference&lt;Object&gt;(obj);obj = null;  // 使对象只被软引用关联</code></pre><h4 id="弱引用"><a href="#弱引用" class="headerlink" title="弱引用"></a>弱引用</h4><p>被弱引用关联的对象一定会被回收，也就是说它只能存活到下一次垃圾回收发生之前。</p><p>使用 WeakReference 类来创建弱引用。</p><pre><code>Object obj = new Object();WeakReference&lt;Object&gt; wf = new WeakReference&lt;Object&gt;(obj);obj = null;</code></pre><h4 id="虚引用"><a href="#虚引用" class="headerlink" title="虚引用"></a>虚引用</h4><p>又称为幽灵引用或者幻影引用，一个对象是否有虚引用的存在，不会对其生存时间造成影响，也无法通过虚引用得到一个对象。</p><p>为一个对象设置虚引用的唯一目的是能在这个对象被回收时收到一个系统通知。</p><p>使用 PhantomReference 来创建虚引用。</p><pre><code>Object obj = new Object();PhantomReference&lt;Object&gt; pf = new PhantomReference&lt;Object&gt;(obj, null);obj = null;</code></pre><h3 id="垃圾收集算法"><a href="#垃圾收集算法" class="headerlink" title="垃圾收集算法"></a>垃圾收集算法</h3><h4 id="1-标记-清除"><a href="#1-标记-清除" class="headerlink" title="1. 标记 - 清除"></a>1. 标记 - 清除</h4><p>顾名思义，标记-清除算法分为两个阶段，标记(mark)和清除(sweep).</p><p>在标记阶段，collector从mutator根对象开始进行遍历，对从mutator根对象可以访问到的对象都打上一个标识，一般是在对象的header中，将其记录为可达对象。</p><p>而在清除阶段，collector对堆内存(heap memory)从头到尾进行线性的遍历，如果发现某个对象没有标记为可达对象-通过读取对象的header信息，则就将其回收。</p><p><img src="/2020/01/07/jvm-bi-xu-zhi-dao-de-ji-chu/2.png" alt></p><p>从上图我们可以看到，在Mark阶段，从根对象1可以访问到B对象，从B对象又可以访问到E对象，所以B,E对象都是可达的。同理，F,G,J,K也都是可达对象。到了Sweep阶段，所有非可达对象都会被collector回收。同时，Collector在进行标记和清除阶段时会将整个应用程序暂停(mutator)，等待标记清除结束后才会恢复应用程序的运行。</p><ul><li><p><strong>优点</strong>：</p><p>实现简单，不需要进行对象进行移动。</p></li><li><p><strong>缺点</strong>：</p><p>标记、清除过程效率低，产生大量不连续的内存碎片，提高了垃圾回收的频率。</p></li></ul><h4 id="2-标记-整理"><a href="#2-标记-整理" class="headerlink" title="2. 标记 - 整理"></a>2. 标记 - 整理</h4><p>其中标记阶段跟标记-清除算法中的标记阶段是一样的，而对于整理阶段，它的工作就是移动所有的可达对象到堆内存的同一个区域中，使他们紧凑的排列在一起，从而将所有 <strong>非可达对象释放出来的空闲内存</strong> 都集中在一起，通过这样的方式来达到减少内存碎片的目的。</p><p><img src="/2020/01/07/jvm-bi-xu-zhi-dao-de-ji-chu/3.png" alt="3"></p><ul><li><p><strong>优点</strong>：</p><p>解决了标记-清理算法存在的内存碎片问题。</p></li><li><p><strong>缺点</strong>：</p><p>仍需要进行局部对象移动，一定程度上降低了效率。</p></li></ul><h4 id="3-复制"><a href="#3-复制" class="headerlink" title="3. 复制"></a>3. 复制</h4><p>这种收集算法解决了标记清除算法存在的效率问题。它将内存区域划分成相同的两个<strong>内存块</strong>。每次仅使用一半的空间，<code>JVM</code>生成的新对象放在一半空间中。当一半空间用完时进行<code>GC</code>，把可到达对象复制到另一半空间，然后把使用过的内存空间一次清理掉。</p><ul><li><p><strong>优点</strong>：</p><p>按顺序分配内存即可，实现简单、运行高效，不用考虑内存碎片。</p></li><li><p><strong>缺点</strong>：</p><p>可用的内存大小缩小为原来的一半，对象存活率高时会频繁进行复制。</p></li></ul><h4 id="4-分代收集"><a href="#4-分代收集" class="headerlink" title="4. 分代收集"></a>4. 分代收集</h4><p>现在的商业虚拟机采用分代收集算法，它根据对象存活周期将内存划分为几块，不同块采用适当的收集算法。</p><p>一般将堆分为新生代和老年代。</p><ul><li>对于<strong>新生代</strong>，每次<code>GC</code>时都有<strong>大量</strong>的对象死亡，只有<strong>少量</strong>对象存活。考虑到复制成本低，适合采用<strong>复制算法</strong>。因此有了<code>From Survivor</code>和<code>To Survivor</code>区域。</li><li>对于<strong>老年代</strong>，因为对象<strong>存活率高</strong>，没有额外的内存空间对它进行担保。因而适合采用<strong>标记-清理算法</strong>和<strong>标记-整理算法</strong>进行回收。</li></ul><h3 id="垃圾收集器"><a href="#垃圾收集器" class="headerlink" title="垃圾收集器"></a>垃圾收集器</h3><p><img src="/2020/01/07/jvm-bi-xu-zhi-dao-de-ji-chu/4.png" alt></p><p>以上是 HotSpot 虚拟机中的 7 个垃圾收集器，连线表示垃圾收集器可以配合使用。</p><ul><li>单线程与多线程：单线程指的是垃圾收集器只使用一个线程，而多线程使用多个线程；</li><li>串行与并行：串行指的是垃圾收集器与用户程序交替执行，这意味着在执行垃圾收集的时候需要停顿用户程序；并行指的是垃圾收集器和用户程序同时执行。除了 CMS 和 G1 之外，其它垃圾收集器都是以串行的方式执行。</li></ul><h4 id="1-Serial-收集器"><a href="#1-Serial-收集器" class="headerlink" title="1. Serial 收集器"></a>1. Serial 收集器</h4><p><img src="/2020/01/07/jvm-bi-xu-zhi-dao-de-ji-chu/5.png" alt></p><p>Serial 翻译为串行，也就是说它以串行的方式执行。</p><p>它是单线程的收集器，只会使用一个线程进行垃圾收集工作。</p><p>它的优点是简单高效，在单个 CPU 环境下，由于没有线程交互的开销，因此拥有最高的单线程收集效率。</p><p>它是 Client 场景下的默认新生代收集器，因为在该场景下内存一般来说不会很大。它收集一两百兆垃圾的停顿时间可以控制在一百多毫秒以内，只要不是太频繁，这点停顿时间是可以接受的。</p><h4 id="2-ParNew-收集器"><a href="#2-ParNew-收集器" class="headerlink" title="2. ParNew 收集器"></a>2. ParNew 收集器</h4><p><a href="https://camo.githubusercontent.com/573a3abc71931daef42e0b42b1876cbe4f940cdc/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f38313533386364352d316263662d346533312d383665352d6531393864663165303133622e6a7067" target="_blank" rel="noopener"><img src="/2020/01/07/jvm-bi-xu-zhi-dao-de-ji-chu/7.png" alt></a></p><p>它是 Serial 收集器的多线程版本。</p><p>它是 Server 场景下默认的新生代收集器，除了性能原因外，主要是因为除了 Serial 收集器，只有它能与 CMS 收集器配合使用。</p><h4 id="3-Parallel-Scavenge-收集器"><a href="#3-Parallel-Scavenge-收集器" class="headerlink" title="3. Parallel Scavenge 收集器"></a>3. Parallel Scavenge 收集器</h4><p>与 ParNew 一样是多线程收集器。</p><p>其它收集器目标是尽可能缩短垃圾收集时用户线程的停顿时间，而它的目标是达到一个可控制的吞吐量，因此它被称为“吞吐量优先”收集器。这里的吞吐量指 CPU 用于运行用户程序的时间占总时间的比值。</p><p>停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验。而高吞吐量则可以高效率地利用 CPU 时间，尽快完成程序的运算任务，适合在后台运算而不需要太多交互的任务。</p><p>缩短停顿时间是以牺牲吞吐量和新生代空间来换取的：新生代空间变小，垃圾回收变得频繁，导致吞吐量下降。</p><p>可以通过一个开关参数打开 GC 自适应的调节策略（GC Ergonomics），就不需要手工指定新生代的大小（-Xmn）、Eden 和 Survivor 区的比例、晋升老年代对象年龄等细节参数了。虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。</p><h4 id="4-Serial-Old-收集器"><a href="#4-Serial-Old-收集器" class="headerlink" title="4. Serial Old 收集器"></a>4. Serial Old 收集器</h4><p><img src="/2020/01/07/jvm-bi-xu-zhi-dao-de-ji-chu/13.png" alt="13"></p><p>是 Serial 收集器的老年代版本，也是给 Client 场景下的虚拟机使用。如果用在 Server 场景下，它有两大用途：</p><ul><li>在 JDK 1.5 以及之前版本（Parallel Old 诞生以前）中与 Parallel Scavenge 收集器搭配使用。</li><li>作为 CMS 收集器的后备预案，在并发收集发生 Concurrent Mode Failure 时使用。</li></ul><h4 id="5-Parallel-Old-收集器"><a href="#5-Parallel-Old-收集器" class="headerlink" title="5. Parallel Old 收集器"></a>5. Parallel Old 收集器<img src="/2020/01/07/jvm-bi-xu-zhi-dao-de-ji-chu/8.png" alt></h4><p>是 Parallel Scavenge 收集器的老年代版本。</p><p>在注重吞吐量以及 CPU 资源敏感的场合，都可以优先考虑 Parallel Scavenge 加 Parallel Old 收集器。</p><h4 id="6-CMS-收集器"><a href="#6-CMS-收集器" class="headerlink" title="6. CMS 收集器"></a>6. CMS 收集器</h4><p><img src="/2020/01/07/jvm-bi-xu-zhi-dao-de-ji-chu/9.png" alt></p><p>CMS（Concurrent Mark Sweep），Mark Sweep 指的是标记 - 清除算法。</p><p>分为以下四个流程：</p><ul><li>初始标记：仅仅只是标记一下 GC Roots 能直接关联到的对象，速度很快，需要停顿。</li><li>并发标记：进行 GC Roots Tracing 的过程，它在整个回收过程中耗时最长，不需要停顿。</li><li>重新标记：为了修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，需要停顿。</li><li>并发清除：不需要停顿。</li></ul><p>在整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，不需要进行停顿。</p><p>具有以下缺点：</p><ul><li>吞吐量低：低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利用率不够高。</li><li>无法处理浮动垃圾，可能出现 Concurrent Mode Failure。浮动垃圾是指并发清除阶段由于用户线程继续运行而产生的垃圾，这部分垃圾只能到下一次 GC 时才能进行回收。由于浮动垃圾的存在，因此需要预留出一部分内存，意味着 CMS 收集不能像其它收集器那样等待老年代快满的时候再回收。如果预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS。</li><li>标记 - 清除算法导致的空间碎片，往往出现老年代空间剩余，但无法找到足够大连续空间来分配当前对象，不得不提前触发一次 Full GC。</li></ul><h4 id="7-G1-收集器"><a href="#7-G1-收集器" class="headerlink" title="7. G1 收集器"></a>7. G1 收集器</h4><p>G1（Garbage-First），它是一款面向服务端应用的垃圾收集器，在多 CPU 和大内存的场景下有很好的性能。HotSpot 开发团队赋予它的使命是未来可以替换掉 CMS 收集器。</p><p>堆被分为新生代和老年代，其它收集器进行收集的范围都是整个新生代或者老年代，而 G1 可以直接对新生代和老年代一起回收。<img src="/2020/01/07/jvm-bi-xu-zhi-dao-de-ji-chu/10.png" alt></p><p>G1 把堆划分成多个大小相等的独立区域（Region），新生代和老年代不再物理隔离。</p><p><a href="https://camo.githubusercontent.com/5049da1b34969b272be2bffc6c6de0206b33253c/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f39626264646565622d653933392d343166302d386538652d3262316130616137653061372e706e67" target="_blank" rel="noopener"><img src="/2020/01/07/jvm-bi-xu-zhi-dao-de-ji-chu/11.png" alt></a></p><p>通过引入 Region 的概念，从而将原来的一整块内存空间划分成多个的小空间，使得每个小空间可以单独进行垃圾回收。这种划分方法带来了很大的灵活性，使得可预测的停顿时间模型成为可能。通过记录每个 Region 垃圾回收时间以及回收所获得的空间（这两个值是通过过去回收的经验获得），并维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region。</p><p>每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描。</p><p><a href="https://camo.githubusercontent.com/5bd72d589ead80c22547e3288a9a406241a1fb6b/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f66393965653737312d633536662d343766622d393134382d6330303336363935623566652e6a7067" target="_blank" rel="noopener"><img src="/2020/01/07/jvm-bi-xu-zhi-dao-de-ji-chu/12.png" alt></a></p><p>如果不计算维护 Remembered Set 的操作，G1 收集器的运作大致可划分为以下几个步骤：</p><ul><li>初始标记</li><li>并发标记</li><li>最终标记：为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程的 Remembered Set Logs 里面，最终标记阶段需要把 Remembered Set Logs 的数据合并到 Remembered Set 中。这阶段需要停顿线程，但是可并行执行。</li><li>筛选回收：首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。</li></ul><p>具备如下特点：</p><ul><li>空间整合：整体来看是基于“标记 - 整理”算法实现的收集器，从局部（两个 Region 之间）上来看是基于“复制”算法实现的，这意味着运行期间不会产生内存空间碎片。</li><li>可预测的停顿：能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在 GC 上的时间不得超过 N 毫秒。</li></ul><h2 id="内存分配与回收策略"><a href="#内存分配与回收策略" class="headerlink" title="内存分配与回收策略"></a>内存分配与回收策略</h2><h3 id="Minor-GC-和-Full-GC"><a href="#Minor-GC-和-Full-GC" class="headerlink" title="Minor GC 和 Full GC"></a>Minor GC 和 Full GC</h3><ul><li>Minor GC：回收新生代，因为新生代对象存活时间很短，因此 Minor GC 会频繁执行，执行的速度一般也会比较快。</li><li>Full GC：回收老年代和新生代，老年代对象其存活时间长，因此 Full GC 很少执行，执行速度会比 Minor GC 慢很多。</li></ul><h3 id="内存分配策略"><a href="#内存分配策略" class="headerlink" title="内存分配策略"></a>内存分配策略</h3><h4 id="1-对象优先在-Eden-分配"><a href="#1-对象优先在-Eden-分配" class="headerlink" title="1. 对象优先在 Eden 分配"></a>1. 对象优先在 Eden 分配</h4><p>大多数情况下，对象在新生代 Eden 上分配，当 Eden 空间不够时，发起 Minor GC。</p><h4 id="2-大对象直接进入老年代"><a href="#2-大对象直接进入老年代" class="headerlink" title="2. 大对象直接进入老年代"></a>2. 大对象直接进入老年代</h4><p>大对象是指需要连续内存空间的对象，最典型的大对象是那种很长的字符串以及数组。</p><p>经常出现大对象会提前触发垃圾收集以获取足够的连续空间分配给大对象。</p><p>-XX:PretenureSizeThreshold，大于此值的对象直接在老年代分配，避免在 Eden 和 Survivor 之间的大量内存复制。</p><h4 id="3-长期存活的对象进入老年代"><a href="#3-长期存活的对象进入老年代" class="headerlink" title="3. 长期存活的对象进入老年代"></a>3. 长期存活的对象进入老年代</h4><p>为对象定义年龄计数器，对象在 Eden 出生并经过 Minor GC 依然存活，将移动到 Survivor 中，年龄就增加 1 岁，增加到一定年龄则移动到老年代中。</p><p>-XX:MaxTenuringThreshold 用来定义年龄的阈值。</p><h4 id="4-动态对象年龄判定"><a href="#4-动态对象年龄判定" class="headerlink" title="4. 动态对象年龄判定"></a>4. 动态对象年龄判定</h4><p>虚拟机并不是永远要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升老年代，如果在 Survivor 中相同年龄所有对象大小的总和大于 Survivor 空间的一半，则年龄大于或等于该年龄的对象可以直接进入老年代，无需等到 MaxTenuringThreshold 中要求的年龄。</p><h4 id="5-空间分配担保"><a href="#5-空间分配担保" class="headerlink" title="5. 空间分配担保"></a>5. 空间分配担保</h4><p>在发生 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的。</p><p>如果不成立的话虚拟机会查看 HandlePromotionFailure 的值是否允许担保失败，如果允许那么就会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC；如果小于，或者 HandlePromotionFailure 的值不允许冒险，那么就要进行一次 Full GC。</p><h3 id="Full-GC-的触发条件"><a href="#Full-GC-的触发条件" class="headerlink" title="Full GC 的触发条件"></a>Full GC 的触发条件</h3><p>对于 Minor GC，其触发条件非常简单，当 Eden 空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件：</p><h4 id="1-调用-System-gc"><a href="#1-调用-System-gc" class="headerlink" title="1. 调用 System.gc()"></a>1. 调用 System.gc()</h4><p>只是建议虚拟机执行 Full GC，但是虚拟机不一定真正去执行。不建议使用这种方式，而是让虚拟机管理内存。</p><h4 id="2-老年代空间不足"><a href="#2-老年代空间不足" class="headerlink" title="2. 老年代空间不足"></a>2. 老年代空间不足</h4><p>老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等。</p><p>为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold 调大对象进入老年代的年龄，让对象在新生代多存活一段时间。</p><h4 id="3-空间分配担保失败"><a href="#3-空间分配担保失败" class="headerlink" title="3. 空间分配担保失败"></a>3. 空间分配担保失败</h4><p>使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果担保失败会执行一次 Full GC。具体内容请参考上面的第 5 小节。</p><h4 id="4-JDK-1-7-及以前的永久代空间不足"><a href="#4-JDK-1-7-及以前的永久代空间不足" class="headerlink" title="4. JDK 1.7 及以前的永久代空间不足"></a>4. JDK 1.7 及以前的永久代空间不足</h4><p>在 JDK 1.7 及以前，HotSpot 虚拟机中的方法区是用永久代实现的，永久代中存放的为一些 Class 的信息、常量、静态变量等数据。</p><p>当系统中要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，在未配置为采用 CMS GC 的情况下也会执行 Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出 java.lang.OutOfMemoryError。</p><p>为避免以上原因引起的 Full GC，可采用的方法为增大永久代空间或转为使用 CMS GC。</p><h4 id="5-Concurrent-Mode-Failure"><a href="#5-Concurrent-Mode-Failure" class="headerlink" title="5. Concurrent Mode Failure"></a>5. Concurrent Mode Failure</h4><p>执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足（可能是 GC 过程中浮动垃圾过多导致暂时性的空间不足），便会报 Concurrent Mode Failure 错误，并触发 Full GC。</p><h2 id="类加载机制概念"><a href="#类加载机制概念" class="headerlink" title="类加载机制概念"></a>类加载机制概念</h2><ul><li>Java虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的加载机制。*Class文件由类装载器装载后，在JVM中将形成一份描述Class结构的元信息对象，通过该元信息对象可以获知Class的结构信息：如构造函数，属性和方法等，Java允许用户借由这个Class相关的元信息对象间接调用Class对象的功能,这里就是我们经常能见到的Class类。</li></ul><h3 id="类加载过程"><a href="#类加载过程" class="headerlink" title="类加载过程"></a>类加载过程</h3><p><img src="/2020/01/07/jvm-bi-xu-zhi-dao-de-ji-chu/6.png" alt></p><p>工作机制</p><p>类装载器就是寻找类的字节码文件，并构造出类在JVM内部表示的对象组件。在Java中，类装载器把一个类装入JVM中，要经过以下步骤：</p><pre><code>  (1) 装载：查找和导入Class文件；  (2) 链接：把类的二进制数据合并到JRE中；     (a)校验：检查载入Class文件数据的正确性；     (b)准备：给类的静态变量分配存储空间；     (c)解析：将符号引用转成直接引用；  (3) 初始化：对类的静态变量，静态代码块执行初始化操作</code></pre><p>Java程序可以动态扩展是由运行期动态加载和动态链接实现的；比如：如果编写一个使用接口的应用程序，可以等到运行时再指定其实际的实现(多态)，解析过程有时候还可以在初始化之后执行；比如：动态绑定(多态)如上图所示，加载、验证、准备、初始化和卸载这五个阶段的顺序是确定的，类的加载过程必须按照这个顺序来按部就班地开始，而解析阶段则不一定，它在某些情况下可以在初始化阶段后再开始。类的生命周期的每一个阶段通常都是互相交叉混合式进行的，通常会在一个阶段执行的过程中调用或激活另外一个阶段。</p><h4 id="装载-加载"><a href="#装载-加载" class="headerlink" title="装载(加载)"></a>装载(加载)</h4><p>类的装载指的是将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的<strong>方法区</strong>内，然后在堆区创建一个java.lang.Class对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中的Class对象，Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。</p><p>类加载器并不需要等到某个类被“首次主动使用”时再加载它，JVM规范允许类加载器在预料某个类将要被使用时就预先加载它，如果在预先加载的过程中遇到了.class文件缺失或存在错误，类加载器必须在程序首次主动使用该类时才报告错误（LinkageError错误）如果这个类一直没有被程序主动使用，那么类加载器就不会报告错误。</p><p>加载.class文件的方式有:</p><p>1). 从本地系统中直接加载2). 通过网络下载.class文件3). 从zip，jar等归档文件中加载.class文件4). 从专有数据库中提取.class文件5). 将Java源文件动态编译为.class文件</p><p>在了解了什么是类的加载后，回头来再看jvm进行类加载阶段都做了什么。虚拟机需要完成以下三件事情：</p><p>1).通过一个类的全限定名称来获取定义此类的二进制字节流。</p><p>2).将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。</p><p>3).在java堆中生成一个代表这个类的java.lang.Class对象，作为方法区这些数据的访问入口。</p><p>相对于类加载过程的其他阶段，加载阶段是开发期相对来说可控性比较强，该阶段既可以使用系统提供的类加载器完成，也可以由用户自定义的类加载器来完成，开发人员可以通过定义自己的类加载器去控制字节流的获取方式。关于这个过程的更多细节，我会在下一节细说，类的加载。加载阶段完成后，虚拟机外部的 二进制字节流就按照虚拟机所需的格式存储在方法区之中，而且在Java堆中也创建一个java.lang.Class类的对象，这样便可以通过该对象访问方法区中的这些数据。</p><h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a><strong>验证</strong></h4><p>验证的目的是为了确保Class文件中的字节流包含的信息符合当前虚拟机的要求，而且不会危害虚拟机自身的安全。不同的虚拟机对类验证的实现可能会有所不同，但大致都会完成以下四个阶段的验证：文件格式的验证、元数据的验证、字节码验证和符号引用验证。</p><p>1）文件格式的验证：验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理，该验证的主要目的是保证输入的字节流能正确地解析并存储于方法区之内。经过该阶段的验证后，字节流才会进入内存的方法区中进行存储，后面的三个验证都是基于方法区的存储结构进行的。</p><p>2）元数据验证：对类的元数据信息进行语义校验（其实就是对类中的各数据类型进行语法校验），保证不存在不符合Java语法规范的元数据信息。</p><p>3）字节码验证：该阶段验证的主要工作是进行数据流和控制流分析，对类的方法体进行校验分析，以保证被校验的类的方法在运行时不会做出危害虚拟机安全的行为。</p><p>4）符号引用验证：这是最后一个阶段的验证，它发生在虚拟机将符号引用转化为直接引用的时候（解析阶段中发生该转化，后面会有讲解），主要是对类自身以外的信息（常量池中的各种符号引用）进行匹配性的校验。</p><h4 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h4><p>准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中进行分配。注：</p><p>1）这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在Java堆中。</p><p>2）这里所设置的初始值通常情况下是数据类型默认的零值（如0、0L、null、false等），而不是被在Java代码中被显式地赋予的值。</p><h4 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h4><p>解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。</p><p>符号引用（Symbolic Reference）：符号引用以一组符号来描述所引用的目标，符号引用可以是任何形式的字面量，符号引用与虚拟机实现的内存布局无关，引用的目标并不一定已经在内存中。</p><p>直接引用（Direct Reference）：直接引用可以是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。直接引用是与虚拟机实现的内存布局相关的，同一个符号引用在不同的虚拟机实例上翻译出来的直接引用一般都不相同，如果有了直接引用，那引用的目标必定已经在内存中存在。</p><p>1)、类或接口的解析：判断所要转化成的直接引用是对数组类型，还是普通的对象类型的引用，从而进行不同的解析。</p><p>2)、字段解析：对字段进行解析时，会先在本类中查找是否包含有简单名称和字段描述符都与目标相匹配的字段，如果有，则查找结束；如果没有，则会按照继承关系从上往下递归搜索该类所实现的各个接口和它们的父接口，还没有，则按照继承关系从上往下递归搜索其父类，直至查找结束。</p><p>3)、类方法解析：对类方法的解析与对字段解析的搜索步骤差不多，只是多了判断该方法所处的是类还是接口的步骤，而且对类方法的匹配搜索，是先搜索父类，再搜索接口。</p><p>4)、接口方法解析：与类方法解析步骤类似，只是接口不会有父类，因此，只递归向上搜索父接口就行了。</p><p><strong>5. 初始化</strong></p><p>类初始化阶段是类加载过程的最后一步，前面的类加载过程中，除了加载（Loading）阶段用户应用程序可以通过自定义类加载器参与之外，其余动作完全由虚拟机主导和控制。到了初始化阶段，才真正开始执行类中定义的Java程序代码。初始化，为类的静态变量赋予正确的初始值，JVM负责对类进行初始化，主要对类变量进行初始化。在Java中对类变量进行初始值设定有两种方式：</p><p>①声明类变量时指定初始值</p><p>②使用静态代码块为类变量指定初始值</p><p>JVM初始化步骤</p><p>1)、假如这个类还没有被加载和连接，则程序先加载并连接该类</p><p>2)、假如该类的直接父类还没有被初始化，则先初始化其直接父类</p><p>3)、假如类中有初始化语句，则系统依次执行这些初始化语句</p><p>初始化阶段时执行类构造器方法()的过程。</p><p>1）类构造器方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块（static{}块）中的语句合并产生的，编译器收集的顺序由语句在源文件中出现的顺序所决定。</p><p>2）类构造器方法与类的构造函数不同，它不需要显式地调用父类构造器，虚拟机会保证在子类的类构造器方法执行之前，父类的类构造器方法已经执行完毕，因此在虚拟机中第一个执行的类构造器方法的类一定是java.lang.Object。</p><p>3）由于父类的类构造器方法方法先执行，也就意味着父类中定义的静态语句块要优先于子类的变量赋值操作。</p><p>4）类构造器方法对于类或者接口来说并不是必需的，如果一个类中没有静态语句块也没有对变量的赋值操作，那么编译器可以不为这个类生成类构造器方法。</p><p>5）接口中可能会有变量赋值操作，因此接口也会生成类构造器方法。但是接口与类不同，执行接口的类构造器方法不需要先执行父接口的类构造器方法。只有当父接口中定义的变量被使用时，父接口才会被初始化。另外，接口的实现类在初始化时也不会执行接口的类构造器方法。</p><p>6）虚拟机会保证一个类的类构造器方法在多线程环境中被正确地加锁和同步。如果有多个线程去同时初始化一个类，那么只会有一个线程去执行这个类的类构造器方法，其它线程都需要阻塞等待，直到活动线程执行类构造器方法完毕。如果在一个类的类构造器方法中有耗时很长的操作，那么就可能造成多个进程阻塞。</p><p><strong>6.结束生命周期</strong></p><p>在以下情况的时候，Java虚拟机会结束生命周期1). 执行了System.exit()方法2). 程序正常执行结束3). 程序在执行过程中遇到了异常或错误而异常终止4). 由于操作系统出现错误而导致Java虚拟机进程终止</p><h3 id="何时开始类的初始化"><a href="#何时开始类的初始化" class="headerlink" title="何时开始类的初始化"></a>何时开始类的初始化</h3><p>什么情况下需要开始类加载过程的第一个阶段:”加载”。虚拟机规范中并没强行约束，这点可以交给虚拟机的的具体实现自由把握，但是对于初始化阶段虚拟机规范是严格规定了如下几种情况，如果类未初始化会对类进行初始化。</p><p>1、创建类的实例</p><p>2、访问类的静态变量(除常量【被final修辞的静态变量】原因:常量一种特殊的变量，因为编译器把他们当作值(value)而不是域(field)来对待。如果你的代码中用到了常变量(constant variable)，编译器并不会生成字节码来从对象中载入域的值，而是直接把这个值插入到字节码中。这是一种很有用的优化，但是如果你需要改变final域的值那么每一块用到那个域的代码都需要重新编译。</p><p>3、访问类的静态方法</p><p>4、反射如(Class.forName(“my.xyz.Test”))</p><p>5、当初始化一个类时，发现其父类还未初始化，则先出发父类的初始化</p><p>6、虚拟机启动时，定义了main()方法的那个类先初始化</p><p>以上情况称为称对一个类进行“主动引用”，除此种情况之外，均不会触发类的初始化，称为“被动引用”接口的加载过程与类的加载过程稍有不同。接口中不能使用static{}块。当一个接口在初始化时，并不要求其父接口全部都完成了初始化，只有真正在使用到父接口时（例如引用接口中定义的常量）才会初始化。</p><h4 id="被动引用例子"><a href="#被动引用例子" class="headerlink" title="被动引用例子"></a><strong>被动引用例子</strong></h4><p>1、子类调用父类的静态变量，子类不会被初始化。只有父类被初始化。。对于静态字段，只有直接定义这个字段的类才会被初始化.</p><p>2、通过数组定义来引用类，不会触发类的初始化</p><p>3、 访问类的常量，不会初始化类</p><pre><code>class SuperClass {      static {          System.out.println(&quot;superclass init&quot;);      }      public static int value = 123;  }  class SubClass extends SuperClass {      static {          System.out.println(&quot;subclass init&quot;);      }  }  public class Test {      public static void main(String[] args) {          System.out.println(SubClass.value);// 被动应用1          SubClass[] sca = new SubClass[10];// 被动引用2      }  }  </code></pre><p>程序运行输出    superclass init123从上面的输入结果证明了被动引用1与被动引用2</p><pre><code>class ConstClass {      static {          System.out.println(&quot;ConstClass init&quot;);      }      public static final String HELLOWORLD = &quot;hello world&quot;;  }  public class Test {      public static void main(String[] args) {          System.out.println(ConstClass.HELLOWORLD);// 调用类常量      }  }  </code></pre><p>程序输出结果hello world从上面的输出结果证明了被动引用3</p><p>** 题目分析**</p><p>上面很详细的介绍了类的加载时机和类的加载过程，通过上面的理论来分析本文开门见上的题目</p><pre><code>class SingleTon {      private static SingleTon singleTon = new SingleTon();      public static int count1;      public static int count2 = 0;      private SingleTon() {          count1++;          count2++;      }      public static SingleTon getInstance() {          return singleTon;      }  }  public class Test {      public static void main(String[] args) {          SingleTon singleTon = SingleTon.getInstance();          System.out.println(&quot;count1=&quot; + singleTon.count1);          System.out.println(&quot;count2=&quot; + singleTon.count2);      }  }  </code></pre><p>分析:</p><p>1:SingleTon singleTon = SingleTon.getInstance();调用了类的SingleTon调用了类的静态方法，触发类的初始化</p><p>2:类加载的时候在准备过程中为类的静态变量分配内存并初始化默认值 singleton=null count1=0,count2=0</p><p>3:类初始化化，为类的静态变量赋值和执行静态代码快。singleton赋值为new SingleTon()调用类的构造方法</p><p>4:调用类的构造方法后count=1;count2=1</p><p>5:继续为count1与count2赋值,此时count1没有赋值操作,所有count1为1,但是count2执行赋值操作就变为0</p><h3 id="类初始化顺序"><a href="#类初始化顺序" class="headerlink" title="类初始化顺序"></a>类初始化顺序</h3><p>现在我们知道什么时候触发类的初始化了，他精确地写在Java语言规范中。但了解清楚 域（fields，静态的还是非静态的）、块（block静态的还是非静态的）、不同类（子类和超类）和不同的接口（子接口，实现类和超接口）的初始化顺序也很重要类。事实上很多核心Java面试题和SCJP问题都是基于这些概念，下面是类初始化的一些规则：</p><pre><code>1.类从顶至底的顺序初始化，所以声明在顶部的字段的早于底部的字段初始化2.超类早于子类和衍生类的初始化3.如果类的初始化是由于访问静态域而触发，那么只有声明静态域的类才被初始化，而不会触发超类的初始化或者子类的4.初始化即使静态域被子类或子接口或者它的实现类所引用。5.接口初始化不会导致父接口的初始化。6.静态域的初始化是在类的静态初始化期间，非静态域的初始化时在类的实例创建期间。这意味这静态域初始化在非静态域之前。7.非静态域通过构造器初始化，子类在做任何初始化之前构造器会隐含地调用父类的构造器，他保证了非静态或实例变量（父类）初始化早于子类</code></pre><h3 id="类加载器"><a href="#类加载器" class="headerlink" title="类加载器"></a>类加载器</h3><p>JVM设计者把类加载阶段中的“通过’类全名’来获取定义此类的二进制字节流”这个动作放到Java虚拟机外部去实现，以便让应用程序自己决定如何去获取所需要的类。实现这个动作的代码模块称为“类加载器”。</p><h4 id="类与类加载器"><a href="#类与类加载器" class="headerlink" title="类与类加载器"></a><strong>类与类加载器</strong></h4><p>对于任何一个类，都需要由加载它的类加载器和这个类来确立其在JVM中的唯一性。也就是说，两个类来源于同一个Class文件，并且被同一个类加载器加载，这两个类才相等。</p><h4 id="双亲委派模型"><a href="#双亲委派模型" class="headerlink" title="双亲委派模型"></a><strong>双亲委派模型</strong></h4><p>从虚拟机的角度来说，只存在两种不同的类加载器：一种是启动类加载器（Bootstrap ClassLoader），该类加载器使用C++语言实现，属于虚拟机自身的一部分。另外一种就是所有其它的类加载器，这些类加载器是由Java语言实现，独立于JVM外部，并且全部继承自抽象类java.lang.ClassLoader。</p><p>从Java开发人员的角度来看，大部分Java程序一般会使用到以下三种系统提供的类加载器：</p><p>1)启动类加载器（Bootstrap ClassLoader）：负责加载JAVA_HOME\lib目录中并且能被虚拟机识别的类库到JVM内存中，如果名称不符合的类库即使放在lib目录中也不会被加载。该类加载器无法被Java程序直接引用。</p><p>2)扩展类加载器（Extension ClassLoader）：该加载器主要是负责加载JAVA_HOME\lib\，该加载器可以被开发者直接使用。</p><p>3)应用程序类加载器（Application ClassLoader）：该类加载器也称为系统类加载器，它负责加载用户类路径（Classpath）上所指定的类库，开发者可以直接使用该类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。</p><p>我们的应用程序都是由这三类加载器互相配合进行加载的，我们也可以加入自己定义的类加载器。这些类加载器之间的关系如下图所示：</p><p>如上图所示的类加载器之间的这种层次关系，就称为类加载器的双亲委派模型（Parent Delegation Model）。该模型要求除了顶层的启动类加载器外，其余的类加载器都应当有自己的父类加载器。子类加载器和父类加载器不是以继承（Inheritance）的关系来实现，而是通过组合（Composition）关系来复用父加载器的代码。</p><p>双亲委派模型的工作过程为：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的加载器都是如此，因此所有的类加载请求都会传给顶层的启动类加载器，只有当父加载器反馈自己无法完成该加载请求（该加载器的搜索范围中没有找到对应的类）时，子加载器才会尝试自己去加载。</p><p>使用这种模型来组织类加载器之间的关系的好处是Java类随着它的类加载器一起具备了一种带有优先级的层次关系。例如java.lang.Object类，无论哪个类加载器去加载该类，最终都是由启动类加载器进行加载，因此Object类在程序的各种类加载器环境中都是同一个类。否则的话，如果不使用该模型的话，如果用户自定义一个java.lang.Object类且存放在classpath中，那么系统中将会出现多个Object类，应用程序也会变得很混乱。如果我们自定义一个rt.jar中已有类的同名Java类，会发现JVM可以正常编译，但该类永远无法被加载运行。在rt.jar包中的java.lang.ClassLoader类中，我们可以查看类加载实现过程的代码，具体源码如下：</p><pre><code>protected synchronized Class loadClass(String name, boolean resolve)          throws ClassNotFoundException {      // 首先检查该name指定的class是否有被加载      Class c = findLoadedClass(name);      if (c == null) {          try {              if (parent != null) {                  // 如果parent不为null，则调用parent的loadClass进行加载                  c = parent.loadClass(name, false);              } else {                  // parent为null，则调用BootstrapClassLoader进行加载                  c = findBootstrapClass0(name);              }          } catch (ClassNotFoundException e) {              // 如果仍然无法加载成功，则调用自身的findClass进行加载              c = findClass(name);          }      }      if (resolve) {          resolveClass(c);      }      return c;  }  </code></pre><p>通过上面代码可以看出，双亲委派模型是通过loadClass()方法来实现的，根据代码以及代码中的注释可以很清楚地了解整个过程其实非常简单：先检查是否已经被加载过，如果没有则调用父加载器的loadClass()方法，如果父加载器为空则默认使用启动类加载器作为父加载器。如果父类加载器加载失败，则先抛出ClassNotFoundException，然后再调用自己的findClass()方法进行加载。</p><h4 id="自定义类加载器"><a href="#自定义类加载器" class="headerlink" title="自定义类加载器"></a><strong>自定义类加载器</strong></h4><p>若要实现自定义类加载器，只需要继承java.lang.ClassLoader 类，并且重写其findClass()方法即可。java.lang.ClassLoader 类的基本职责就是根据一个指定的类的名称，找到或者生成其对应的字节代码，然后从这些字节代码中定义出一个 Java 类，即 java.lang.Class 类的一个实例。除此之外，ClassLoader 还负责加载 Java 应用所需的资源，如图像文件和配置文件等，ClassLoader 中与加载类相关的方法如下：</p><p>方法说明getParent()  返回该类加载器的父类加载器。</p><p>loadClass(String name) 加载名称为 二进制名称为name 的类，返回的结果是 java.lang.Class 类的实例。</p><p>findClass(String name) 查找名称为 name 的类，返回的结果是 java.lang.Class 类的实例。</p><p>findLoadedClass(String name) 查找名称为 name 的已经被加载过的类，返回的结果是 java.lang.Class 类的实例。</p><p>resolveClass(Class&lt;?&gt; c) 链接指定的 Java 类。</p><p>注意：在JDK1.2之前，类加载尚未引入双亲委派模式，因此实现自定义类加载器时常常重写loadClass方法，提供双亲委派逻辑，从JDK1.2之后，双亲委派模式已经被引入到类加载体系中，自定义类加载器时不需要在自己写双亲委派的逻辑，因此不鼓励重写loadClass方法，而推荐重写findClass方法。</p><p>在Java中，任意一个类都需要由加载它的类加载器和这个类本身一同确定其在java虚拟机中的唯一性，即比较两个类是否相等，只有在这两个类是由同一个类加载器加载的前提之下才有意义，否则，即使这两个类来源于同一个Class类文件，只要加载它的类加载器不相同，那么这两个类必定不相等(这里的相等包括代表类的Class对象的equals()方法、isAssignableFrom()方法、isInstance()方法和instanceof关键字的结果)。例子代码如下：</p><pre><code>/**      * 一、ClassLoader加载类的顺序      *  1.调用 findLoadedClass(String) 来检查是否已经加载类。      *  2.在父类加载器上调用 loadClass 方法。如果父类加载器为 null，则使用虚拟机的内置类加载器。      *  3.调用 findClass(String) 方法查找类。      * 二、实现自己的类加载器      *  1.获取类的class文件的字节数组      *  2.将字节数组转换为Class类的实例      */      public class ClassLoaderTest {          public static void main(String[] args) throws InstantiationException, IllegalAccessException, ClassNotFoundException {              //新建一个类加载器              MyClassLoader cl = new MyClassLoader(&quot;myClassLoader&quot;);              //加载类，得到Class对象              Class&lt;?&gt; clazz = cl.loadClass(&quot;classloader.Animal&quot;);              //得到类的实例              Animal animal=(Animal) clazz.newInstance();              animal.say();          }      }      class Animal{          public void say(){              System.out.println(&quot;hello world!&quot;);          }      }      class MyClassLoader extends ClassLoader {          //类加载器的名称          private String name;          //类存放的路径          private String path = &quot;E:\\workspace\\Algorithm\\src&quot;;          MyClassLoader(String name) {              this.name = name;          }          MyClassLoader(ClassLoader parent, String name) {              super(parent);              this.name = name;          }          /**          * 重写findClass方法          */          @Override          public Class&lt;?&gt; findClass(String name) {              byte[] data = loadClassData(name);              return this.defineClass(name, data, 0, data.length);          }          public byte[] loadClassData(String name) {              try {                  name = name.replace(&quot;.&quot;, &quot;//&quot;);                  FileInputStream is = new FileInputStream(new File(path + name + &quot;.class&quot;));                  ByteArrayOutputStream baos = new ByteArrayOutputStream();                  int b = 0;                  while ((b = is.read()) != -1) {                      baos.write(b);                  }                  return baos.toByteArray();              } catch (Exception e) {                  e.printStackTrace();              }              return null;          }      }  </code></pre><p>类加载器双亲委派模型是从JDK1.2以后引入的，并且只是一种推荐的模型，不是强制要求的，因此有一些没有遵循双亲委派模型的特例：(了解)</p><p>(1).在JDK1.2之前，自定义类加载器都要覆盖loadClass方法去实现加载类的功能，JDK1.2引入双亲委派模型之后，loadClass方法用于委派父类加载器进行类加载，只有父类加载器无法完成类加载请求时才调用自己的findClass方法进行类加载，因此在JDK1.2之前的类加载的loadClass方法没有遵循双亲委派模型，因此在JDK1.2之后，自定义类加载器不推荐覆盖loadClass方法，而只需要覆盖findClass方法即可。</p><p>(2).双亲委派模式很好地解决了各个类加载器的基础类统一问题，越基础的类由越上层的类加载器进行加载，但是这个基础类统一有一个不足，当基础类想要调用回下层的用户代码时无法委派子类加载器进行类加载。为了解决这个问题JDK引入了ThreadContext线程上下文，通过线程上下文的setContextClassLoader方法可以设置线程上下文类加载器。</p><p>JavaEE只是一个规范，sun公司只给出了接口规范，具体的实现由各个厂商进行实现，因此JNDI，JDBC,JAXB等这些第三方的实现库就可以被JDK的类库所调用。线程上下文类加载器也没有遵循双亲委派模型。</p><p>(3).近年来的热码替换，模块热部署等应用要求不用重启java虚拟机就可以实现代码模块的即插即用，催生了OSGi技术，在OSGi中类加载器体系被发展为网状结构。OSGi也没有完全遵循双亲委派模型。</p><h4 id="动态加载Jar-amp-amp-ClassLoader-隔离问题"><a href="#动态加载Jar-amp-amp-ClassLoader-隔离问题" class="headerlink" title="动态加载Jar &amp;&amp; ClassLoader 隔离问题"></a><strong>动态加载Jar &amp;&amp; ClassLoader 隔离问题</strong></h4><p>动态加载Jar：</p><p>Java 中动态加载 Jar 比较简单，如下：</p><pre><code>URL[] urls = new URL[] {new URL(&quot;file:libs/jar1.jar&quot;)};  URLClassLoader loader = new URLClassLoader(urls, parentLoader);  </code></pre><p>表示加载 libs 下面的 jar1.jar，其中 parentLoader 就是上面1中的 parent，可以为当前的 ClassLoader。</p><p>ClassLoader 隔离问题：</p><p>大家觉得一个运行程序中有没有可能同时存在两个包名和类名完全一致的类？JVM 及 Dalvik 对类唯一的识别是 ClassLoader id + PackageName + ClassName，所以一个运行程序中是有可能存在两个包名和类名完全一致的类的。并且如果这两个”类”不是由一个 ClassLoader 加载，是无法将一个类的示例强转为另外一个类的，这就是 ClassLoader 隔离。 如 Android 中碰到如下异常[java] view plain copy</p><pre><code>android.support.v4.view.ViewPager can not be cast to android.support.v4.view.ViewPager  </code></pre><p>当碰到这种问题时可以通过 instance.getClass().getClassLoader(); 得到 ClassLoader，看 ClassLoader 是否一样。</p><p>加载不同 Jar 包中公共类：</p><p>现在 Host 工程包含了 common.jar, jar1.jar, jar2.jar，并且 jar1.jar 和 jar2.jar 都包含了 common.jar，我们通过 ClassLoader 将 jar1, jar2 动态加载进来，这样在 Host 中实际是存在三份 common.jar.</p><p>我们怎么保证 common.jar 只有一份而不会造成上面3中提到的 ClassLoader 隔离的问题呢，其实很简单，在生成 jar1 和 jar2 时把 common.jar 去掉，只保留 host 中一份，以 host ClassLoader 为 parentClassLoader 即可。</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>动态代理的实现方式与区别</title>
      <link href="/2020/01/06/dong-tai-dai-li-de-shi-xian-fang-shi-yu-qu-bie/"/>
      <url>/2020/01/06/dong-tai-dai-li-de-shi-xian-fang-shi-yu-qu-bie/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是代理"><a href="#什么是代理" class="headerlink" title="什么是代理"></a>什么是代理</h3><p>我个人理解代理就是让A类可以去调用B类的方法,从而可以在调用前后加上一些逻辑</p><p>理解了代理的意思，你脑海中恐怕还有两个巨大的疑问：</p><ul><li>怎么实现代理模式</li><li>代理模式有什么实际用途</li></ul><p>要理解这两个问题，看一个简单的例子：</p><pre><code>public interface Flyable {    void fly();}public class Bird implements Flyable {    @Override    public void fly() {        System.out.println(&quot;Bird is flying...&quot;);        try {            Thread.sleep(new Random().nextInt(1000));        } catch (InterruptedException e) {            e.printStackTrace();        }    }}</code></pre><p>很简单的一个例子，用一个随机睡眠时间模拟小鸟在空中的飞行时间。接下来问题来了，如果我要知道小鸟在天空中飞行了多久，怎么办？</p><p>有人说，很简单，在Bird-&gt;fly()方法的开头记录起始时间，在方法结束记录完成时间，两个时间相减就得到了飞行时间。</p><pre><code>   @Override    public void fly() {        long start = System.currentTimeMillis();        System.out.println(&quot;Bird is flying...&quot;);        try {            Thread.sleep(new Random().nextInt(1000));        } catch (InterruptedException e) {            e.printStackTrace();        }        long end = System.currentTimeMillis();        System.out.println(&quot;Fly time = &quot; + (end - start));    }</code></pre><p>的确，这个方法没有任何问题，接下来加大问题的难度。如果Bird这个类来自于某个SDK（或者说Jar包）提供，你无法改动源码，怎么办？</p><p>一定会有人说，我可以在调用的地方这样写：</p><pre><code>public static void main(String[] args) {        Bird bird = new Bird();        long start = System.currentTimeMillis();        bird.fly();        long end = System.currentTimeMillis();        System.out.println(&quot;Fly time = &quot; + (end - start));}</code></pre><p>这个方案看起来似乎没有问题，但其实你忽略了准备这些方法所需要的时间，执行一个方法，需要开辟栈内存、压栈、出栈等操作，这部分时间也是不可以忽略的。因此，这个解决方案不可行。那么，还有什么方法可以做到呢？</p><p><strong>使用继承</strong></p><p>继承是最直观的解决方案，相信你已经想到了，至少我最开始想到的解决方案就是继承。为此，我们重新创建一个类Bird2，在Bird2中我们只做一件事情，就是调用父类的fly方法，在前后记录时间，并打印时间差：</p><pre><code>public class Bird2 extends Bird {    @Override    public void fly() {        long start = System.currentTimeMillis();        super.fly();        long end = System.currentTimeMillis();        System.out.println(&quot;Fly time = &quot; + (end - start));    }}</code></pre><p>这是一种解决方案，还有一种解决方案叫做：<strong>聚合</strong>，其实也是比较容易想到的。我们再次创建新类Bird3，在Bird3的构造方法中传入Bird实例。同时，让Bird3也实现Flyable接口，并在fly方法中调用传入的Bird实例的fly方法：</p><pre><code>public class Bird3 implements Flyable {    private Bird bird;    public Bird3(Bird bird) {        this.bird = bird;    }    @Override    public void fly() {        long start = System.currentTimeMillis();        bird.fly();        long end = System.currentTimeMillis();        System.out.println(&quot;Fly time = &quot; + (end - start));    }}</code></pre><p>为了记录Bird-&gt;fly()方法的执行时间，我们在前后添加了记录时间的代码。同样地，通过这种方法我们也可以获得小鸟的飞行时间。那么，这两种方法孰优孰劣呢？咋一看，不好评判！</p><p>继续深入思考，用问题推导来解答这个问题：</p><p><strong>问题一</strong>：如果我还需要在fly方法前后打印日志，记录飞行开始和飞行结束，怎么办？有人说，很简单！继承Bird2并在在前后添加打印语句即可。那么，问题来了，请看问题二。</p><p><strong>问题二</strong>：如果我需要调换执行顺序，先打印日志，再获取飞行时间，怎么办？有人说，再新建一个类Bird4继承Bird，打印日志。再新建一个类Bird5继承Bird4，获取方法执行时间。</p><p>问题显而易见：使用继承将导致类无限制扩展，同时灵活性也无法获得保障。那么，使用 <a href="https://stackoverflow.com/questions/885937/what-is-the-difference-between-association-aggregation-and-composition" target="_blank" rel="noopener">聚合</a> 是否可以避免这个问题呢？答案是：可以！但我们的类需要稍微改造一下。修改Bird3类，将聚合对象Bird类型修改为Flyable</p><pre><code>public class Bird3 implements Flyable {    private Flyable flyable;    public Bird3(Flyable flyable) {        this.flyable = flyable;    }    @Override    public void fly() {        long start = System.currentTimeMillis();        flyable.fly();        long end = System.currentTimeMillis();        System.out.println(&quot;Fly time = &quot; + (end - start));    }}</code></pre><p>为了让你看的更清楚，我将Bird3更名为BirdTimeProxy，即用于获取方法执行时间的代理的意思。同时我们新建BirdLogProxy代理类用于打印日志：</p><pre><code>public class BirdLogProxy implements Flyable {    private Flyable flyable;    public BirdLogProxy(Flyable flyable) {        this.flyable = flyable;    }    @Override    public void fly() {        System.out.println(&quot;Bird fly start...&quot;);        flyable.fly();        System.out.println(&quot;Bird fly end...&quot;);    }}</code></pre><p>接下来神奇的事情发生了，如果我们需要先记录日志，再获取飞行时间，可以在调用的地方这么做：</p><pre><code>    public static void main(String[] args) {        Bird bird = new Bird();        BirdLogProxy p1 = new BirdLogProxy(bird);        BirdTimeProxy p2 = new BirdTimeProxy(p1);        p2.fly();    }</code></pre><p>反过来，可以这么做：</p><pre><code> public static void main(String[] args) {        Bird bird = new Bird();        BirdTimeProxy p2 = new BirdTimeProxy(bird);        BirdLogProxy p1 = new BirdLogProxy(p2);        p1.fly(); }</code></pre><p>看到这里，有同学可能会有疑问了。虽然现象看起来，聚合可以灵活调换执行顺序。可是，为什么 <a href="https://stackoverflow.com/questions/885937/what-is-the-difference-between-association-aggregation-and-composition" target="_blank" rel="noopener">聚合</a> 可以做到，而继承不行呢。我们用一张图来解释一下：</p><p><img src="/2020/01/06/dong-tai-dai-li-de-shi-xian-fang-shi-yu-qu-bie/1.png" alt></p><h3 id="静态代理"><a href="#静态代理" class="headerlink" title="静态代理"></a>静态代理</h3><p>接下来，观察上面的类BirdTimeProxy，在它的fly方法中我们直接调用了flyable-&gt;fly()方法。换而言之，BirdTimeProxy其实代理了传入的Flyable对象，这就是典型的静态代理实现。</p><p>从表面上看，静态代理已经完美解决了我们的问题。可是，试想一下，如果我们需要计算SDK中100个方法的运行时间，同样的代码至少需要重复100次，并且创建至少100个代理类。往小了说，如果Bird类有多个方法，我们需要知道其他方法的运行时间，同样的代码也至少需要重复多次。因此，静态代理至少有以下两个局限性问题：</p><ul><li>如果同时代理多个类，依然会导致类无限制扩展</li><li>如果类中有多个方法，同样的逻辑需要反复实现</li></ul><p>那么，我们是否可以使用同一个代理类来代理任意对象呢？我们以获取方法运行时间为例，是否可以使用同一个类（例如：TimeProxy）来计算任意对象的任一方法的执行时间呢？甚至再大胆一点，代理的逻辑也可以自己指定。比如，获取方法的执行时间，打印日志，这类逻辑都可以自己指定。这就是本文重点探讨的问题，也是最难理解的部分：<strong>动态代理</strong>。</p><h3 id="使用动态代理"><a href="#使用动态代理" class="headerlink" title="使用动态代理"></a>使用动态代理</h3><h3 id="2-1-InvocationHandler接口"><a href="#2-1-InvocationHandler接口" class="headerlink" title="2.1 InvocationHandler接口"></a>2.1 InvocationHandler接口</h3><p>在使用动态代理时，我们需要定义一个位于代理类与委托类之间的中介类，这个中介类被要求实现InvocationHandler接口，这个接口的定义如下：</p><pre><code>/** * 调用处理程序 */public interface InvocationHandler {     Object invoke(Object proxy, Method method, Object[] args); } </code></pre><p>从InvocationHandler这个名称我们就可以知道，实现了这个接口的中介类用做“调用处理器”。当我们调用代理类对象的方法时，这个“调用”会转送到invoke方法中，代理类对象作为proxy参数传入，参数method标识了我们具体调用的是代理类的哪个方法，args为这个方法的参数。这样一来，我们对代理类中的所有方法的调用都会变为对invoke的调用，这样我们可以在invoke方法中添加统一的处理逻辑(也可以根据method参数对不同的代理类方法做不同的处理)。因此我们只需在中介类的invoke方法实现中输出“before”，然后调用委托类的invoke方法，再输出“after”。下面我们来一步一步具体实现它。</p><h3 id="2-2-委托类的定义"><a href="#2-2-委托类的定义" class="headerlink" title="2.2 委托类的定义"></a>2.2 委托类的定义</h3><p>动态代理方式下，要求委托类必须实现某个接口，这里我们实现的是Flyable接口。委托类Bird类的定义如下：</p><pre><code>public interface Flyable {    void fly();}public class Bird implements Flyable {    @Override    public void fly() {        System.out.println(&quot;Bird is flying...&quot;);        try {            Thread.sleep(new Random().nextInt(1000));        } catch (InterruptedException e) {            e.printStackTrace();        }    }}</code></pre><h3 id="2-3中介类"><a href="#2-3中介类" class="headerlink" title="2.3中介类"></a>2.3中介类</h3><p>上面我们提到过，中介类必须实现InvocationHandler接口，作为调用处理器”拦截“对代理类方法的调用。中介类的定义如下：</p><pre><code>public class DynamicProxy implements InvocationHandler {     //obj为委托类对象;     private Object obj;     public DynamicProxy(Object obj) {        this.obj = obj;    }     @Override     public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {         System.out.println(&quot;Bird fly start...&quot;);        Object result = method.invoke(obj, args);         System.out.println(&quot;Bird fly end...&quot;);        return result;     }} </code></pre><p>从以上代码中我们可以看到，中介类持有一个委托类对象引用，在invoke方法中调用了委托类对象的相应方法，看到这里是不是觉得似曾相识?</p><p>通过聚合方式持有委托类对象引用，把外部对invoke的调用最终都转为对委托类对象的调用。这不就是我们上面介绍的静态代理的一种实现方式吗?</p><p>实际上，中介类与委托类构成了静态代理关系，在这个关系中，中介类是代理类，委托类就是委托类;</p><p>代理类与中介类也构成一个静态代理关系，在这个关系中，中介类是委托类，代理类是代理类。</p><p>也就是说，<strong>动态代理关系由两组静态代理关系组成，这就是动态代理的原理</strong>。下面我们来介绍一下如何”指示“以动态生成代理类。</p><h3 id="2-4动态生成代理类"><a href="#2-4动态生成代理类" class="headerlink" title="2.4动态生成代理类"></a>2.4动态生成代理类</h3><p>动态生成代理类的相关代码如下：</p><pre><code>public class Main {     public static void main(String[] args) {           Bird bird =  Proxy.newProxyInstance(Flyable.class,Bird.getClass().getClassLoader(), new MyInvocationHandler(new Bird()));        bird.fly();    }} </code></pre><p>在以上代码中，我们调用Proxy类的newProxyInstance方法来获取一个代理类实例。这个代理类实现了我们指定的接口并且会把方法调用分发到指定的调用处理器。这个方法的声明如下：</p><pre><code>public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException </code></pre><p>方法的三个参数含义分别如下：</p><p>loader：定义了代理类的ClassLoder;interfaces：代理类实现的接口列表h：调用处理器，也就是我们上面定义的实现了InvocationHandler接口的类实例</p><p><strong>上面我们已经简单提到过动态代理的原理，这里再简单的总结下：首先通过newProxyInstance方法获取代理类实例，而后我们便可以通过这个代理类实例调用代理类的方法，对代理类的方法的调用实际上都会调用中介类(调用处理器)的invoke方法，在invoke方法中我们调用委托类的相应方法，并且可以添加自己的处理逻辑。</strong></p><p>想象一下，到此为止，如果我们还需要对其它任意对象进行代理，是否还需要改动newProxyInstance方法的源码，答案是：完全不需要！</p><p>只要你在newProxyInstance方法中指定代理需要实现的接口，指定用于自定义处理的InvocationHandler对象，整个代理的逻辑处理都在你自定义的InvocationHandler实现类中进行处理。至此，而我们终于可以从不断地写代理类用于实现自定义逻辑的重复工作中解放出来了，从此需要做什么，交给InvocationHandler。</p><p>事实上，我们之前给自己定下的目标“使用同一个类来计算任意对象的任一方法的执行时间”已经实现了。严格来说，是我们超额完成了任务，TimeProxy不仅可以计算方法执行的时间，也可以打印方法执行日志，这完全取决于你的InvocationHandler接口实现。因此，这里取名为TimeProxy其实已经不合适了。</p><h4 id="动态代理的实现方式与区别"><a href="#动态代理的实现方式与区别" class="headerlink" title="动态代理的实现方式与区别"></a>动态代理的实现方式与区别</h4><h5 id="jDK代理"><a href="#jDK代理" class="headerlink" title="jDK代理"></a>jDK代理</h5><p> JDK的动态代理主要涉及到java.lang.reflect包中的两个类：Proxy和InvocationHandler。其中 InvocationHandler是一个接口就是拦截器的接口。，可以通过实现该接口定义横切逻辑，并通过反射机制调用目标类的代码，动态将横切逻辑和业务逻辑编织在一起，上面的实现就是通过JDK代理。</p><p><strong>InvocationHandler的作用</strong></p><p>在动态代理中InvocationHandler是核心，每个代理实例都具有一个关联的调用处理程序(InvocationHandler)。对代理实例调用方法时，将对方法调用进行编码并将其指派到它的调用处理程序(InvocationHandler)的 invoke 方法。所以对代理方法的调用都是通InvocationHadler的invoke来实现中，而invoke方法根据传入的代理对象，方法和参数来决定调用代理的哪个方法</p><h5 id="代理模式"><a href="#代理模式" class="headerlink" title="代理模式"></a><strong>代理模式</strong></h5><p>使用代理模式必须要让代理类和目标类实现相同的接口，客户端通过代理类来调用目标方法，代理类会将所有的方法调用分派到目标对象上反射执行，还可以在分派过程中添加”前置通知”和后置处理（如在调用目标方法前校验权限，在调用完目标方法后打印日志等）等功能。</p><p>具体有如下四步骤：</p><p>1.通过实现 InvocationHandler 接口创建自己的调用处理器；</p><p>2.通过为 Proxy 类指定 ClassLoader 对象和一组 interface 来创建动态代理类；</p><p>3.通过反射机制获得动态代理类的构造函数，其唯一参数类型是调用处理器接口类型；</p><p>4.通过构造函数创建动态代理类实例，构造时调用处理器对象作为参数被传入。</p><h4 id="cglib代理"><a href="#cglib代理" class="headerlink" title="cglib代理"></a>cglib代理</h4><p>CGlib是一个强大的,高性能,高质量的Code生成类库。cglib封装了asm，可以在运行期动态生成新的class，它可以在运行期扩展Java类与实现Java接口。 CGLIB是<strong>针对类实现代理</strong>的，主要对指定的类生成一个子类，并覆盖其中的方法， 因为是继承，所以不能使用final来修饰类或方法。和jdk代理实现不同的是，cglib不要求类实现接口。</p><p>JDK动态代理和CGLIB字节码生成的区别？</p><p>CGLib所创建的动态代理对象的性能比JDK的高大概10倍，但CGLib在创建代理对象的时间比JDK大概多8倍，所以对于singleton的代理对象或者具有实例池的代理，因为无需重复的创建代理对象，所以比较适合CGLib动态代理技术，反之选择JDK代理</p><ul><li><p>JDK动态代理只能对实现了接口的类生成代理，而不能针对类</p></li><li><p>CGLIB是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法</p><p>因为是继承，所以该类或方法最好不要声明成final </p></li></ul><h3 id="答疑解惑"><a href="#答疑解惑" class="headerlink" title="答疑解惑"></a>答疑解惑</h3><h4 id="invoke方法的第一个参数proxy到底有什么作用？"><a href="#invoke方法的第一个参数proxy到底有什么作用？" class="headerlink" title="invoke方法的第一个参数proxy到底有什么作用？"></a>invoke方法的第一个参数proxy到底有什么作用？</h4><p>这个问题其实也好理解，如果你的接口中有方法需要返回自身，如果在invoke中没有传入这个参数，将导致实例无法正常返回。在这种场景中，proxy的用途就表现出来了。简单来说，这其实就是最近非常火的链式编程的一种应用实现。</p><h4 id="动态代理到底有什么用？"><a href="#动态代理到底有什么用？" class="headerlink" title="动态代理到底有什么用？"></a>动态代理到底有什么用？</h4><p>学习任何一门技术，一定要问一问自己，这到底有什么用。其实，在这篇文章的讲解过程中，我们已经说出了它的主要用途。你发现没，使用动态代理我们居然可以在不改变源码的情况下，直接在方法中插入自定义逻辑。这有点不太符合我们的一条线走到底的编程逻辑，这种编程模型有一个专业名称叫 <a href="https://baike.baidu.com/item/AOP/1332219" target="_blank" rel="noopener">AOP</a>。所谓的AOP，就像刀一样，抓住时机，趁机插入。</p><p>基于这样一种动态特性，我们可以用它做很多事情，例如：</p><ul><li>事务提交或回退（Web开发中很常见）</li><li>权限管理</li><li>自定义缓存逻辑处理</li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>java异常</title>
      <link href="/2020/01/05/java-yi-chang/"/>
      <url>/2020/01/05/java-yi-chang/</url>
      
        <content type="html"><![CDATA[<h2 id="Java异常介绍"><a href="#Java异常介绍" class="headerlink" title="Java异常介绍"></a>Java异常介绍</h2><p>异常时什么？就是指阻止当前方法或作用域继续执行的问题,当程序运行时出现异常时,系统就会自动生成一个Exception对象来通知程序进行相应的处理。Java异常的类型有很多种，下面我们就使用一张图来看一下Java异常的继承层次结构：</p><p><img src="/2020/01/05/java-yi-chang/1.png" alt></p><h3 id="Java异常中的Error"><a href="#Java异常中的Error" class="headerlink" title="Java异常中的Error"></a>Java异常中的Error</h3><p>Error一般表示编译时或者系统错误，例如：虚拟机相关的错误，系统崩溃（例如：我们开发中有时会遇到的OutOfMemoryError）等。这种错误无法恢复或不可捕获,将导致应用程序中断,通常应用程序无法处理这些错误,因此也不应该试图用catch来进行捕获。</p><h3 id="Java异常中的Exception"><a href="#Java异常中的Exception" class="headerlink" title="Java异常中的Exception"></a>Java异常中的Exception</h3><p>上面我们有介绍，Java异常的中的Exception分为受检查异常和运行时异常（不受检查异常）。下面我们展开介绍。</p><h4 id="Java中的受检查异常"><a href="#Java中的受检查异常" class="headerlink" title="Java中的受检查异常"></a>Java中的受检查异常</h4><p>相信大家在写IO操作的代码的时候，一定有过这样的记忆，对File或者Stream进行操作的时候一定需要使用try-catch包起来，否则编译会失败，这是因为这些异常类型是受检查的异常类型。编译器在编译时，对于受检异常必须进行try…catch或throws处理,否则无法通过编译。常见的受检查异常包括：IO操作、ClassNotFoundException、线程操作等。</p><h4 id="Java中的非受检查异常（运行时异常）"><a href="#Java中的非受检查异常（运行时异常）" class="headerlink" title="Java中的非受检查异常（运行时异常）"></a>Java中的非受检查异常（运行时异常）</h4><p>RuntimeException及其子类都统称为非受检查异常，例如：NullPointExecrption、NumberFormatException（字符串转换为数字）、ArrayIndexOutOfBoundsException（数组越界）、ClassCastException（类型转换错误）、ArithmeticException（算术错误）等。</p><h3 id="Java的异常处理"><a href="#Java的异常处理" class="headerlink" title="Java的异常处理"></a>Java的异常处理</h3><p>Java处理异常的一般格式是这样的：</p><pre><code>try{    ///可能会抛出异常的代码}catch(Type1 id1){    //处理Type1类型异常的代码}catch(Type2 id2){    //处理Type2类型异常的代码}</code></pre><p>try块中放置可能会发生异常的代码(但是我们不知道具体会发生哪种异常)。如果异常发生了，try块抛出系统自动生成的异常对象，然后异常处理机制将负责搜寻参数与异常类型相匹配的第一个处理程序，然后进行catch语句执行(不会在向下查找)。如果我们的catch语句没有匹配到，那么JVM虚拟机还是会抛出异常的。</p><h4 id="Java中的throws关键字"><a href="#Java中的throws关键字" class="headerlink" title="Java中的throws关键字"></a>Java中的throws关键字</h4><p>如果在当前方法不知道该如何处理该异常时，则可以使用throws对异常进行抛出给调用者处理或者交给JVM。JVM对异常的处理方式是：打印异常的跟踪栈信息并终止程序运行。throws在使用时应处于方法签名之后使用，可以抛出多种异常并用英文字符逗号’,’隔开。下面是一个例子：</p><pre><code>public void f() throws ClassNotFoundException,IOException{}</code></pre><p>这样我们调用f()方法的时候必须要catch-ClassNotFoundException和IOException这两个异常或者catch-Exception基类。<br>注意：<br>throws的这种使用方式只是Java编译期要求我们这样做的，我们完全可以只在方法声明中throws相关异常，但是在方法里面却不抛出任何异常，这样也能通过编译，我们通过这种方式间接的绕过了Java编译期的检查。这种方式有一个好处：为异常先占一个位置，以后就可以抛出这种异常而不需要修改已有的代码。在定义抽象类和接口的时候这种设计很重要，这样派生类或者接口实现就可以抛出这些预先声明的异常。</p><h4 id="打印异常信息"><a href="#打印异常信息" class="headerlink" title="打印异常信息"></a>打印异常信息</h4><p>异常类的基类Exception中提供了一组方法用来获取异常的一些信息.所以如果我们获得了一个异常对象,那么我们就可以打印出一些有用的信息,最常用的就是void printStackTrace()这个方法,这个方法将返回一个由栈轨迹中的元素所构成的数组,其中每个元素都表示栈中的一帧.元素0是栈顶元素,并且是调用序列中的最后一个方法调用(这个异常被创建和抛出之处);他有几个不同的重载版本,可以将信息输出到不同的流中去.下面的代码显示了如何打印基本的异常信息:</p><pre><code>public void f() throws IOException{    System.out.println(&quot;Throws SimpleException from f()&quot;);     throw new IOException(&quot;Crash&quot;); } public static void main(String[] agrs) {    try {        new B().f();    } catch (IOException e) {        System.out.println(&quot;Caught  Exception&quot;);        System.out.println(&quot;getMessage(): &quot;+e.getMessage());        System.out.println(&quot;getLocalizedMessage(): &quot;+e.getLocalizedMessage());        System.out.println(&quot;toString(): &quot;+e.toString());        System.out.println(&quot;printStackTrace(): &quot;);        e.printStackTrace(System.out);    }}</code></pre><p>我们来看输出：</p><pre><code>Throws SimpleException from f()Caught  ExceptiongetMessage(): CrashgetLocalizedMessage(): CrashtoString(): java.io.IOException: CrashprintStackTrace(): java.io.IOException: Crash    at com.learn.example.B.f(RunMain.java:19)    at com.learn.example.RunMain.main(RunMain.java:26)</code></pre><h4 id="使用finally进行清理"><a href="#使用finally进行清理" class="headerlink" title="使用finally进行清理"></a>使用finally进行清理</h4><p>引入finally语句的原因是我们希望一些代码总是能得到执行,无论try块中是否抛出异常.这样异常处理的基本格式变成了下面这样:</p><pre><code>try{    //可能会抛出异常的代码}catch(Type1 id1){    //处理Type1类型异常的代码}catch(Type2 id2){    //处理Type2类型异常的代码}finally{    //总是会执行的代码}</code></pre><p>在Java中希望除内存以外的资源恢复到它们的初始状态的时候需要使用的finally语句。例如打开的文件或者网络连接，屏幕上的绘制的图像等。下面我们来看一下案例：</p><pre><code>public class FinallyException {    static int count = 0;    public static void main(String[] args) {        while (true){            try {                if (count++ == 0){                    throw new ThreeException();                }                System.out.println(&quot;no Exception&quot;);            }catch (ThreeException e){                System.out.println(&quot;ThreeException&quot;);            }finally {                System.out.println(&quot;in finally cause&quot;);                if(count == 2)                    break;            }        }    }}class ThreeException extends Exception{}</code></pre><p>我们来看输出：</p><pre><code>ThreeExceptionin finally causeno Exceptionin finally cause</code></pre><p>如果我们在try块或者catch块里面有return语句的话，那么finally语句还会执行吗？我们看下面的例子：</p><pre><code>public class MultipleReturns {    public static void f(int i){        System.out.println(&quot;start.......&quot;);        try {            System.out.println(&quot;1&quot;);            if(i == 1)                return;            System.out.println(&quot;2&quot;);            if (i == 2)                return;            System.out.println(&quot;3&quot;);            if(i == 3)                return;            System.out.println(&quot;else&quot;);            return;        }finally {            System.out.println(&quot;end&quot;);        }    }    public static void main(String[] args) {        for (int i = 1; i&lt;4; i++){            f(i);        }    }}</code></pre><p>我们来看运行结果：</p><pre><code>start.......1endstart.......12endstart.......123end</code></pre><p>我们看到即使我们在try或者catch块中使用了return语句，finally子句还是会执行。那么有什么情况finally子句不会执行呢？<br>有下面两种情况会导致Java异常的丢失</p><ul><li>finally中重写抛出异常（finally中重写抛出另一种异常会覆盖原来捕捉到的异常）</li><li>在finally子句中返回（即return）</li></ul><h3 id="Java异常栈"><a href="#Java异常栈" class="headerlink" title="Java异常栈"></a>Java异常栈</h3><p>前面稍微提到了点Java异常栈的相关内容，这一节我们通过一个简单的例子来更加直观的了解异常栈的相关内容。我们再看Exception异常的时候会发现，发生异常的方法会在最上层，main方法会在最下层，中间还有其他的调用层次。这其实是栈的结构，先进后出的。下面我们通过例子来看下：</p><pre><code>public class WhoCalled {    static void f() {        try {            throw new Exception();        } catch (Exception e) {            for (StackTraceElement ste : e.getStackTrace()){                System.out.println(ste.getMethodName());            }        }    }    static void g(){        f();    }    static void h(){        g();    }    public static void main(String[] args) {        f();        System.out.println(&quot;---------------------------&quot;);        g();        System.out.println(&quot;---------------------------&quot;);        h();        System.out.println(&quot;---------------------------&quot;);    }}</code></pre><p>我们来看输出结果：</p><pre><code>fmain---------------------------fgmain---------------------------fghmain---------------------------</code></pre><p>可以看到异常信息都是从内到外的，按我的理解查看异常的时候要从第一条异常信息看起，因为那是异常发生的源头。</p><h3 id="重新抛出异常及异常链"><a href="#重新抛出异常及异常链" class="headerlink" title="重新抛出异常及异常链"></a>重新抛出异常及异常链</h3><p>我们知道每遇到一个异常信息，我们都需要进行try…catch,一个还好，如果出现多个异常呢？分类处理肯定会比较麻烦，那就一个Exception解决所有的异常吧。这样确实是可以，但是这样处理势必会导致后面的维护难度增加。最好的办法就是将这些异常信息封装，然后捕获我们的封装类即可。<br>我们有两种方式处理异常，一是throws抛出交给上级处理，二是try…catch做具体处理。但是这个与上面有什么关联呢？try…catch的catch块我们可以不需要做任何处理，仅仅只用throw这个关键字将我们封装异常信息主动抛出来。然后在通过关键字throws继续抛出该方法异常。它的上层也可以做这样的处理，以此类推就会产生一条由异常构成的异常链。<br>通过使用异常链，我们可以提高代码的可理解性、系统的可维护性和友好性。<br>我们捕获异常以后一般会有两种操作</p><ul><li>捕获后抛出原来的异常，希望保留最新的异常抛出点－－fillStackTrace</li><li>捕获后抛出新的异常，希望抛出完整的异常链－－initCause</li></ul><h4 id="捕获异常后重新抛出异常"><a href="#捕获异常后重新抛出异常" class="headerlink" title="捕获异常后重新抛出异常"></a>捕获异常后重新抛出异常</h4><p>在函数中捕获了异常，在catch模块中不做进一步的处理，而是向上一级进行传递catch(Exception e){ throw e;}，我们通过例子来看一下：</p><pre><code>public class ReThrow {    public static void f()throws Exception{        throw new Exception(&quot;Exception: f()&quot;);    }    public static void g() throws Exception{        try{            f();        }catch(Exception e){            System.out.println(&quot;inside g()&quot;);            throw e;        }    }    public static void main(String[] args){        try{            g();        }        catch(Exception e){            System.out.println(&quot;inside main()&quot;);            e.printStackTrace(System.out);        }    }}</code></pre><p>我们来看输出：</p><pre><code>inside g()inside main()java.lang.Exception: Exception: f()        //异常的抛出点还是最初抛出异常的函数f()    at com.learn.example.ReThrow.f(RunMain.java:5)    at com.learn.example.ReThrow.g(RunMain.java:10)    at com.learn.example.RunMain.main(RunMain.java:21)</code></pre><h4 id="fillStackTrace——覆盖前边的异常抛出点-获取最新的异常抛出点"><a href="#fillStackTrace——覆盖前边的异常抛出点-获取最新的异常抛出点" class="headerlink" title="fillStackTrace——覆盖前边的异常抛出点(获取最新的异常抛出点)"></a>fillStackTrace——覆盖前边的异常抛出点(获取最新的异常抛出点)</h4><p>在此抛出异常的时候进行设置catch(Exception e){ (Exception)e.fillInStackTrace();}我们通过例子看一下：(还是刚才的例子)</p><pre><code>public void g() throws Exception{    try{        f();    }catch(Exception e){        System.out.println(&quot;inside g()&quot;);        throw (Exception)e.fillInStackTrace();    }}</code></pre><p>运行结果如下：</p><pre><code>inside g()inside main()java.lang.Exception: Exception: f()        //显示的就是最新的抛出点    at com.learn.example.ReThrow.g(RunMain.java:13)    at com.learn.example.RunMain.main(RunMain.java:21)</code></pre><h4 id="捕获异常后抛出新的异常（保留原来的异常信息，区别于捕获异常之后重新抛出）"><a href="#捕获异常后抛出新的异常（保留原来的异常信息，区别于捕获异常之后重新抛出）" class="headerlink" title="捕获异常后抛出新的异常（保留原来的异常信息，区别于捕获异常之后重新抛出）"></a>捕获异常后抛出新的异常（保留原来的异常信息，区别于捕获异常之后重新抛出）</h4><p>如果我们在抛出异常的时候需要保留原来的异常信息，那么有两种方式</p><ul><li>方式1:Exception e＝new Exception(); e.initCause(ex);</li><li>方式2:Exception e =new Exception(ex);</li></ul><pre><code>class ReThrow {    public void f(){        try{             g();          }catch(NullPointerException ex){             //方式1             Exception e=new Exception();             //将原始的异常信息保留下来             e.initCause(ex);             //方式2             //Exception e=new Exception(ex);             try {                throw e;            } catch (Exception e1) {                e1.printStackTrace();            }         }    }    public void g() throws NullPointerException{        System.out.println(&quot;inside g()&quot;);        throw new NullPointerException();    }}public class RunMain {    public static void main(String[] agrs) {        try{            new ReThrow().f();        }        catch(Exception e){            System.out.println(&quot;inside main()&quot;);            e.printStackTrace(System.out);        }    }}</code></pre><p>在这个例子里面，我们先捕获NullPointerException异常，然后在抛出Exception异常，这时候如果我们不使用initCause方法将原始异常（NullPointerException）保存下来的话，就会丢失NullPointerException。只会显示Eception异常。下面我们来看结果：</p><pre><code>//没有调用initCause方法的输出inside g()java.lang.Exception    at com.learn.example.ReThrow.f(RunMain.java:9)    at com.learn.example.RunMain.main(RunMain.java:31)//调用initCasue方法保存原始异常信息的输出inside g()java.lang.Exception    at com.learn.example.ReThrow.f(RunMain.java:9)    at com.learn.example.RunMain.main(RunMain.java:31)Caused by: java.lang.NullPointerException    at com.learn.example.ReThrow.g(RunMain.java:24)    at com.learn.example.ReThrow.f(RunMain.java:6)    ... 1 more</code></pre><p>我们看到我们使用initCause方法保存后，原始的异常信息会以Caused by的形式输出。</p><h3 id="Java异常的限制"><a href="#Java异常的限制" class="headerlink" title="Java异常的限制"></a>Java异常的限制</h3><p>当Java异常遇到继承或者接口的时候是存在限制的，下面我们来看看有哪些限制。</p><ul><li>规则一：子类在重写父类抛出异常的方法时，要么不抛出异常，要么抛出与父类方法相同的异常或该异常的子类。如果被重写的父类方法只抛出受检异常，则子类重写的方法可以抛出非受检异常。例如，父类方法抛出了一个受检异常IOException，重写该方法时不能抛出Exception，对于受检异常而言，只能抛出IOException及其子类异常，也可以抛出非受检异常。我们通过例子来看下：</li></ul><pre><code>class A {      public void fun() throws Exception {}  }  class B extends A {      public void fun() throws IOException, RuntimeException {}  }</code></pre><p>父类抛出的异常包含所有异常，上面的写法正确。</p><pre><code>class A {      public void fun() throws RuntimeException {}  }  class B extends A {      public void fun() throws IOException, RuntimeException {}  }</code></pre><p>子类IOException超出了父类的异常范畴，上面的写法错误。</p><pre><code>class A {      public void fun() throws IOException {}  }  class B extends A {      public void fun() throws IOException, RuntimeException, ArithmeticException{}}</code></pre><p>RuntimeException不属于IO的范畴，并且超出了父类的异常范畴。但是RuntimeException和ArithmeticException属于运行时异常，子类重写的方法可以抛出任何运行时异常。所以上面的写法正确。</p><ul><li>规则儿：子类在重写父类抛出异常的方法时，如果实现了有相同方法签名的接口且接口中的该方法也有异常声明，则子类重写的方法要么不抛出异常，要么抛出父类中被重写方法声明异常与接口中被实现方法声明异常的交集。</li></ul><pre><code>class Test {    public Test() throws IOException {}    void test() throws IOException {}}interface I1{    void test() throw Exception;}class SubTest extends Test implements I1 {    public SubTest() throws Exception,NullPointerException, NoSuchMethodException {}    void test() throws IOException {}}</code></pre><p>在SubTest类中，test方法要么不抛出异常，要么抛出IOException或其子类（例如，InterruptedIOException）。</p><h3 id="Java异常与构造器"><a href="#Java异常与构造器" class="headerlink" title="Java异常与构造器"></a>Java异常与构造器</h3><p>如果一个构造器中就发生异常了，那我们如何处理才能正确的清呢？也许你会说使用finally啊，它不是一定会执行的吗？这可不一定，如果构造器在其执行过程中遇到了异常，这时候对象的某些部分还没有正确的初始化，而这时候却会在finally中对其进行清理，显然这样会出问题的。<br>原则：<br>对于在构造器阶段可能会抛出异常，并且要求清理的类，最安全的方式是使用嵌套的try子句。</p><pre><code>try {    InputFile in=new InpputFile(&quot;Cleanup.java&quot;);    try {        String string;        int i=1;        while ((string=in.getLine())!=null) {}    }catch (Exception e) {        System.out.println(&quot;Cause Exception in main&quot;);        e.printStackTrace(System.out);    }finally {        in.dispose();    }}catch (Exception e) {    System.out.println(&quot;InputFile construction failed&quot;);}</code></pre><p>我们来仔细看一下这里面的逻辑，对InputFile的构造在第一个try块中是有效的，如果构造器失败，抛出异常，那么会被最外层的catch捕获到，这时候InputFile对象的dispose方法是不需要执行的。如果构造成功，那么进入第二层try块，这时候finally块肯定是需要被调用的（对象需要dispose）。</p><h2 id="java项目中的异常处理方式"><a href="#java项目中的异常处理方式" class="headerlink" title="java项目中的异常处理方式"></a>java项目中的异常处理方式</h2><h3 id="统一抛出异常"><a href="#统一抛出异常" class="headerlink" title="统一抛出异常"></a>统一抛出异常</h3><ul><li>首先定义一个自定义异常类继承RuntimeException</li></ul><pre><code>@Data@AllArgsConstructorpublic class CustomException extends RuntimeException {    ResultCode resultCode;}</code></pre><ul><li>然后定义一个异常抓取类负责抛出异常</li></ul><pre><code>public class ExceptionCast {    public static void cast(ResultCode resultCode){        throw new CustomException(resultCode);    }}</code></pre><ul><li>在业务代码中抛出异常</li></ul><pre><code>if (!optional.isPresent()) {    ExceptionCast.cast(CmsCode.CMS_SITE_NOEXISTS);}</code></pre><h3 id="统一抓获异常再具体处理"><a href="#统一抓获异常再具体处理" class="headerlink" title="统一抓获异常再具体处理"></a>统一抓获异常再具体处理</h3><ul><li>异常分两类处理,一种是自己定义的异常,一种是系统异常.对创建一份Map保存一些已经事先知道或者遇到过的异常,当捕获到该类异常时获取预先设置的响应状态,遇到没有见过的异常则统一响应</li></ul><pre><code>@RestControllerAdvicepublic class ExceptionCatch {    Logger logger = LoggerFactory.getLogger(ExceptionCatch.class);    //使用EXCEPTIONS存放异常类型和错误代码的映射,ImmutableMap的特点是一旦创建就不可改变,并且线程安全.    private static ImmutableMap&lt;Class&lt;? extends Throwable&gt;, ResultCode&gt; EXCEPTIONS;    //使用builder来构建一个异常类型和错误代码的异常    protected static ImmutableMap.Builder&lt;Class&lt;? extends Throwable&gt;, ResultCode&gt; builder = ImmutableMap.builder();    static {        builder.put(HttpMediaTypeNotSupportedException.class, CommonCode.INVAILDPARAM);    }    //捕获CustomException异常    @ExceptionHandler(CustomException.class)    public ResponseResult customException(CustomException customException) {        logger.error(&quot;catch exception : {}\r\nexception:&quot;, customException.getMessage(), customException);        ResultCode resultCode = customException.getResultCode();        return new ResponseResult(resultCode);    }    //捕获不可预知异常    @ExceptionHandler(Exception.class)    public ResponseResult exception(Exception exception) {        logger.error(&quot;catch exception : {}\r\nexception:&quot;, exception.getMessage(), exception);        if (EXCEPTIONS == null) {            EXCEPTIONS = builder.build();        }        ResultCode resultCode = EXCEPTIONS.get(exception.getClass());        ResponseResult responseResult;        if (resultCode != null) {            responseResult = new ResponseResult(resultCode);        } else {            responseResult = new ResponseResult(CommonCode.SERVER_ERROR);        }        return responseResult;    }}</code></pre>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>javaWeb三大核心组件之servlet</title>
      <link href="/2020/01/04/javaweb-san-da-he-xin-zu-jian-zhi-servlet/"/>
      <url>/2020/01/04/javaweb-san-da-he-xin-zu-jian-zhi-servlet/</url>
      
        <content type="html"><![CDATA[<h1 id="javaWeb三大核心组件之Servlet"><a href="#javaWeb三大核心组件之Servlet" class="headerlink" title="javaWeb三大核心组件之Servlet"></a>javaWeb三大核心组件之Servlet</h1><h3 id="什么是Servlet"><a href="#什么是Servlet" class="headerlink" title="什么是Servlet"></a>什么是Servlet</h3><p>Java Servlet 是运行在 Web 服务器或应用服务器上的程序，它是作为来自 Web 浏览器或其他 HTTP 客户端的请求和 HTTP 服务器上的数据库或应用程序之间的中间层。</p><p>使用 Servlet，您可以收集来自网页表单的用户输入，呈现来自数据库或者其他源的记录，还可以动态创建网页。</p><p>Java Servlet 通常情况下与使用 CGI（Common Gateway Interface，公共网关接口）实现的程序可以达到异曲同工的效果。但是相比于 CGI，Servlet 有以下几点优势：</p><p>性能明显更好。</p><p>Servlet 在 Web 服务器的地址空间内执行。这样它就没有必要再创建一个单独的进程来处理每个客户端请求。</p><p>Servlet 是独立于平台的，因为它们是用 Java 编写的。</p><p>服务器上的 Java 安全管理器执行了一系列限制，以保护服务器计算机上的资源。因此，Servlet 是可信的。</p><p>Java 类库的全部功能对 Servlet 来说都是可用的。它可以通过 sockets 和 RMI 机制与 applets、数据库或其他软件进行交互。</p><h3 id="Tomcat与Servlet的关系"><a href="#Tomcat与Servlet的关系" class="headerlink" title="Tomcat与Servlet的关系"></a>Tomcat与Servlet的关系</h3><p>Tomcat 是Web应用服务器,是一个Servlet/JSP容器. Tomcat 作为Servlet容器,负责处理客户请求,把请求传送给Servlet,并将Servlet的响应传送回给客户.而Servlet是一种运行在支持Java语言的服务器上的组件.。</p><p>Servlet最常见的用途是扩展Java Web服务器功能,提供非常安全的,可移植的,易于使用的CGI替代品。从http协议中的请求和响应可以得知，浏览器发出的请求是一个请求文本，而浏览器接收到的也应该是一个响应文本。</p><p><img src="/2020/01/04/javaweb-san-da-he-xin-zu-jian-zhi-servlet/1" alt></p><ol><li>Tomcat将http请求文本接收并解析，然后封装成HttpServletRequest类型的request对象，所有的HTTP头数据读可以通过request对象调用对应的方法查询到。</li><li>Tomcat同时会要响应的信息封装为HttpServletResponse类型的response对象，通过设置response属性就可以控制要输出到浏览器的内容，然后将response交给tomcat，tomcat就会将其变成响应文本的格式发送给浏览器。</li></ol><p>Java Servlet API 是Servlet容器(tomcat)和servlet之间的接口，它定义了serlvet的各种方法，还定义了Servlet容器传送给Servlet的对象类，其中最重要的就是ServletRequest和ServletResponse。所以说我们在编写servlet时，需要实现Servlet接口，按照其规范进行操作。</p><h3 id="Servlet执行过程"><a href="#Servlet执行过程" class="headerlink" title="Servlet执行过程"></a>Servlet执行过程</h3><p> 在浏览器的地址栏输入：<a href="http://ip:port/appNames/servlet" target="_blank" rel="noopener">http://ip:port/appNames/servlet</a></p><p>  1）通过浏览器和ip：port和这个服务器建立连接。<br>  2） 浏览器会生成一个请求数据包（路径appNames/servlet）向服务器发送请求。<br>  3） 服务器收到请求数据包，分析请求资源路径做精准定位，通过请求的appName查找webapps文件下面的appName做匹配，匹配上了需要获取web.xml中的servlet(mapping)。 <br>  4） 服务器创建两个对象：<br>    第一个对象：请求对象，该对象实现了HttpServletRequest接口，服务器会将请求数据包中的数据解析出来,存储在该对象里。这样做的好处是没有必要理解http协议，只需要读取request。<br>    第二个对象：响应对象，实现了HttpServletResponse接口，作用是servlet处理完成后的结果可以存放到该对象上，然后服务器依据该对象的数据生成响应数据包。<br>  5） servlet在执行servlet()方法时，可以通过request获取请求数据，也可以将处理结果存放到response上。然后服务器与响应对象直接形成一个默契，生成一个响应数据包给浏览器。<br>  6）浏览器解析服务器返回的响应数据包，生成响应的结果。</p><p>  </p><p><img src="/2020/01/04/javaweb-san-da-he-xin-zu-jian-zhi-servlet/2.png" alt></p><p>Servlet访问的过程：<br>Http请求—-&gt;web.xml——–&gt;  url -pattern—–&gt;servlet-name—–&gt;servlet-class—–&gt;   QuickStratServlet(对应的Class文件)</p><h3 id="Servlet生命周期"><a href="#Servlet生命周期" class="headerlink" title="Servlet生命周期"></a>Servlet生命周期</h3><p>SpringMVC是基于servlet，控制器基于方法级别的拦截，处理器设计为单实例，所以应该了解一下Servlet的生命周期。</p><p>Servlet 加载—&gt;实例化—&gt;服务—&gt;销毁。</p><p><strong>init</strong>（）：</p><p>在Servlet的生命周期中，仅执行一次init()方法。它是在服务器装入Servlet时执行的，负责初始化Servlet对象。可以配置服务器，以在启动服务器或客户机首次访问Servlet时装入Servlet。无论有多少客户机访问Servlet，都不会重复执行init（）。</p><p><strong>service</strong>（）：</p><p>它是Servlet的核心，负责响应客户的请求。每当一个客户请求一个HttpServlet对象，该对象的Service()方法就要调用，而且传递给这个方法一个“请求”（ServletRequest）对象和一个“响应”（ServletResponse）对象作为参数。在HttpServlet中已存在Service()方法。默认的服务功能是调用与HTTP请求的方法相应的do功能。</p><p><strong>destroy</strong>（）：</p><p>仅执行一次，在服务器端停止且卸载Servlet时执行该方法。当Servlet对象退出生命周期时，负责释放占用的资源。一个Servlet在运行service()方法时可能会产生其他的线程，因此需要确认在调用destroy()方法时，这些线程已经终止或完成。</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SpringMVC原理</title>
      <link href="/2020/01/04/springmvc-yuan-li/"/>
      <url>/2020/01/04/springmvc-yuan-li/</url>
      
        <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><a href="http://lib.csdn.net/base/javaee" target="_blank" rel="noopener">spring</a> MVC主要由DispatcherServlet、处理器映射、处理器(控制器)、视图解析器、视图组成。他的两个核心是:</p><p><strong>处理器映射：</strong> 选择使用哪个控制器来处理请求。<br><strong>视图解析器：</strong> 选择结果应该如何渲染。</p><h2 id="运行原理"><a href="#运行原理" class="headerlink" title="运行原理"></a>运行原理</h2><p>下图是在Spring官网开发手册上找到的，它清晰的诠释了Spring MVC的运行原理</p><p><img src="/2020/01/04/springmvc-yuan-li/16b5eb3870589ac4.png" alt></p><p>①客户端的所有请求都交给前端控制器DispatcherServlet来处理，它会负责调用系统的其他模块来真正处理用户的请求。</p><p>② DispatcherServlet收到请求后，将根据请求的信息（包括URL、HTTP协议方法、请求头、请求参数、Cookie等）以及HandlerMapping的配置找到处理该请求的Handler（任何一个对象都可以作为请求的Handler）。</p><p>③在这个地方Spring会通过HandlerAdapter对该处理器进行封装。</p><p>④ HandlerAdapter是一个适配器，它用统一的接口对各种Handler中的方法进行调用。</p><p>⑤ Handler完成对用户请求的处理后，会返回一个ModelAndView对象给DispatcherServlet，ModelAndView顾名思义，包含了数据模型以及相应的视图的信息。</p><p>⑥ ModelAndView的视图是逻辑视图，DispatcherServlet还要借助ViewResolver完成从逻辑视图到真实视图对象的解析工作。⑦ 当得到真正的视图对象后，DispatcherServlet会利用视图对象对模型数据进行渲染。</p><p>⑧ 客户端得到响应，可能是一个普通的HTML页面，也可以是XML或JSON字符串，还可以是一张图片或者一个PDF文件。</p><p><img src="/2020/01/04/springmvc-yuan-li/16b5eb3870589ac4.png" alt></p><h2 id="接口的解释"><a href="#接口的解释" class="headerlink" title="接口的解释"></a>接口的解释</h2><table><thead><tr><th>接口名称</th><th>功能</th></tr></thead><tbody><tr><td>DispatcherServlet</td><td>Spring提供的前端控制器，客户端的所有请求都由DispatcherServlet负责分发，当然在DispatcherServlet分发之前，还需要一个匹配请求的过程，这个由HandlerMapping来完成。</td></tr><tr><td>HandlerMapping</td><td>完成客户端请求到Controller映射的工作</td></tr><tr><td>Controller</td><td>用于处理用户请求，返回处理结果</td></tr><tr><td>ViewResolver</td><td>Web应用中查找View对象，从而将相应结果渲染给客户端</td></tr></tbody></table><h2 id="DispatcherServlet："><a href="#DispatcherServlet：" class="headerlink" title="DispatcherServlet："></a>DispatcherServlet：</h2><p>是整个Spring MVC的核心。它负责接收HTTP请求组织协调Spring MVC的各个组成部分。</p><p>其主要工作有以下三项：</p><ol><li>截获符合特定格式的URL请求。</li><li>初始化DispatcherServlet上下文对应WebApplicationContext，并将其与业务层、持久化层的WebApplicationContext建立关联。</li><li>初始化Spring MVC的各个组成组件，并装配到DispatcherServlet中。</li></ol><h2 id="SpringMVC启动流程"><a href="#SpringMVC启动流程" class="headerlink" title="SpringMVC启动流程"></a>SpringMVC启动流程</h2><h4 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h4><p>大家都知道，我们在使用spring mvc时通常会在<code>web.xml</code>文件中做如下配置：</p><p><code>web.xml</code></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app version=&quot;3.0&quot; xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot;    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;    xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd&quot;&gt;    &lt;!-- 上下文参数，在监听器中被使用 --&gt;    &lt;context-param&gt;        &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;        &lt;param-value&gt;            classpath:applicationContext.xml        &lt;/param-value&gt;    &lt;/context-param&gt;    &lt;!-- 监听器配置 --&gt;    &lt;listener&gt;        &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;    &lt;/listener&gt;    &lt;!-- 前端控制器配置 --&gt;    &lt;servlet&gt;        &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt;        &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt;        &lt;init-param&gt;            &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;            &lt;param-value&gt;classpath:applicationContext-mvc.xml&lt;/param-value&gt;        &lt;/init-param&gt;        &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;    &lt;/servlet&gt;    &lt;servlet-mapping&gt;        &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt;        &lt;url-pattern&gt;/&lt;/url-pattern&gt;    &lt;/servlet-mapping&gt;&lt;/web-app&gt;</code></pre><p>上面的配置总结起来有几点内容，分别是：</p><ul><li>1、配置Spring Web上下文监听器，该监听器同时是Spring mvc启动的入口，至于为什么，后面第二节将会讲到</li><li>2、前端控制器<code>DispatcherServlet</code>,该控制器是Spring mvc处理各种请求的入口及处理器</li></ul><p>当我们将spring mvc应用部署到tomcat时，当你不配置任何的<code>context-param</code>和<code>listener</code>参数,只配置一个<code>DispatcherServlet</code>时，那么tomcat在启动的时候是不会初始化spring web上下文的，换句话说，tomcat是不会初始化spring框架的，因为你并没有告诉它们spring的配置文件放在什么地方，以及怎么去加载。所以<code>listener</code>监听器帮了我们这个忙，那么为什么配置监听器之后就可以告诉tomcat怎么去加载呢？因为<code>listener</code>是实现了servlet技术规范的监听器组件，tomcat在启动时会先加载<code>web.xml</code>中是否有servlet监听器存在，有则启动它们。<code>ContextLoaderListener</code>是spring框架对servlet监听器的一个封装，本质上还是一个servlet监听器，所以会被执行，但由于<code>ContextLoaderListener</code>源码中是基于<code>contextConfigLocation</code>和<code>contextClass</code>两个配置参数去加载相应配置的，因此就有了我们配置的<code>context-param</code>参数了，<code>servlet</code>标签里的初始化参数也是同样的道理，即告诉web服务器在启动的同时把spring web上下文（<code>WebApplicationContext</code>）也给初始化了。</p><p>上面讲了下tomcat加载spring mvc应用的大致流程，接下来将从源码入手分析启动原理。</p><h4 id="Spring-MVC-web-上下文启动源码分析"><a href="#Spring-MVC-web-上下文启动源码分析" class="headerlink" title="Spring MVC web 上下文启动源码分析"></a>Spring MVC web 上下文启动源码分析</h4><p>假设现在我们把上面<code>web.xml</code>文件中的<code>&lt;load-on-startup&gt;1&lt;/load-on-startup&gt;</code>给去掉，那么默认tomcat启动时只会初始化spring web上下文，也就是说只会加载到<code>applicationContext.xml</code>这个文件，对于<code>applicationContext-mvc.xml</code>这个配置文件是加载不到的，<code>&lt;load-on-startup&gt;1&lt;/load-on-startup&gt;</code>的意思就是让<code>DispatcherServlet</code>延迟到使用的时候(<code>也就是处理请求的时候</code>)再做初始化。</p><p>我们已经知道spring web是基于<code>servlet</code>标准去封装的，那么很明显，servlet怎么初始化，<code>WebApplicationContext</code>web上下文就应该怎么初始化。我们先看看<code>ContextLoaderListener</code>的源码是怎样的。</p><pre><code>public class ContextLoaderListener extends ContextLoader implements ServletContextListener {    // 初始化方法    @Override    public void contextInitialized(ServletContextEvent event) {        initWebApplicationContext(event.getServletContext());    }    // 销毁方法    @Override    public void contextDestroyed(ServletContextEvent event) {        closeWebApplicationContext(event.getServletContext());        ContextCleanupListener.cleanupAttributes(event.getServletContext());    }}</code></pre><p><code>ContextLoaderListener</code>类实现了<code>ServletContextListener</code>，本质上是一个servlet监听器，tomcat将会优先加载servlet监听器组件，并调用<code>contextInitialized</code>方法,在<code>contextInitialized</code>方法中调用<code>initWebApplicationContext</code>方法初始化Spring web上下文，看到这焕然大悟，原来Spring mvc的入口就在这里，哈哈<del>~</del>赶紧跟进去<code>initWebApplicationContext</code>方法看看吧！</p><p><code>initWebApplicationContext()</code>方法：</p><pre><code>// 创建web上下文，默认是XmlWebApplicationContextif (this.context == null) {    this.context = createWebApplicationContext(servletContext);}if (this.context instanceof ConfigurableWebApplicationContext) {    ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) this.context;    // 如果该容器还没有刷新过    if (!cwac.isActive()) {        if (cwac.getParent() == null) {            ApplicationContext parent = loadParentContext(servletContext);            cwac.setParent(parent);        }        // 配置并刷新容器        configureAndRefreshWebApplicationContext(cwac, servletContext);    }}</code></pre><p>上面的方法只做了两件事：</p><ul><li>1、如果spring web容器还没有创建，那么就创建一个全新的spring web容器，并且该容器为root根容器，下面第三节讲到的servlet spring web容器是在此根容器上创建起来的</li><li>2、配置并刷新容器</li></ul><p>上面代码注释说到默认创建的上下文容器是<code>XmlWebApplicationContext</code>,为什么不是其他web上下文呢？为啥不是下面上下文的任何一种呢？</p><p><img src="https://user-gold-cdn.xitu.io/2018/7/23/164c646d5fb17327?imageView2/0/w/1280/h/960/format/webp/ignore-error/1" alt="img"></p><p>我们可以跟进去<code>createWebApplicationContext</code>后就可以发现默认是从一个叫<code>ContextLoader.properties</code>文件加载配置的，该文件的内容为：</p><pre><code>org.springframework.web.context.WebApplicationContext=org.springframework.web.context.support.XmlWebApplicationContext</code></pre><p>具体实现为：</p><pre><code>protected Class&lt;?&gt; determineContextClass(ServletContext servletContext) {    // 自定义上下文，否则就默认创建XmlWebApplicationContext    String contextClassName = servletContext.getInitParameter(CONTEXT_CLASS_PARAM);    if (contextClassName != null) {        try {            return ClassUtils.forName(contextClassName, ClassUtils.getDefaultClassLoader());        }        catch (ClassNotFoundException ex) {            throw new ApplicationContextException(                    &quot;Failed to load custom context class [&quot; + contextClassName + &quot;]&quot;, ex);        }    }    else {        // 从属性文件中加载类名，也就是org.springframework.web.context.support.XmlWebApplicationContext        contextClassName = defaultStrategies.getProperty(WebApplicationContext.class.getName());        try {            return ClassUtils.forName(contextClassName, ContextLoader.class.getClassLoader());        }        catch (ClassNotFoundException ex) {            throw new ApplicationContextException(                    &quot;Failed to load default context class [&quot; + contextClassName + &quot;]&quot;, ex);        }    }}</code></pre><p>上面可以看出其实我们也可以自定义spring web的上下文的，那么怎么去指定我们自定义的上下文呢？答案是通过在<code>web.xml</code>中指定<code>contextClass</code>参数，因此第一小结结尾时说<code>contextClass</code>参数和<code>contextConfigLocation</code>很重要~~至于<code>contextConfigLocation</code>参数，我们跟进<code>configureAndRefreshWebApplicationContext</code>即可看到，如下图：</p><p><img src="/2020/01/04/springmvc-yuan-li/3.png" alt="img"></p><p><strong>总结：</strong></p><p>spring mvc启动流程大致就是从一个叫<code>ContextLoaderListener</code>开始的，它是一个servlet监听器，能够被web容器发现并加载，初始化监听器<code>ContextLoaderListener</code>之后，接着就是根据配置如<code>contextConfigLocation</code>和<code>contextClass</code>创建web容器了，如果你不指定<code>contextClass</code>参数值，则默认创建的spring web容器类型为<code>XmlWebApplicationContext</code>,最后一步就是根据你配置的<code>contextConfigLocation</code>文件路径去配置并刷新容器了。</p><h4 id="DispatcherServlet控制器的初始化"><a href="#DispatcherServlet控制器的初始化" class="headerlink" title="DispatcherServlet控制器的初始化"></a>DispatcherServlet控制器的初始化</h4><p>好了，上面我们简单地分析了Spring mvc容器初始化的源码，我们永远不会忘记，我们默认创建的容器类型为<code>XmlWebApplicationContext</code>,当然我们也不会忘记，在<code>web.xml</code>中，我们还有一个重要的配置，那就是<code>DispatcherServlet</code>。下面我们就来分析下<code>DispatcherServlet</code>的初始化过程。</p><p><code>DispatcherServlet</code>，就是一个servlet，一个用来处理request请求的servlet，它是spring mvc的核心，所有的请求都经过它，并由它指定后续操作该怎么执行，咋一看像一扇门，因此我管它叫“闸门”。在我们继续之前，我们应该共同遵守一个常识，那就是——-无论是监听器还是servlet，都是servlet规范组件，web服务器都可以发现并加载它们。</p><p>下面我们先看看<code>DispatcherServlet</code>的继承关系：</p><p><img src="/2020/01/04/springmvc-yuan-li/4.png" alt="img"></p><p>看到这我们是不是一目了然了，<code>DispatcherServlet</code>继承了<code>HttpServlet</code>这个类，<code>HttpServlet</code>是servlet技术规范中专门用于处理http请求的servlet，这就不难解释为什么spring mvc会将<code>DispatcherServlet</code>作为统一请求入口了。</p><p>因为一个servlet的生命周期是<code>init()</code>-&gt;<code>service()</code>-&gt;<code>destory()</code>，那么<code>DispatcherServlet</code>怎么初始化呢？看上面的继承图，我们进到<code>HttpServletBean</code>去看看。</p><p>果不其然，<code>HttpServletBean</code>类中有一个<code>init()</code>方法，<code>HttpServletBean</code>是一个抽象类，<code>init()</code>方法如下：</p><p><img src="/2020/01/04/springmvc-yuan-li/5.png" alt="img"></p><p>可以看出方法采用<code>final</code>修饰，因为<code>final</code>修饰的方法是不能被子类继承的，也就是子类没有同样的<code>init()</code>方法了，这个<code>init</code>方法就是<code>DispatcherServlet</code>的初始化入口了。</p><p>接着我们跟进<code>FrameworkServlet</code>的<code>initServletBean()</code>方法：</p><p><img src="/2020/01/04/springmvc-yuan-li/6.png" alt="img"></p><p>在方法中将会初始化不同于第一小节的web容器，请记住，这个新的spring web 容器是专门为<code>dispactherServlet</code>服务的，而且这个新容器是在第一小节根ROOT容器的基础上创建的，我们在<code>&lt;servlet&gt;</code>标签中配置的初始化参数被加入到新容器中去。</p><p>至此，<code>DispatcherSevlet</code>的初始化完成了，听着有点蒙蔽，但其实也是这样，上面的分析仅仅只围绕一个方法，它叫<code>init()</code>，所有的servlet初始化都将调用该方法。</p><p><strong>总结:</strong></p><p><code>dispactherServlet</code>的初始化做了两件事情，第一件事情就是根据根web容器，也就是我们第一小节创建的<code>XmlWebApplicationContext</code>，然后创建一个专门为<code>dispactherServlet</code>服务的web容器，第二件事情就是将你在web.xml文件中对<code>dispactherServlet</code>进行的相关配置加载到新容器当中。</p><p>发现其实大致流程就差不多了。</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>回头再看spring</title>
      <link href="/2020/01/04/hui-tou-zai-kan-spring/"/>
      <url>/2020/01/04/hui-tou-zai-kan-spring/</url>
      
        <content type="html"><![CDATA[<h2 id="回头再看Spring"><a href="#回头再看Spring" class="headerlink" title="回头再看Spring"></a>回头再看Spring</h2><h3 id="什么是Spring"><a href="#什么是Spring" class="headerlink" title="什么是Spring"></a>什么是Spring</h3><p>Spring是个包含一系列功能的合集，如快速构建微服务的Spring Boot，管理一系列微服务的Spring Cloud，支持认证与鉴权的Spring Security，基于MVC的Web框架Spring MVC。但IOC与AOP依然是核心。</p><h3 id="Spring-Bean"><a href="#Spring-Bean" class="headerlink" title="Spring Bean"></a>Spring Bean</h3><p><strong>IOC的底层原理：文档解析xml文件，反射动态创建对象，然后保存name和Object，然后对每个对象属性进行属性注入</strong></p><h5 id="加载Bean的主要逻辑"><a href="#加载Bean的主要逻辑" class="headerlink" title="加载Bean的主要逻辑"></a>加载Bean的主要逻辑</h5><p>​    1.获取配置文件资源</p><p>​    2.对获取的xml资源进行一定的处理检验</p><p>​    3.处理包装资源</p><p>​    4.解析处理包装过后的资源</p><p>​    5.加载提取bean并注册(添加到beanDefinitionMap中</p><h5 id="Bean的生命周期"><a href="#Bean的生命周期" class="headerlink" title="Bean的生命周期"></a>Bean的生命周期</h5><ul><li>Bean的建立，由BeanFactory读取Bean定义文件，并创建Bean实例；</li><li>执行Bean的属性注入,Setter注入；</li><li>如果Bean类实现了org.springframework.beans.factory.BeanNameAware接口,则执行其setBeanName方法；</li><li>如果Bean类实现了org.springframework.beans.factory.BeanFactoryAware接口,则执行其setBeanFactory方法；</li><li>如果容器中有实现org.springframework.beans.factory.BeanPostProcessors接口的实例，则任何Bean在初始化之前都会执行这个实例的processBeforeInitialization()方法；</li><li>如果Bean类实现了org.springframework.beans.factory.InitializingBean接口，则执行其afterPropertiesSet()方法；</li><li>调用Bean的初始化方法”init-method” (！！注意，init-method方法没有参数)；</li><li>如果容器中有实现org.springframework.beans.factory.BeanPostProcessors接口的实例，则任何Bean在初始化之后都会执行这个实例的processAfterInitialization()方法；</li><li>使用Bean做一些业务逻辑….</li><li>使用完，容器关闭，如果Bean类实现了org.springframework.beans.factory.DisposableBean接口，则执行它的destroy()方法；</li><li>在容器关闭时，可以在Bean定义文件中使用“destory-method”定义的方法，销毁Bean (！！注意，destory-method方法没有参数)；</li></ul><h5 id="Bean的作用域"><a href="#Bean的作用域" class="headerlink" title="Bean的作用域"></a>Bean的作用域</h5><ul><li>Singleton: 这是默认的作用域，这种范围确保不管接受多少个请求，每个容器中只有一个bean的实例，单例模式有BeanFactory自身维护；</li><li>Prototype: 原形范围与单例范围相反，为每一个bean请求提供一个实例；</li></ul><ul><li>Request: 在请求bean范围内会为每一个来自客户端的网络请求创建一个实例，在请求完成以后，bean会失效并被垃圾回收器回收；</li><li>Session: 与请求范围类似，确保每个session中有一个bean的实例，在session过期后，bean会随之失效；</li><li>global-session: global-session和Portlet应用相关。当你的应用部署在Portlet容器中工作时，它包含很多portlet。如果你想要声明让所有的portlet共用全局的存储变量的话，那么这全局变量需要存储在global-session中。</li></ul><h3 id="Spring-IOC"><a href="#Spring-IOC" class="headerlink" title="Spring IOC"></a>Spring IOC</h3><p>IOC(控制反转):本质就是自己的信息(全类名等)配置在文件中或者加上注解,让容器可以通过反射的方式来创建对象,从而接管对象,代替了自己通过new创建对象.其实就是讲对象的管理创建交给了容器来做.</p><p>依赖注入:在运行过程中,会在需要这个对象的位置坐上一个标记,容器会负责创建对象实例并注入其中;</p><h4 id="Spring-IOC容器的初始化过程"><a href="#Spring-IOC容器的初始化过程" class="headerlink" title="Spring IOC容器的初始化过程"></a>Spring IOC容器的初始化过程</h4><p>IoC容器的初始化就是含有BeanDefinition信息的Resource的定位、载入、解析、注册四个过程，最终我们配置的bean，以beanDefinition的数据结构存在于IoC容器即内存中。</p><h5 id="Resource定位过程"><a href="#Resource定位过程" class="headerlink" title="Resource定位过程"></a>Resource定位过程</h5><p>这个Resource定位指的是BeanDefinition的资源定位，它由ResourceLoader通过统一的Resource接口来完成，这个Resource对各种形式的BeanDefinition的使用提供了统一接口。</p><h5 id="BeanDefinition的载入"><a href="#BeanDefinition的载入" class="headerlink" title="BeanDefinition的载入"></a>BeanDefinition的载入</h5><p>该载入过程把用户定义好的Bean表示成IoC容器内部的数据结构，而这个容器内部的数据结构就BeanDefinition.</p><h5 id="向IoC容器注册这些BeanDefinition"><a href="#向IoC容器注册这些BeanDefinition" class="headerlink" title="向IoC容器注册这些BeanDefinition"></a>向IoC容器注册这些BeanDefinition</h5><p>这个过程是通过调用BeanDefinitionRegistry接口的实现来完成的，这个注册过程把载入过程中解析得到的BeanDefinition向IoC容器进行注册,在IoC容器内部将BeanDefinition注入到一个HashMap中去，Ioc容器是通过这个HashMap来持有这些BeanDefinition数据的。</p><p>容器的初始化是通过AbstractApplicationContext的refresh()实现的。</p><p>整个过程如下图:</p><p><img src="/2020/01/04/hui-tou-zai-kan-spring/1.png" alt></p><h3 id="Spring-AOP"><a href="#Spring-AOP" class="headerlink" title="Spring AOP"></a>Spring AOP</h3><p>面向切面的编程，是一种编程技术，<strong>是OOP（面向对象编程）的补充和完善</strong>。OOP的执行是一种从上往下的流程，并没有从左到右的关系。因此在OOP编程中，会有大量的重复代码。而<strong>AOP则是将这些与业务无关的重复代码抽取出来，然后再嵌入到业务代码当中</strong>。常见的应用有：权限管理、日志、事务管理等。</p><p>AOP有三种植入切面的方法：其一是编译期织入，这要求使用特殊的Java编译器，AspectJ是其中的代表者；其二是类装载期织入，而这要求使用特殊的类装载器，AspectJ和AspectWerkz是其中的代表者；其三为动态代理织入，在运行期为目标类添加增强生成子类的方式，<strong>Spring AOP采用动态代理织入切面</strong>。</p><p>AspectJ是静态代理的增强，所谓的静态代理就是AOP框架会在编译阶段生成AOP代理类，因此也称为编译时增强。</p><p>它会在<strong>编译阶段</strong>将Aspect织入Java字节码中， 运行的时候就是经过增强之后的AOP对象。</p><p>AspectJ在编译时就增强了目标对象，Spring AOP的动态代理则是在每次运行时动态的增强，生成AOP代理对象，区别在于生成AOP代理对象的时机不同，相对来说<strong>AspectJ的静态代理方式具有更好的性能</strong>，但是AspectJ<strong>需要特定的编译器</strong>进行处理，而Spring AOP则无需特定的编译器处理。</p><p>Spring AOP中的动态代理主要有两种方式，<strong>JDK动态代理</strong>和<strong>CGLIB动态代理</strong>。JDK动态代理通过反射来接收被代理的类，并且要求被代理的类必须实现一个接口。<strong>JDK动态代理的核心是InvocationHandler接口和Proxy类</strong>。</p><p>如果目标类没有实现接口，那么Spring AOP会选择使用CGLIB来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态地生成某个类的子类，注意，<strong>CGLIB是通过继承的方式做的动态代理</strong>，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的</p><h4 id="jDK代理"><a href="#jDK代理" class="headerlink" title="jDK代理"></a>jDK代理</h4><p> JDK的动态代理主要涉及到java.lang.reflect包中的两个类：Proxy和InvocationHandler。其中 InvocationHandler是一个接口就是拦截器的接口。，可以通过实现该接口定义横切逻辑，并通过反射机制调用目标类的代码，动态将横切逻辑和业务逻辑编织在一起。</p><h5 id="InvocationHandler的作用"><a href="#InvocationHandler的作用" class="headerlink" title="InvocationHandler的作用"></a>InvocationHandler的作用</h5><p>在动态代理中InvocationHandler是核心，每个代理实例都具有一个关联的调用处理程序(InvocationHandler)。对代理实例调用方法时，将对方法调用进行编码并将其指派到它的调用处理程序(InvocationHandler)的 invoke 方法。所以对代理方法的调用都是通InvocationHadler的invoke来实现中，而invoke方法根据传入的代理对象，方法和参数来决定调用代理的哪个方法</p><h5 id="代理模式"><a href="#代理模式" class="headerlink" title="代理模式"></a>代理模式</h5><p>使用代理模式必须要让代理类和目标类实现相同的接口，客户端通过代理类来调用目标方法，代理类会将所有的方法调用分派到目标对象上反射执行，还可以在分派过程中添加”前置通知”和后置处理（如在调用目标方法前校验权限，在调用完目标方法后打印日志等）等功能。</p><p>具体有如下四步骤：</p><p>1.通过实现 InvocationHandler 接口创建自己的调用处理器；</p><p>2.通过为 Proxy 类指定 ClassLoader 对象和一组 interface 来创建动态代理类；</p><p>3.通过反射机制获得动态代理类的构造函数，其唯一参数类型是调用处理器接口类型；</p><p>4.通过构造函数创建动态代理类实例，构造时调用处理器对象作为参数被传入。</p><p><img src="/2020/01/04/hui-tou-zai-kan-spring/clipboard.png" alt></p><h4 id="利用cglib代理实现AOP"><a href="#利用cglib代理实现AOP" class="headerlink" title="利用cglib代理实现AOP"></a>利用cglib代理实现AOP</h4><p>CGlib是一个强大的,高性能,高质量的Code生成类库。cglib封装了asm，可以在运行期动态生成新的class，它可以在运行期扩展Java类与实现Java接口。 CGLIB是<strong>针对类实现代理</strong>的，主要对指定的类生成一个子类，并覆盖其中的方法， 因为是继承，所以不能使用final来修饰类或方法。和jdk代理实现不同的是，cglib不要求类实现接口。</p><p>JDK动态代理和CGLIB字节码生成的区别？</p><p>CGLib所创建的动态代理对象的性能比JDK的高大概10倍，但CGLib在创建代理对象的时间比JDK大概多8倍，所以对于singleton的代理对象或者具有实例池的代理，因为无需重复的创建代理对象，所以比较适合CGLib动态代理技术，反之选择JDK代理</p><ul><li><p>JDK动态代理只能对实现了接口的类生成代理，而不能针对类</p></li><li><p>CGLIB是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法</p><p>因为是继承，所以该类或方法最好不要声明成final </p></li></ul><p>1、如果目标对象实现了接口，默认情况下会采用JDK的动态代理实现AOP</p><p>2、如果目标对象实现了接口，可以强制使用CGLIB实现AOP</p><p>3、如果目标对象没有实现了接口，必须采用CGLIB库，spring会自动在JDK动态代理和CGLIB之间转换</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>理解KMP回溯</title>
      <link href="/2020/01/03/li-jie-kmp-hui-su/"/>
      <url>/2020/01/03/li-jie-kmp-hui-su/</url>
      
        <content type="html"><![CDATA[<h4 id="理解KMP回溯"><a href="#理解KMP回溯" class="headerlink" title="理解KMP回溯"></a>理解KMP回溯</h4><p>相信大家都看过KMP算法，但是对于它的回溯确是难以理解。我们先来看一下KMP中的next数组生成代码：</p><pre><code>    //用于生成next数组    private static int[] get_next(String target){        int[] next = new int[target.length()];        next[0] = -1;        int i = 0, j = -1;        while(i &lt; target.length() - 1){            if (j == -1 || target.charAt(i) == target.charAt(j)) {                ++i;                ++j;                next[i] = j;            } else {                j = next[j];            }        }        return next;    }</code></pre><ul><li>其中数组的next中的值计算方式是：</li></ul><p>next[j] = Max{k | 1&lt;k&lt;j,且‘p1p2…pk’=‘p(j-k)…p(j - 1)’}</p><ul><li><h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5></li></ul><p><em>简单来说next[j]表示的就是两个相等的字符串的长度，这两个字符串分别是从头开始记的长度为next[j]的和以next[j]的前一个字符结尾的长度为next[j]。</em></p><ul><li><h5 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h5></li></ul><p>例如：字符串”ababaaaba” next = [-1,0,0,1,2,3,1,1,2]<br>其中的回溯环节就是从next[5] = 3 到 next[6] = 1;</p><p>其中next[5]时：是”ababa”中前缀”aba”与后缀”aba”的长度，当i = 6时，”ababaa”中”a”不等于”b”,所以回溯到j = next[j],其中j为现在next[5]的值。</p><ul><li><h5 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h5></li></ul><p><strong>我开始也是很不明白为什么就可以直接回到j = next[next[5]] = 0处开始向后比较，后来仔细研究发现原因是，通过前面的比较它已经排除了所有的前缀字符串等于后缀字符串的长度大于回溯到当前j的可能 。</strong><br>就拿上面的“ababa”到“ababaa”举例：<br>其实我们想不通的无非就是它是怎么排除“aba”!=”baa”转而直接去判断前缀“ab”是否等于后缀“aa”,后来我仔细分析才发现因为如果前面的“aba” = “baa”要成立，必须有“前缀ab”等于后缀“ba”,而得到next[5]=3的时候已经隐式的得到的第一个“ba”等于第二个“ba”(当时是“aba” = “aba”)<br>从而有“aba”中三个值都应该相等，与前面矛盾。可能你早就看不懂我在说什么了，来一点数学表达式比较实际：</p><ul><li><h5 id="数学证明"><a href="#数学证明" class="headerlink" title="数学证明"></a>数学证明</h5></li></ul><p>②开始有p1p2….pj = pi - j ….pi-1，可以得出pj = pj-1  j = 1,2,…<br>假设 next[j] = k 就有 p1p2…pk = pj-k …pj-1    k = 1,2…<br>若加入pi != pj + 1,则需要回溯到判断pk 是否等于pj;<br>首先证明：pi-j+1…pi ！= p1p2…pj,反证：假设：pi-j+1…pi = p1p2…p，又p1p2….pj = pi - j ….pi-1<br>所以有pi - j ….pi-1 =pi-j+1…pi ,得到pi-j=pi-j+1=…=pi;与前面矛盾，所以有pi-j+1…pi ！= p1p2…pj<br>同理可以得出pi-j+2…pi ！= p1p2…pj-1  。。。。。pi-j+k…pi ！= p1p2…pj-k+1  。。。。。<br>所以可以直接回溯到j = next[j]继续向后判断</p><p>KMP完整代码</p><pre><code>    private static int[] get_next(String target){        int[] next = new int[target.length()];        next[0] = -1;        int i = 0, j = -1;        while(i &lt; target.length() - 1){            if (j == -1 || target.charAt(i) == target.charAt(j)) {                ++i;                ++j;                next[i] = j;            } else {                j = next[j];            }        }        return next;    }    int kmp(String s, String pattern) {        int i = 0,j = 0;        int slen = s.length(), plen = pattern.length();        int[] next = get_next(pattern);        while (i &lt; slen &amp;&amp; j &lt; plen) {            if (s.charAt(i) == pattern.charAt(j)) {                i++;                j++;            } else {                if (next[j] == -1) {                    i++;                    j = 0;                } else {                    j = next[j];                }            }            if (j == plen) {                return i - j;            }        }        return -1;    }</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>使用VMware安装linux虚拟机</title>
      <link href="/2019/12/29/shi-yong-vmware-an-zhuang-linux-xu-ni-ji/"/>
      <url>/2019/12/29/shi-yong-vmware-an-zhuang-linux-xu-ni-ji/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=552192975&auto=0&height=66"></iframe></div><h1 id="使用VMware安装linux虚拟机"><a href="#使用VMware安装linux虚拟机" class="headerlink" title="使用VMware安装linux虚拟机"></a>使用VMware安装linux虚拟机</h1><h3 id="使用背景"><a href="#使用背景" class="headerlink" title="使用背景"></a>使用背景</h3><p><em>Linux是一种自由和开放源码的操作系统，存在着许多不同的Linux发行版本，但它们都使用了Linux内核。现在的服务器基本都是使用linux,其中CentOS使用广泛,还有ubuntu也是linux中的佼佼者.业内也说,凡是<strong>java开发,不懂linux均是扯淡.</strong>本文主要为后面搭建基于Hadoop集群的大数据大数据平台打下基础。</em></p><h4 id="linux具有如下优点"><a href="#linux具有如下优点" class="headerlink" title="linux具有如下优点"></a>linux具有如下优点</h4><ul><li>开源</li><li>多用户，多任务，丰富的网络功能，可靠的系统安全，良好的可移植性，具有标准兼容性</li><li>良好的用户界面，出色的速度性能</li><li>服务器不使用图形化界面(图形界面占用资源)</li><li>机房部署方便，无需配置操作界面</li></ul><p><strong>下载地址</strong><a href="http://www.centos.org/" target="_blank" rel="noopener">:http://www.centos.org/</a></p><h3 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h3><ul><li>Windows10</li><li>VMware Workstation12</li><li>CentOS7</li></ul><h4 id="VMware-Workstation12安装"><a href="#VMware-Workstation12安装" class="headerlink" title="VMware Workstation12安装"></a>VMware Workstation12安装</h4><p>①双击VMware-workstation-full-版本号.exe</p><p>②点击next</p><p>③选择Typical(你要是想自己配置也可以选custom 不推荐)</p><p>④选择安装目录</p><p>⑤想检查升级就勾上(check for product updates on startup),否则直接下一步</p><p>⑥选择创建快捷方式的位置,然后下一步</p><p>⑦点击continue完成</p><p>⑧Finish完成</p><p><strong>注意:如果你不熟悉就按部就班来,不要有什么骚操作,我记得我开始安装的时候禁用了哪两个网卡,后来哪两个网卡找不到了,我就把这个卸载了重新装,还是不行,这个问题的解决还是因为我一个月后重装了电脑</strong></p><h4 id="CentOS7安装"><a href="#CentOS7安装" class="headerlink" title="CentOS7安装"></a>CentOS7安装</h4><p>①安装VMware Workstation</p><p>②打开VM,点击创建新的虚拟机</p><p>③选择 典型（推荐）→ 下一步 </p><p>④选择稍后安装操作系统再点击下一步</p><p>⑤选择操作系统和版本(linux 64)</p><p>⑥输入虚拟机名称和安装路径</p><p>⑦设置磁盘大小并选中将虚拟磁盘拆成多个文件</p><p>⑧自定义硬件</p><p>⑨选择CentOS安装镜像文件</p><p><img src="/2019/12/29/shi-yong-vmware-an-zhuang-linux-xu-ni-ji/1577631145598.png" alt></p><p>⑩开机启动后选择Install CentOS 7并enter</p><ul><li>弹出如下图形化的安装界面：</li></ul><p><img src="/2019/12/29/shi-yong-vmware-an-zhuang-linux-xu-ni-ji/format.png" alt></p><ul><li>日期和时间：</li></ul><p><img src="/2019/12/29/shi-yong-vmware-an-zhuang-linux-xu-ni-ji/format1.png" alt><img src="/2019/12/29/shi-yong-vmware-an-zhuang-linux-xu-ni-ji/format2.png" alt></p><ul><li>如果你安装的是英文版，需要将时区改为上海。</li></ul><p><img src="/2019/12/29/shi-yong-vmware-an-zhuang-linux-xu-ni-ji/format3.png" alt><img src="/2019/12/29/shi-yong-vmware-an-zhuang-linux-xu-ni-ji/format4.png" alt><img src="/2019/12/29/shi-yong-vmware-an-zhuang-linux-xu-ni-ji/20171126014938900.png" alt></p><ul><li><strong>网络和主机名</strong></li></ul><p><img src="/2019/12/29/shi-yong-vmware-an-zhuang-linux-xu-ni-ji/20171126015037602.png" alt><img src="/2019/12/29/shi-yong-vmware-an-zhuang-linux-xu-ni-ji/20171126015051216.png" alt></p><ul><li>然后选择开始安装**</li></ul><p><img src="/2019/12/29/shi-yong-vmware-an-zhuang-linux-xu-ni-ji/format7.png" alt>基本的系统就安装好了</p><h3 id="配置网络"><a href="#配置网络" class="headerlink" title="配置网络"></a>配置网络</h3><ul><li>linu有三种网络模式,分别是Host-Only、NAT、桥接。一般安装好以后会默认选择NAT。</li></ul><ul><li>进入之后修改ip地址信息</li></ul><pre><code>vi /etc/ sysconfig/network-scripts/ifcfg-eth0DEVICE=eth0 #网卡名称HWADDR=08:00:27:8E:9D:25 #MAC地址TYPE=Ethernet #网络类型,这里是以太网UUID=5f2d815e-bd3b-4995-9009-823542e77304ONBOOT=yes NM_CONTROLLED=yesBOOTPROTO=staticSTATIC=trueIPADDR=192.168.1.21 #ip地址NETMASK=255.255.255.0 #子网掩码GATEWAY=192.168.1.1 #网管DNS1=202.202.0.33 #域名解析地址DNS2=114.114.114.114DNS3=8.8.8.8</code></pre><ul><li>配置好以后重启网络服务</li></ul><pre><code>services network restart</code></pre><ul><li>ifconfig查看IP地址</li></ul><pre><code>ifconfigeth0      Link encap:Ethernet  HWaddr 08:00:27:8E:9D:25            inet addr:192.168.1.21  Bcast:192.168.1.255  Mask:255.255.255.0          inet6 addr: fe80::a00:27ff:fe8e:9d25/64 Scope:Link          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1          RX packets:1756623 errors:0 dropped:0 overruns:0 frame:0          TX packets:1952463 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:1000           RX bytes:1445482120 (1.3 GiB)  TX bytes:1626059931 (1.5 GiB)lo        Link encap:Local Loopback            inet addr:127.0.0.1  Mask:255.0.0.0          inet6 addr: ::1/128 Scope:Host          UP LOOPBACK RUNNING  MTU:65536  Metric:1          RX packets:2258 errors:0 dropped:0 overruns:0 frame:0          TX packets:2258 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:0           RX bytes:590708 (576.8 KiB)  TX bytes:590708 (576.8 KiB)</code></pre><ul><li>ping ip地址测试网络是否配置好</li></ul><pre><code>ping www.baidu.com</code></pre><p><strong>按照以上操作完成安装以后可以直接克隆改虚拟机，然后修改配置就可以生成多台</strong></p><p><strong>在每个主机的/etc/hosts文件设置上每个主机的ip和名字的映射关系</strong></p><pre><code>vi /etc/hosts192.168.1.21 master192.168.1.23 slave1192.168.1.24 slave2192.168.1.25 slave3</code></pre><p><img src="/2019/12/29/shi-yong-vmware-an-zhuang-linux-xu-ni-ji/1577691474359.png" alt></p><h4 id="配置免密登录"><a href="#配置免密登录" class="headerlink" title="配置免密登录"></a>配置免密登录</h4><ul><li>主要用于两个机器之间相互登录不需要验证</li></ul><p>①在第一台机器使用命令ssh-keygen -t rsa生成私钥和秘钥</p><pre><code>ssh-keygen -t rsa</code></pre><p>②复制到另一台机器</p><pre><code>ssh-copy-id root@slave1</code></pre><p><strong>如此就可以实现slave登录master免密,按照这个做法,每两台机器都配置上。</strong></p><h5 id="科普：免密登录原理"><a href="#科普：免密登录原理" class="headerlink" title="科普：免密登录原理"></a>科普：免密登录原理</h5><p><img src="/2019/12/29/shi-yong-vmware-an-zhuang-linux-xu-ni-ji/1577692561419.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>hexo博文包含图片的坑</title>
      <link href="/2019/12/29/hexo-bo-wen-bao-han-tu-pian-de-keng/"/>
      <url>/2019/12/29/hexo-bo-wen-bao-han-tu-pian-de-keng/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=414414&auto=0&height=66"></iframe></div><h1 id="hexo博文包含图片的坑"><a href="#hexo博文包含图片的坑" class="headerlink" title="hexo博文包含图片的坑"></a>hexo博文包含图片的坑</h1><h3 id="网上有很多关于这个的教程-主要的总结如下"><a href="#网上有很多关于这个的教程-主要的总结如下" class="headerlink" title="网上有很多关于这个的教程,主要的总结如下"></a>网上有很多关于这个的教程,主要的总结如下</h3><ul><li>①修改博客目录下的_config_yml的post_asset_folder为true</li></ul><pre class="line-numbers language-java"><code class="language-java">post_asset_folder<span class="token operator">:</span> <span class="token boolean">true</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>②安装hexo-asset-image插件</li></ul><pre><code>npm install hexo-asset-image --save</code></pre><ul><li>③hexo new  file_name 时会在source/_post/下生成file_name的文件夹,将需要使用的图片放置在里面,然后使用相对路径引入</li></ul><pre><code>![用于图片加载失败时显示的内容](/file_name/image_name)</code></pre><ul><li>如此博客中的图片最后会和.md文件一起生成到public\2019\12\27\file_name中,这样在hexe g 时,可以看到命令窗口会打印修改后的路径,如下</li></ul><pre><code>Start processingupdate link as:--&gt;/2019/12/27/first/1577523021175.pngupdate link as:--&gt;/2019/12/27/first/1577523021175.png</code></pre><h3 id="我遇到的问题"><a href="#我遇到的问题" class="headerlink" title="我遇到的问题"></a>我遇到的问题</h3><pre><code>Start processingupdate link as:--&gt;.io//2019/12/27/first/1577523021175.pngupdate link as:--&gt;.io//2019/12/27/first/1577523021175.png</code></pre><ul><li>经过一番搜寻,发现hexo-asset-image会将图片的地址修改,具体的源码信息可见\node_modules\hexo-asset-image\index.js,打开后内容如下:</li></ul><pre><code>&#39;use strict&#39;;var cheerio = require(&#39;cheerio&#39;);function getPosition(str, m, i) {  return str.split(m, i).join(m).length;}hexo.extend.filter.register(&#39;after_post_render&#39;, function(data){  var config = hexo.config;  if(config.post_asset_folder){    var link = data.permalink;    var beginPos = getPosition(link, &#39;/&#39;, 3) + 1;    var appendLink = &#39;&#39;;    // In hexo 3.1.1, the permalink of &quot;about&quot; page is like &quot;.../about/index.html&quot;.    // if not with index.html endpos = link.lastIndexOf(&#39;.&#39;) + 1 support hexo-abbrlink    if(/.*\/index\.html$/.test(link)) {      // when permalink is end with index.html, for example 2019/02/20/xxtitle/index.html      // image in xxtitle/ will go to xxtitle/index/      appendLink = &#39;index/&#39;;      var endPos = link.lastIndexOf(&#39;/&#39;);    }    else {      var endPos = link.lastIndexOf(&#39;.&#39;) ;    }    link = link.substring(beginPos, endPos) + &#39;/&#39; + appendLink;    var toprocess = [&#39;excerpt&#39;, &#39;more&#39;, &#39;content&#39;];    for(var i = 0; i &lt; toprocess.length; i++){      var key = toprocess[i];      var $ = cheerio.load(data[key], {        ignoreWhitespace: false,        xmlMode: false,        lowerCaseTags: false,        decodeEntities: false      });      $(&#39;img&#39;).each(function(){        if ($(this).attr(&#39;src&#39;)){          // For windows style path, we replace &#39;\&#39; to &#39;/&#39;.          var src = $(this).attr(&#39;src&#39;).replace(&#39;\\&#39;, &#39;/&#39;);          if(!(/http[s]*.*|\/\/.*/.test(src)            || /^\s+\//.test(src)            || /^\s*\/uploads|images\//.test(src))) {            // For &quot;about&quot; page, the first part of &quot;src&quot; can&#39;t be removed.            // In addition, to support multi-level local directory.            var linkArray = link.split(&#39;/&#39;).filter(function(elem){              return elem != &#39;&#39;;            });            var srcArray = src.split(&#39;/&#39;).filter(function(elem){              return elem != &#39;&#39; &amp;&amp; elem != &#39;.&#39;;            });            if(srcArray.length &gt; 1)            srcArray.shift();            src = srcArray.join(&#39;/&#39;);            $(this).attr(&#39;src&#39;, config.root + link + src);            console.info&amp;&amp;console.info(&quot;update link as:--&gt;&quot;+config.root + link + src);          }        }else{          console.info&amp;&amp;console.info(&quot;no src attr, skipped...&quot;);          console.info&amp;&amp;console.info($(this));        }      });      data[key] = $.html();    }  }});</code></pre><ul><li>通过查看源码发现里面有对生成博客图片的地址修改:</li></ul><pre><code>link = link.substring(beginPos, endPos) + &#39;/&#39; + appendLink;</code></pre><ul><li>通过排查发现图片的路径的endPos为:</li></ul><pre><code>var endPos = link.lastIndexOf(&#39;.&#39;) ;</code></pre><ul><li>我打印data.permalink得到</li></ul><pre><code>http://tigerLuHai.github.io/2019/12/27/first/</code></pre><p>如此在截取字符串的时候就会多出四个字符  <strong>.io/</strong></p><p>最后发现这段代码的作用就是要将data.permalink中路径的<a href="https://tigerLuhai.gituhub.io/去掉,因为在后面部署到github时使用相对路径访问会重新加上这个前缀,如果这里有就会重复,导致地址为https://tigerLuhai.gituhub.io/http://tigerLuHai.github.io/2019/12/27/first/的现象" target="_blank" rel="noopener">https://tigerLuhai.gituhub.io/去掉,因为在后面部署到github时使用相对路径访问会重新加上这个前缀,如果这里有就会重复,导致地址为https://tigerLuhai.gituhub.io/http://tigerLuHai.github.io/2019/12/27/first/的现象</a>.</p><p>明白了需求就可以修改代码为</p><pre><code>var endPos = link.lastIndexOf(&#39;/&#39;) ;</code></pre><p>这样就可以正常部署了.</p>]]></content>
      
      
      <categories>
          
          <category> tool </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>FastDFS分布式文件系统安装使用教程</title>
      <link href="/2019/12/28/fastdfs-fen-bu-shi-wen-jian-xi-tong-an-zhuang-shi-yong-jiao-cheng/"/>
      <url>/2019/12/28/fastdfs-fen-bu-shi-wen-jian-xi-tong-an-zhuang-shi-yong-jiao-cheng/</url>
      
        <content type="html"><![CDATA[<div align="middle"><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="86" src="//music.163.com/outchain/player?type=2&id=479553545&auto=1&height=66"></iframe></div><h1 id="FastDFS分布式文件系统安装使用教程"><a href="#FastDFS分布式文件系统安装使用教程" class="headerlink" title="FastDFS分布式文件系统安装使用教程"></a>FastDFS分布式文件系统安装使用教程</h1><h3 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h3><p>分布式文件系统用于<strong>海量</strong>文件存储及传输访问的瓶颈问题，对海量视频的管理、对<strong>海量</strong>图片的管理等,FastDFS与其他分布式文件系统相比的一个显著优点就是特别<strong>适合大量小文件(图片等)的存储,因为它在存储时没有对文件切片分割.</strong></p><h3 id="主流的分布式文件系统"><a href="#主流的分布式文件系统" class="headerlink" title="主流的分布式文件系统"></a>主流的分布式文件系统</h3><h4 id="①NFS"><a href="#①NFS" class="headerlink" title="①NFS"></a>①NFS</h4><p><img src="/2019/12/28/fastdfs-fen-bu-shi-wen-jian-xi-tong-an-zhuang-shi-yong-jiao-cheng/1577523021175.png" alt="1577523021175"></p><h4 id="②GFS"><a href="#②GFS" class="headerlink" title="②GFS"></a>②GFS</h4><p><img src="/2019/12/28/fastdfs-fen-bu-shi-wen-jian-xi-tong-an-zhuang-shi-yong-jiao-cheng/1577523108700.png" alt="1577523108700"></p><h4 id="③HDFS"><a href="#③HDFS" class="headerlink" title="③HDFS"></a>③HDFS</h4><p><img src="/2019/12/28/fastdfs-fen-bu-shi-wen-jian-xi-tong-an-zhuang-shi-yong-jiao-cheng/1577523192987.png" alt="1577523192987"></p><h4 id="④FastDFS"><a href="#④FastDFS" class="headerlink" title="④FastDFS"></a>④FastDFS</h4><p>FastDFS是用c语言编写的一款开源的分布式文件系统，它是由淘宝资深架构师余庆编写并开源。FastDFS专为互联 网量身定制，充分考虑了冗余备份、负载均衡、线性扩容等机制，并注重高可用、高性能等指标，使用FastDFS很 容易搭建一套高性能的文件服务器集群提供文件上传、下载等服务。</p><p><strong>FastDFS架构包括 Tracker server和Storageserver。客户端请求Tracker server进行文件上传、下载，通过Tracker server调度最终由Storage server完成文件上传和下载。</strong> </p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><h4 id="服务器环境"><a href="#服务器环境" class="headerlink" title="服务器环境"></a>服务器环境</h4><ul><li>CentOS6.9(CenttOS安装过程一致)</li></ul><ul><li>IP: 192.168.1.21,192.168.1.23,192.168.1.24,192.168.1.25</li></ul><h4 id="安装Linux基本环境"><a href="#安装Linux基本环境" class="headerlink" title="安装Linux基本环境"></a>安装Linux基本环境</h4><p>参见Hadoop的安装使用教程中Linux环境搭建</p><h4 id="安装gcc环境-FastDFS是由c语言编写"><a href="#安装gcc环境-FastDFS是由c语言编写" class="headerlink" title="安装gcc环境(FastDFS是由c语言编写)"></a>安装gcc环境(FastDFS是由c语言编写)</h4><pre class="line-numbers language-linux"><code class="language-linux">yum install gcc-c++<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="安装-libevent"><a href="#安装-libevent" class="headerlink" title="安装 libevent"></a>安装 libevent</h4><pre class="line-numbers language-yum"><code class="language-yum">yum -y install libevent<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="安装-libfastcommon"><a href="#安装-libfastcommon" class="headerlink" title="安装 libfastcommon"></a>安装 libfastcommon</h4><pre><code>将 libfastcommonV1.0.7.tar.gz 拷贝至/usr/local/下cd /usr/localtar -zxvf libfastcommonV1.0.7.tar.gzcd libfastcommon-1.0.7./make.sh./make.sh install</code></pre><p>注意：<strong>libfastcommon 安装好后会自动将库文件拷贝至/usr/lib64 下，由于 FastDFS 程序引用 usr/lib 目录所以需要将/usr/lib64 下的库文件拷贝至/usr/lib 下。</strong></p><p>需要拷贝的文件</p><p><img src="/2019/12/28/fastdfs-fen-bu-shi-wen-jian-xi-tong-an-zhuang-shi-yong-jiao-cheng/1577524769079.png" alt="1577524769079"></p><h4 id="tracker-编译安装"><a href="#tracker-编译安装" class="headerlink" title="tracker 编译安装"></a>tracker 编译安装</h4><pre class="line-numbers language-将"><code class="language-将">将 FastDFS_v5.05.tar.gz 拷贝至/usr/local/下tar -zxvf FastDFS_v5.05.tar.gzcd FastDFS./make.sh 编译./make.sh install 安装<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>安装成功将安装目录下的 conf 下的文件拷贝到/etc/fdfs/下。</p><pre><code>cp -r /usr/local/FastDFS/conf/ /etc/fdfs/</code></pre><h5 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h5><p>安装成功后进入/etc/fdfs目录</p><p><img src="/2019/12/28/fastdfs-fen-bu-shi-wen-jian-xi-tong-an-zhuang-shi-yong-jiao-cheng/1577525471795.png" alt="1577525471795"></p><p>拷贝一份新的 tracker 配置文件：</p><pre><code>cp tracker.conf.sample tracker.conf</code></pre><p>修改 tracker.conf</p><pre><code>vi tracker.confbase_path=/home/yuqing/FastDFS #数据(日志等)存储路径,自己设置http.server_port=80 #配置 http 端口：</code></pre><h5 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h5><pre><code>/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf start</code></pre><p>查看端口</p><pre><code>netstat -nltp</code></pre><p><img src="/2019/12/28/fastdfs-fen-bu-shi-wen-jian-xi-tong-an-zhuang-shi-yong-jiao-cheng/1577535905825.png" alt="1577535905825"></p><h4 id="storage-安装"><a href="#storage-安装" class="headerlink" title="storage 安装"></a>storage 安装</h4><ul><li>安装 libevent</li><li>安装 libfastcommon</li><li>编译安装(与tracker相同)</li></ul><h5 id="配置-1"><a href="#配置-1" class="headerlink" title="配置"></a>配置</h5><pre><code>vi storage.confgroup_name=group1 #分组,同一分组为设置冗余防止宕机不可用base_path=/home/yuqing/FastDFS #数据存储路径,自己设置store_path0=/home/yuqing/FastDFS #文件存储路径,自己设置tracker_server=192.168.101.3:22122 #配置 tracker 服务器:IPtracker_server=192.168.1.21:22122 #如果有多个则配置多个 trackerhttp.server_port=80</code></pre><h5 id="启动-1"><a href="#启动-1" class="headerlink" title="启动"></a>启动</h5><pre><code>/usr/bin/fdfs_storaged /etc/fdfs/storage.conf start</code></pre><h5 id="分发配置"><a href="#分发配置" class="headerlink" title="分发配置"></a>分发配置</h5><p>将FastDFS分发到各个节点,并修改配置,分发脚本如下</p><pre><code>#!/bin/bashpcount=$#if((pcount=0));thenecho no args;exit;fip1=$1fname=`basename $p1`echo fname=$fnamepdir=`cd -P $(dirname $p1);pwd`echo pdir=$pdiruser=`whoami`echo user=$userfor((host=1;host&lt;4;host++));doecho ------------salve$host-------------rsync -rvl $pdir/$fname $user@slave$host:$pdirdone</code></pre><h3 id="利用可通过-usr-bin-fdfs-test-程序测试"><a href="#利用可通过-usr-bin-fdfs-test-程序测试" class="headerlink" title="利用可通过/usr/bin/fdfs_test 程序测试"></a>利用可通过/usr/bin/fdfs_test 程序测试</h3><p>修改/etc/fdfs/client.conf</p><p>tracker_server 根据自己部署虚拟机的情况配置</p><pre><code>base_path = /home/yuqing/fastdfstracker-server=192.168.1.21:22122</code></pre><p>使用格式：</p><pre><code>/usr/bin/fdfs_test 客户端配置文件地址 upload 上传文件</code></pre><p>比如将/home 下的图片上传到 FastDFS 中：</p><pre><code>/usr/bin/fdfs_test /etc/fdfs/client.conf upload /home/tomcat.png</code></pre><p>打印日志如下:</p><pre><code>This is FastDFS client test program v5.05Copyright (C) 2008, Happy Fish / YuQingFastDFS may be copied only under the terms of the GNU GeneralPublic License V3, which may be found in the FastDFS source kit.Please visit the FastDFS Home Page http://www.csource.org/ for more detail.[2019-12-28 20:13:02] DEBUG - base_path=/home/fastdfs, connect_timeout=30, network_timeout=60, tracker_server_count=1, anti_steal_token=0, anti_steal_secret_key length=0, use_connection_pool=0, g_connection_pool_max_idle_time=3600s, use_storage_id=0, storage server id count: 0tracker_query_storage_store_list_without_group:         server 1. group_name=, ip_addr=192.168.1.24, port=23000group_name=group1, ip_addr=192.168.1.24, port=23000storage_upload_by_filenamegroup_name=group1, remote_filename=M00/00/00/wKgBGF4HRs6AOwJJAAId77F78II587.pngsource ip address: 192.168.1.24file timestamp=2019-12-28 20:13:02file size=138735file crc32=2977689730example file url: http://192.168.1.24/group1/M00/00/00/wKgBGF4HRs6AOwJJAAId77F78II587.pngstorage_upload_slave_by_filenamegroup_name=group1, remote_filename=M00/00/00/wKgBGF4HRs6AOwJJAAId77F78II587_big.pngsource ip address: 192.168.1.24file timestamp=2019-12-28 20:13:02file size=138735file crc32=2977689730</code></pre><p><a href="http://192.168.1.24/group1/M00/00/00/wKgBGF4HRs6AOwJJAAId77F78II587.png" target="_blank" rel="noopener">http://192.168.1.24/group1/M00/00/00/wKgBGF4HRs6AOwJJAAId77F78II587.png</a><br>就是文件的下载路径。对应服务器的base_path/fdfs_storage/data/00/00/wKgBGF4HRs6AOwJJAAId77F78II587.png文件</p><p>现在还没有和 nginx 整合无法使用 http 下载。</p><h3 id="Nginx整合FastDFS"><a href="#Nginx整合FastDFS" class="headerlink" title="Nginx整合FastDFS"></a>Nginx整合FastDFS</h3><h4 id="FastDFS-nginx-module"><a href="#FastDFS-nginx-module" class="headerlink" title="FastDFS-nginx-module"></a>FastDFS-nginx-module</h4><p>将 FastDFS-nginx-module_v1.16.tar.gz 传 至 fastDFS 的 storage 服 务 器 的</p><p>/usr/local/下，执行如下命令：</p><pre><code>cd /usr/localtar -zxvf FastDFS-nginx-module_v1.16.tar.gzcd FastDFS-nginx-module/srcvi config</code></pre><p><img src="/2019/12/28/fastdfs-fen-bu-shi-wen-jian-xi-tong-an-zhuang-shi-yong-jiao-cheng/1577536510714.png" alt="1577536510714"></p><p>将/usr/local修改为/usr，注意这里有三场，不要改漏了。</p><p>将 FastDFS-nginx-module/src 下的 mod_FastDFS.conf 拷贝至/etc/fdfs/下</p><pre><code>cp mod_FastDFS.conf /etc/fdfs/vi /etc/fdfs/mod_FastDFS.confbase_path=/home/FastDFS # 保持和之前安装时一致tracker_server=192.168.1.21:22122url_have_group_name=true #url 中包含 group 名称store_path0=/home/fastdfs/fdfs_storage #指定文件存储路径,和之前一致</code></pre><p>将 libfdfsclient.so 拷贝至/usr/lib 下</p><pre><code>cp /usr/lib64/libfdfsclient.so /usr/lib/</code></pre><p>创建 nginx/client 目录</p><pre><code>mkdir -p /var/temp/nginx/client</code></pre><h4 id="安装nginx"><a href="#安装nginx" class="headerlink" title="安装nginx"></a>安装nginx</h4><p>详细教程可见nginx使用感悟</p><p>将 nginx-1.8.0.tar.gz 拷贝到/usr/local 下</p><p>解压 nginx-1.8.0.tar.gz</p><p>进入 nginx-1.8.0 目录，执行如下配置命令：</p><pre><code>./configure --add-module=/usr/local/FastDFS-nginx-module/srcmake make install</code></pre><p>在nginx中增加如下虚拟机配置:</p><p>storage配置:</p><pre><code>server { listen 80; server_name 192.168.1.23; 本机ip location /group1/M00/{ root /home/FastDFS/fdfs_storage/data;  #以自己配置的地址为准 ngx_FastDFS_module; } }</code></pre><p>tracker配置:</p><pre><code>#storage 群 group1 组upstream storage_server_group1{ server 192.168.1.23:80 weight=10;server 192.168.1.24:80 weight=10; } #storage 群 group2 组upstream storage_server_group2{ server 192.168.1.25:80 weight=10; } server {listen 80;server_name ccc.test.com;location /group1{proxy_redirect off;proxy_set_header Host $host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_pass http://storage_server_group1;}location /group2{proxy_redirect off;proxy_set_header Host $host;proxy_set_header X-Real-IP $remote_addr;proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;proxy_pass http://storage_server_group2; } }</code></pre><h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><p>使用浏览器 http 访问文件，这里访问上传图片测试的文件：</p><p>访问 storage：<a href="http://192.168.1.24/group1/M00/00/00/wKgBGF4HRs6AOwJJAAId77F78II587.png" target="_blank" rel="noopener">http://192.168.1.24/group1/M00/00/00/wKgBGF4HRs6AOwJJAAId77F78II587.png</a></p><p><img src="/2019/12/28/fastdfs-fen-bu-shi-wen-jian-xi-tong-an-zhuang-shi-yong-jiao-cheng/1577537632508.png" alt="1577537632508"></p><p>ip 地址改为 192.168.1.24也可以访问到文件，因为同一个分组的 storage 文件互相同步。</p><h3 id="编写java代码上传下载文件"><a href="#编写java代码上传下载文件" class="headerlink" title="编写java代码上传下载文件"></a>编写java代码上传下载文件</h3><p><strong>SpringBoot测试方案</strong></p><p>引入依赖</p><pre><code>        &lt;dependency&gt;            &lt;groupId&gt;net.oschina.zcx7878&lt;/groupId&gt;            &lt;artifactId&gt;fastdfs-client-java&lt;/artifactId&gt;            &lt;version&gt;1.27.0.0&lt;/version&gt;        &lt;/dependency&gt;</code></pre><pre><code>@SpringBootTest@RunWith(SpringRunner.class)public class TestFastDFS {    @Test    public void upload() throws IOException, MyException {        //加载fastdfs-client.properties配置文件        ClientGlobal.initByProperties(&quot;config/fastdfs-client.properties&quot;);        //定义TrackerClient,用于请求TrackerClient        TrackerClient trackerClient = new TrackerClient();        //创建TrackerServer        TrackerServer trackerServer = trackerClient.getConnection();        //通过TrackerServer获取storeServer        StorageServer storeServer = trackerClient.getStoreStorage(trackerServer);        //通过TrackerServer and storeServer 创建storageClient1        StorageClient1 storageClient1 = new StorageClient1(trackerServer, storeServer);        //fileId group1/M00/00/00/wKgBF13ebJuALXhqAAI6t5YoKLQ94..log        String fileId = storageClient1.upload_file1(&quot;C:\\Users\\tiger\\Pictures\\Feedback\\{A687785D-19C3-4B2E-A00A-2667141271EB}\\Capture001.png&quot;, &quot;.png&quot;, null);        System.out.println(fileId);        //获取tracker客户端    }    @Test    public void download() throws IOException, MyException {        //加载fastdfs-client.properties配置文件        ClientGlobal.initByProperties(&quot;config/fastdfs-client.properties&quot;);        //定义TrackerClient,用于请求TrackerClient        TrackerClient trackerClient = new TrackerClient();        //创建TrackerServer        TrackerServer trackerServer = trackerClient.getConnection();        //通过TrackerServer获取storeServer        StorageServer storeServer = trackerClient.getStoreStorage(trackerServer);        //通过TrackerServer and storeServer 创建storageClient1        StorageClient1 storageClient1 = new StorageClient1(trackerServer, storeServer);        //fileId group1/M00/00/00/wKgBF13ebJuALXhqAAI6t5YoKLQ94..log        byte[] bytes = storageClient1.download_file1(&quot;group1/M00/00/00/wKgBF13ebJuALXhqAAI6t5YoKLQ94..log&quot;);        FileOutputStream fos = new FileOutputStream(new File(&quot;hello&quot;));        fos.write(bytes);    }}</code></pre><p>config/fastdfs-client.properties</p><pre><code>fastdfs.connect_timeout_in_seconds = 5fastdfs.network_timeout_in_seconds = 30fastdfs.charset = UTF-8fastdfs.tracker_servers = 192.168.1.21:22122</code></pre><p>运行upload得到路径</p><pre><code>group1/M00/00/00/wKgBGF4HUZiAOKLgAB7xNcvo_Vw00..png根据自己配置的路径可以得到访问的http协议路径为:http://192.168.1.21/group1/M00/00/00/wKgBGF4HUZiAOKLgAB7xNcvo_Vw00..png</code></pre><p>效果如下</p><p><img src="/2019/12/28/fastdfs-fen-bu-shi-wen-jian-xi-tong-an-zhuang-shi-yong-jiao-cheng/1577538377340.png" alt="1577538377340"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p><strong>FastDFS相对于HDFS等分布式文件的优势在于它不切分文件,所以下载文件的时候没有拼装文件的过程,而且可以锁定一台机器进行网络I/O,所以速度很快.不过正所谓这也是它的缺点,这导致它不能用于存储大文件.所以FastDFS适合存储大量图片小视频之类的文件.</strong></p><h3 id="安装过程遇到的一些问题"><a href="#安装过程遇到的一些问题" class="headerlink" title="安装过程遇到的一些问题"></a>安装过程遇到的一些问题</h3><p>①安装nginx No rule to make target “/usr/local/fastdfs-nginx-module/src/ngx_http_fastdfs_module.c”, needed by objs/addon/src/ngx_http_fastdfs_module.o . Stop. 修改fastdfs-nginx-module/src/config文件中的路径,删除local(注意一共有三个)</p><p>②nginx安装cp: <code>conf/koi-win&#39; and</code>/usr/local/nginx/conf/koi-win’ are the same file 解决 将./configure –prefix=/usr/local/nginx 改为 ./configure –prefix=/usr/local/nginx –conf-path=/usr/local/nginx/nginx.conf</p><p>③nginx编码严格.直接复制会出现nginx: [emerg] unknown directive “ “ in /usr/local/nginx-1.12.0-storage/conf/nginx.conf:49）：所以需要手动输入nginx.conf</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch原理及基本使用</title>
      <link href="/2019/12/27/elasticsearch-yuan-li-ji-ji-ben-shi-yong/"/>
      <url>/2019/12/27/elasticsearch-yuan-li-ji-ji-ben-shi-yong/</url>
      
        <content type="html"><![CDATA[<h1 id="ElasticSearch基本原理"><a href="#ElasticSearch基本原理" class="headerlink" title="ElasticSearch基本原理"></a>ElasticSearch基本原理</h1><h3 id="ElasticSearch-基本原理"><a href="#ElasticSearch-基本原理" class="headerlink" title="ElasticSearch 基本原理"></a>ElasticSearch 基本原理</h3><h4 id="搜索原理"><a href="#搜索原理" class="headerlink" title="搜索原理"></a>搜索原理</h4><p>ElasticSearch简介</p><ul><li><code>ElasticSearch</code>就是独立的网络上的一个或一组进程节点</li><li>它能对外提供搜索服务（使用<code>http</code>或<code>transport</code>协议），自从<code>ElasticSearch 7.0</code>开始，主要支持<code>http</code>协议</li><li><code>ElaticSearch</code>对内其实就是一个数据库</li></ul><h4 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h4><p>搜索是以词为单位做最基本单位的搜索单元，依赖分词器构建分词，用分词器倒排索引。</p><p><img src="/2019/12/27/elasticsearch-yuan-li-ji-ji-ben-shi-yong/D:%5CMyBlot%5Cbolt%5Csource_posts%5CElasticSearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%5C1.png" alt="img"></p><h4 id="倒排索引"><a href="#倒排索引" class="headerlink" title="倒排索引"></a>倒排索引</h4><ul><li><p>正向索引</p><pre><code>对文档中的所有分词进行遍历，获取与 keyword 中分词相同的词，命中一次，这种索引方法需要对每一个文档都进行遍历，性能上不是很好。复制代码</code></pre><p>​</p><p><img src="/2019/12/27/elasticsearch-yuan-li-ji-ji-ben-shi-yong/D:%5CMyBlot%5Cbolt%5Csource_posts%5CElasticSearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%5C2.png" alt="img"></p><p>​</p></li><li><p>倒排索引</p><pre><code>对文档的分词结果进行遍历，如果 keyword 命中该分词，那么通过该分词便可以找到所有含有这个分词的文档，性能相对较高。复制代码</code></pre><p>​</p><p><img src="/2019/12/27/elasticsearch-yuan-li-ji-ji-ben-shi-yong/D:%5CMyBlot%5Cbolt%5Csource_posts%5CElasticSearch%E5%8E%9F%E7%90%86%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%5C3.png" alt="img"></p><p>​</p></li></ul><h3 id="TF-IDF打分"><a href="#TF-IDF打分" class="headerlink" title="TF/IDF打分"></a>TF/IDF打分</h3><pre><code>TF：词频，这个 document 文档包含了多少个这个词，包含越多表名越相关DF：文档频率，包含该词的文档总数目IDF：DF 取反复制代码</code></pre><table><thead><tr><th>单词 ID</th><th>单词</th><th>文档频率</th><th>倒排列表（DocID;TF;<pos>）</pos></th></tr></thead><tbody><tr><td>1</td><td>谷歌</td><td>5</td><td>（1;1;&lt;1&gt;），（2;1;&lt;1&gt;），（3;2;&lt;1;6&gt;），（4;1;&lt;1&gt;），（5;1;&lt;1&gt;）</td></tr><tr><td>2</td><td>地图</td><td>5</td><td>（1;1;&lt;2&gt;），（2;1;&lt;2&gt;），（3;1;&lt;2&gt;），（4;1;&lt;2&gt;），（5;1;&lt;2&gt;）</td></tr><tr><td>3</td><td>之父</td><td>4</td><td>（1;1;&lt;3&gt;），（2;1;&lt;3&gt;），（4;1;&lt;3&gt;），（5;1;&lt;3&gt;）</td></tr><tr><td>4</td><td>跳槽</td><td>2</td><td>（1;1;&lt;4&gt;），（4;1;&lt;4&gt;）</td></tr><tr><td>5</td><td>Facebook</td><td>5</td><td>（1;1;&lt;5&gt;），（2;1;&lt;5&gt;），（3;1;&lt;8&gt;），（4;1;&lt;5&gt;），（5;1;&lt;8&gt;）</td></tr><tr><td>6</td><td>加盟</td><td>3</td><td>（2;1;&lt;4&gt;），（3;1;&lt;7&gt;），（5;1;&lt;5&gt;）</td></tr><tr><td>7</td><td>创始人</td><td>1</td><td>（3;1;&lt;3&gt;）</td></tr><tr><td>8</td><td>拉斯</td><td>2</td><td>（3;1;&lt;4&gt;），（5;1;&lt;4&gt;）</td></tr><tr><td>9</td><td>离开</td><td>1</td><td>（3;1;&lt;5&gt;）</td></tr><tr><td>10</td><td>与</td><td>1</td><td>（4;1;&lt;6&gt;）</td></tr></tbody></table><h3 id="名词定义"><a href="#名词定义" class="headerlink" title="名词定义"></a>名词定义</h3><p>在<code>ElasticSearch</code>中的<code>索引=数据库，类型=表，文档=行数据</code>，在<code>ElasticSearch 7.0</code>逐步的废弃了类型这样一个定义，也就是说索引和类型统称为索引，也就是说在<code>ElasticSearch 7.0</code>中只有索引和文档的定义，其中索引相当于索引和类型。</p><p>与关系型数据库的名词之间的类比：</p><table><thead><tr><th>关系型数据库</th><th>ElasticSearch</th></tr></thead><tbody><tr><td>Database</td><td>Index</td></tr><tr><td>Table</td><td>Type</td></tr><tr><td>Row</td><td>Document</td></tr><tr><td>Column</td><td>Field</td></tr><tr><td>Schema</td><td>Mapping</td></tr><tr><td>Index</td><td>Everything is indexed</td></tr><tr><td>SQL</td><td>Query DSL</td></tr><tr><td>SELECT * FROM table …</td><td>GET http://…</td></tr><tr><td>UPDATE table SET …</td><td>PUT http://…</td></tr></tbody></table><h3 id="分布式原理"><a href="#分布式原理" class="headerlink" title="分布式原理"></a>分布式原理</h3><h4 id="分片"><a href="#分片" class="headerlink" title="分片"></a>分片</h4><p>ElasticSearch 中默认的主分片（num_of_shards）数量为 5，默认的副本（num_of_replicas）个数为 1<br>主分片数量一旦设置就不会再改变，但是副本个数可以进行改变</p><p>在单集群节点中，如果主分片数量为 1，副本也为 1，那么此时单节点集群的 health 为 yellow，因为单节点必然会存在副本和主分片在同一个节点，会导致该索引的 health 为 yellow。此时可以修改集群的副本个数为 0，则索引和集群的状态会变回到 green 状态。</p><h4 id="主从"><a href="#主从" class="headerlink" title="主从"></a>主从</h4><pre><code>单节点集群中，如果创建了一个索引，默认会拥有副本，此时由于副本和主分片在一个节点，则单节点集群的 health 为 yellow。创建索引时，根据未来的扩展性，将主分片个数设置为合适的个数，但也不能太多，因为数量太多，如果数据量没有达到对应的瓶颈，在进行数据聚合时，就需要遍历所有的主分片，才能得到结果。# 双节点集群当新增一个集群节点的，此时节点集群会将副本全部存放到新增的节点上，此时如果索引的副本个数为 1 时，那么此时集群的状态会变成 green。# 三节点集群当继续新增一个集群节点时，此时节点集群会使用负载均衡，将某个索引的主分片会迁移到新增的节点上，会将部分的副本也迁移到新增的节点上# 三台节点构成 es 集群# 副本个数为 2 时此时 es 集群会将副本个数分别存放于另外两个节点之上，这两个节点中并没有改索引的主分片，此时主从分离，读写也分离。es 集群其实是一个对称的结构，es 集群的 master 用来管理所有负载的一个核心节点，也就是说如果要进行写操作，如果要写的索引的主分片就在 master 上，那么就可以直接进行写请求；如果写的索引在另一个节点上，那么 master 会将这个写请求进行转发，转发到要写的主分片所在的节点，由该节点进行写请求。# 当关掉此时的 master 节点时此时 es 集群会进行选举，选举一个新的 master 节点，会比较剩余的节点中的 metadata 是最新的，就会通过 paxos 方式从具备竞争主节点能力的机器中竞选主节点。此时被关掉的 master 上存在的主分片会均匀的分布到剩余的集群节点上，此时由于副本个数为 2，所以集群的状态又会变成 yellow，但是集群对外响应服务的能力还是有的。# 三台节点构成 es 集群，并且状态为 green# PUT 请求当有操作请求集群进行新建索引操作时，无论请求的是 es 集群的那个节点，都会将该请求转发到 master 节点，因为只有 master 节点知道所有的 metadata 存储的位置，此时 master 会对进行写请求的操作进行识别，如果写请求的 documenId 识别是在 master 节点上，那么由 master 节点进行写操作，如果在 node3 上，就会将该写请求进行转发，转发到 node3，由 node3 完成写请求。写操作完成之后，node3 会异步的将数据改动同步回其对应的副本。# GET 请求进行读请求时，master 会进行一次路由计算，计算出该读请求属于 R0 副本的操作，假设 master 节点上也存在 R0 副本，假设上一次的操作是在 master 节点上进行，那么 es 集群会根据负载均衡，将该读请求转发到另一个存在 R0 副本的节点上。如果读请求第一次命中在别的节点上，但是由于是读请求，可以由每个节点进行路由操作。# 总结也就是说，master 节点只路由读请求，所有的写请求可以由其命中的节点进行处理，此时整个 es 集群就做到了负载均衡和读写分离。</code></pre><p><strong>思考:为什么es开始的时候特别慢,用一会儿就很快了?</strong></p><p>其实es的数据本来大部分存在磁盘上,操作系统存在一个系统缓存,加载使用后的数据会放在里面,访问速度远远大于直接从磁盘读取.</p>]]></content>
      
      
      <categories>
          
          <category> tool </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop集群搭建与简单使用</title>
      <link href="/2019/12/01/hadoop-ji-qun-da-jian-yu-jian-dan-shi-yong/"/>
      <url>/2019/12/01/hadoop-ji-qun-da-jian-yu-jian-dan-shi-yong/</url>
      
        <content type="html"><![CDATA[<h1 id="Hadoop集群搭建与简单使用"><a href="#Hadoop集群搭建与简单使用" class="headerlink" title="Hadoop集群搭建与简单使用"></a>Hadoop集群搭建与简单使用</h1><p>首先需要搭建一个linux的集群,可以参见我的博客<a href="https://tigerluhai.github.io/2019/12/29/shi-yong-vmware-an-zhuang-linux-xu-ni-ji/">linux集群搭建</a></p><blockquote><p>Hadoop的运行是基于java的,所以需要先安装JDK,而且JDK版本必须高于1.7</p></blockquote><h3 id="安装JDK"><a href="#安装JDK" class="headerlink" title="安装JDK"></a>安装JDK</h3><p>（1）查询是否安装Java软件：</p><pre><code> rpm -qa | grep java</code></pre><p>（2）如果安装的版本低于1.7，卸载该JDK：</p><pre><code>sudo rpm -e 软件包</code></pre><p>（3）查看JDK安装路径：</p><pre><code> which(or whereis) java</code></pre><p>（4）解压JDK：</p><pre><code>tar -zxvf 安装包名 -C 目标路径</code></pre><p>（5）配置JDK环境：</p><pre><code>vi /etc/profile在文件末尾加上#JAVA_HOMEexport JAVA_HOME=/opt/module/jdk1.8.0_144export PATH=$PATH:$JAVA_HOME/bin</code></pre><p><strong>修改后需要立即生效需要运行如下命令:</strong></p><pre><code>source /etc/profile</code></pre><p>（6）测试JDK安装是否成功：</p><pre><code>java -version</code></pre><h3 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h3><p>（1）解压hadoop安装包到指定位置</p><pre><code>tar -zxvf 安装包 -C 指定目录</code></pre><p>（2）添加环境变量</p><pre><code>vi /etc/profile在文件末尾加上#HADOOP_HOMEexport HADOOP_HOME=/opt/module/hadoop-3.1.2export PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbin</code></pre><p>(3)修改配置文件</p><ul><li>集群部署规划</li></ul><table><thead><tr><th></th><th>master</th><th>slave1</th><th>slave2</th><th>slave3</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNodeDataNode</td><td>SecondaryNameNode DataNode</td><td>DataNode</td><td>DataNode</td></tr><tr><td>YARN</td><td>NodeManager</td><td>ResourceManager</td><td>NodeManager</td><td>NodeManager</td></tr></tbody></table><ul><li>核心配置文件</li></ul><p>配置core-site.xml</p><pre><code>vi core-site.xml# 在该文件中编写如下配置&lt;!-- 指定HDFS中NameNode的地址 --&gt;&lt;property&gt;    &lt;name&gt;fs.defaultFS&lt;/name&gt;    &lt;value&gt;hdfs://master:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;&lt;property&gt;    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;    &lt;value&gt;/opt/module/hadoop-3.1.2/data/tmp&lt;/value&gt;&lt;/property&gt;</code></pre><ul><li>HDFS配置文件</li></ul><p>配置hadoop-env.sh</p><pre><code> vi hadoop-env.shexport JAVA_HOME=/opt/module/jdk1.8.0_144</code></pre><p>配置hdfs-site.xml</p><pre><code>vi hdfs-site.xml&lt;!--配置副本数量--&gt;&lt;property&gt;        &lt;name&gt;dfs.replication&lt;/name&gt;        &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定Hadoop辅助名称节点主机配置 --&gt;&lt;property&gt;      &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;      &lt;value&gt;slave1:50090&lt;/value&gt;&lt;/property&gt;</code></pre><p>注意:文件副本数量不只是dfs.replication决定,而是min(datanode节点数,dfs.replication)</p><ul><li>YARN配置文件</li></ul><p>配置yarn-env.sh</p><pre><code> vi yarn-env.shexport JAVA_HOME=/opt/module/jdk1.8.0_144</code></pre><ul><li>配置yarn-site.xml</li></ul><pre><code> vi yarn-site.xml&lt;!-- Reducer获取数据的方式 --&gt;&lt;property&gt;    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定YARN的ResourceManager的地址 --&gt;&lt;property&gt;    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;    &lt;value&gt;slave2&lt;/value&gt;&lt;/property&gt;</code></pre><ul><li>MapReduce配置文件</li></ul><pre><code>vi mapred-env.shexport JAVA_HOME=/opt/module/jdk1.8.0_144</code></pre><p><strong>注意：hadoop3之前的版本,mapreduce会继承hadoop的配置,所以可以不用配置这一项,但是3以后的版本必须配置,否则运行mapreduce时会报环境出错。</strong></p><ul><li>配置mapred-site.xml</li></ul><pre><code>cp mapred-site.xml.template mapred-site.xmlvi mapred-site.xml&lt;!-- 指定MR运行在Yarn上 --&gt;&lt;property&gt;        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;        &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;</code></pre><p>（4）在集群上分发配置好的Hadoop配置文件</p><p>编写分发脚本</p><pre><code>vi /usr/local/bin/xsync#!/bin/bashpcount=$#if((pcount=0));thenecho no args;exit;fip1=$1fname=`basename $p1`echo fname=$fnamepdir=`cd -P $(dirname $p1);pwd`echo pdir=$pdiruser=`whoami`echo user=$userfor((host=1;host&lt;4;host++));doecho ------------salve$host-------------rsync -rvl $pdir/$fname $user@slave$host:$pdirdone</code></pre><pre><code>xsync /opt/module/hadoop-3.1.2/</code></pre><h3 id="集群单节点启动"><a href="#集群单节点启动" class="headerlink" title="集群单节点启动"></a>集群单节点启动</h3><p>（1）如果集群是第一次启动，需要<strong>格式化NameNode</strong></p><pre><code>hadoop namenode -format</code></pre><p>（2）在master上启动NameNode</p><pre><code>hadoop-daemon.sh start namenode</code></pre><p>查看节点启动状态</p><pre><code> jps</code></pre><p><strong>注意:jps用于查看java进程,namenode和datanode都是java进程</strong></p><p>（3）在master,slave1,slave2以及slave3上分别启动DataNode</p><pre><code>hadoop-daemon.sh start datanode</code></pre><pre><code> jps3461 NameNode3608 Jps3561 DataNode</code></pre><p>思考：每次都一个一个节点启动，如果节点数太多怎么办？</p><h3 id="群起集群"><a href="#群起集群" class="headerlink" title="群起集群"></a>群起集群</h3><ol><li>配置slaves</li></ol><pre><code>vi /opt/module/hadoop-3.1.2/etc/hadoop/slaves在该文件中增加如下内容：masterslave1slave2slave3</code></pre><p><strong>注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。</strong></p><p>同步所有节点配置文件</p><pre><code> xsync slaves</code></pre><ol start="2"><li>启动集群</li></ol><p>（1）如果集群是第一次启动，需要格式化NameNode（注意格式化之前，一定要先停止上次启动的所有namenode和datanode进程，然后再删除data和log数据）</p><pre><code>hdfs namenode -format</code></pre><p>（2）启动HDFS</p><pre><code>start-dfs.sh</code></pre><p>（3）启动YARN</p><pre><code>start-yarn.sh</code></pre><p><strong>注意：NameNode和ResourceManger如果不是同一台机器，不能在NameNode上启动 YARN，应该在ResouceManager所在的机器上启动YARN。</strong></p><h3 id="集群基本操作"><a href="#集群基本操作" class="headerlink" title="集群基本操作"></a>集群基本操作</h3><ul><li>-ls: 显示目录信息</li></ul><pre><code>hadoop fs -ls /</code></pre><ul><li>mkdir：在HDFS上创建目录</li></ul><pre><code> hadoop fs -mkdir -p /parent/test</code></pre><ul><li>moveFromLocal：从本地剪切粘贴到HDFS</li></ul><pre><code>hadoop fs  -moveFromLocal  ./kongming.txt  /parent/test</code></pre><ul><li>appendToFile：追加一个文件到已经存在的文件末尾</li></ul><pre><code> hadoop fs -appendToFile liubei.txt /parent/test/kongming.txt</code></pre><ul><li>cat：显示文件内容</li></ul><pre><code>hadoop fs -cat /parent/test/kongming.txt</code></pre><ul><li>chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</li><li>copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去</li></ul><pre><code> hadoop fs -copyFromLocal README.txt /</code></pre><ul><li>copyToLocal：从HDFS拷贝到本地</li></ul><pre><code> hadoop fs -copyToLocal /parent/test/kongming.txt ./</code></pre><ul><li>cp ：从HDFS的一个路径拷贝到HDFS的另一个路径</li></ul><pre><code>hadoop fs -cp /parent/test/kongming.txt  /zhuge.txt</code></pre><ul><li>mv：在HDFS目录中移动文件</li></ul><pre><code>hadoop fs -mv /zhuge.txt /parent/test/</code></pre><ul><li>get：等同于copyToLocal，就是从HDFS下载文件到本地</li></ul><pre><code>hadoop fs -get /parent/test/kongming.txt ./</code></pre><ul><li>getmerge：合并下载多个文件，比如HDFS的目录 /parent/test//test下有多个文件:log.1, log.2,log.3,…</li></ul><pre><code>hadoop fs -getmerge /parent/test/test/* ./zaiyiqi.txt</code></pre><ul><li>put：等同于copyFromLocal</li></ul><pre><code> hadoop fs -put ./zaiyiqi.txt /</code></pre><ul><li>tail：显示一个文件的末尾</li></ul><ul><li>rm：删除文件或文件夹</li></ul><ul><li>rmdir：删除空目录</li></ul><ul><li>du统计文件夹的大小信息</li></ul><h3 id="JAVA代码操作HDFS"><a href="#JAVA代码操作HDFS" class="headerlink" title="JAVA代码操作HDFS"></a>JAVA代码操作HDFS</h3><pre><code>    Logger logger = LoggerFactory.getLogger(HdfsClient.class);    FileSystem fileSystem;    Configuration configuration;    @Test    /**     * 测试环境正常     */    public void HDFS_ENV() throws IOException {        Logger logger = LoggerFactory.getLogger(HdfsClient.class);        //1.获取hdfs客户端对象        Configuration configuration = new Configuration();        configuration.set(&quot;fs.defaultFS&quot;,&quot;hdfs://master:9000&quot;);        FileSystem fileSystem = FileSystem.get(configuration);        //2.在hdfs上执行相关操作        boolean mkdirs = fileSystem.mkdirs(new Path(&quot;/client_test_environment&quot;));        //3.关闭资源        fileSystem.close();        System.out.println(mkdirs);    }    @Before    /**     * 创建fileSystem对象     */    public void createFileSystem() throws URISyntaxException, IOException, InterruptedException {        //1.获取fs对象        configuration = new Configuration();        fileSystem = FileSystem.get(new URI(&quot;hdfs://master:9000&quot;), configuration, &quot;root&quot;);    }    @Test    public void copyFromLocalFile() throws IOException{        //执行上传操作        fileSystem.copyFromLocalFile(new Path(&quot;D:\\logs\\xc.2019-04-29.log&quot;),new Path(&quot;/client_test_environment/&quot;));        //关闭资源        fileSystem.close();    }    @Test    public void copyToLocalFile() throws IOException{        //执行上传操作        fileSystem.copyToLocalFile(true,new Path(&quot;/client_test_environment/xc.2019-04-29.log&quot;),new Path(&quot;D:\\logs\\xc.2019-04-29-back.log&quot;),false);        //关闭资源        fileSystem.close();    }    @Test    public void listFiles() throws IOException {        //查看文件信息        RemoteIterator&lt;LocatedFileStatus&gt; iterator = fileSystem.listFiles(new Path(&quot;/&quot;), true);        while (iterator.hasNext()){            LocatedFileStatus fileStatus = iterator.next();            //获取文件名称，文件权限，文件长度，块信息            logger.info(fileStatus.getPath().getName());            logger.info(fileStatus.getLen()+&quot;&quot;);            logger.info(fileStatus.getPermission()+&quot;&quot;);            BlockLocation[] blockLocations = fileStatus.getBlockLocations();            for (BlockLocation blockLocation : blockLocations) {                logger.info(blockLocation.toString());            }            logger.info(&quot;----------------                    ----------------------&quot;);        }    }</code></pre>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
